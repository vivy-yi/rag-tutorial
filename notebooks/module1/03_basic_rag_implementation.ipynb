{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬3ç« ï¼šåŸºç¡€RAGå®ç°\n",
    "\n",
    "> ä»é›¶å¼€å§‹æ„å»ºç¬¬ä¸€ä¸ªå®Œæ•´çš„RAGç³»ç»Ÿ\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "æœ¬Notebookå°†å¸¦ä½ ï¼š\n",
    "- âœ… åŠ è½½å’Œå¤„ç†æ–‡æ¡£\n",
    "- âœ… å®ç°æ–‡æœ¬åˆ†å—\n",
    "- âœ… åˆ›å»ºå‘é‡ç´¢å¼•\n",
    "- âœ… æ„å»ºå®Œæ•´çš„RAGæŸ¥è¯¢ç³»ç»Ÿ\n",
    "- âœ… æµ‹è¯•å’Œè¯„ä¼°ç³»ç»Ÿ\n",
    "\n",
    "## é¢„è®¡æ—¶é—´\n",
    "\n",
    "- æ–‡æ¡£åŠ è½½ï¼š30åˆ†é’Ÿ\n",
    "- æ–‡æœ¬åˆ†å—ï¼š30åˆ†é’Ÿ\n",
    "- å‘é‡ç´¢å¼•ï¼š40åˆ†é’Ÿ\n",
    "- RAGæŸ¥è¯¢ï¼š30åˆ†é’Ÿ\n",
    "- æµ‹è¯•è¯„ä¼°ï¼š20åˆ†é’Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# æ£€æŸ¥ç¯å¢ƒ\n",
    "print(\"æ£€æŸ¥ç¯å¢ƒ...\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {os.sys.version}\")\n",
    "\n",
    "# æ¨¡æ‹ŸLlamaIndexå’ŒChromaDBï¼ˆå¦‚æœæœªå®‰è£…ï¼‰\n",
    "try:\n",
    "    import llama_index\n",
    "    import chromadb\n",
    "    print(\"âœ… LlamaIndexå’ŒChromaDBå·²å®‰è£…\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  éƒ¨åˆ†åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤º\")\n",
    "    print(\"   è¯·è¿è¡Œ: pip install llama-index-core chromadb\")\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®ç›®å½•\n",
    "DATA_DIR = Path(\"./data/sample_documents\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\næ•°æ®ç›®å½•: {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ–‡æ¡£åŠ è½½ä¸å¤„ç†\n",
    "\n",
    "### 2.1 åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¤ºä¾‹æ–‡æ¡£\n",
    "sample_docs = {\n",
    "    \"doc1.txt\": \"\"\"\n",
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€\n",
    "\n",
    "Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚\n",
    "å®ƒçš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§ã€‚\n",
    "Pythonå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½ã€‚\n",
    "\"\"\",\n",
    "    \n",
    "    \"doc2.txt\": \"\"\"\n",
    "RAGæŠ€æœ¯è¯¦è§£\n",
    "\n",
    "RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§AIæŠ€æœ¯ã€‚\n",
    "å®ƒç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹ã€‚\n",
    "RAGå¯ä»¥å‡å°‘LLMçš„å¹»è§‰é—®é¢˜ï¼Œæé«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€‚\n",
    "\"\"\",\n",
    "    \n",
    "    \"doc3.txt\": \"\"\"\n",
    "å‘é‡æ•°æ®åº“æŒ‡å—\n",
    "\n",
    "å‘é‡æ•°æ®åº“ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡ã€‚\n",
    "å¸¸è§çš„å‘é‡æ•°æ®åº“åŒ…æ‹¬Chromaã€Pineconeå’ŒMilvusã€‚\n",
    "å®ƒä»¬ä½¿ç”¨HNSWç­‰ç®—æ³•å®ç°å¿«é€Ÿè¿‘ä¼¼æœç´¢ã€‚\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# å†™å…¥æ–‡ä»¶\n",
    "for filename, content in sample_docs.items():\n",
    "    file_path = DATA_DIR / filename\n",
    "    file_path.write_text(content, encoding='utf-8')\n",
    "    print(f\"âœ… åˆ›å»º: {filename}\")\n",
    "\n",
    "print(f\"\\næ€»å…±åˆ›å»ºäº† {len(sample_docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£åŠ è½½å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDocumentLoader:\n",
    "    \"\"\"\n",
    "    ç®€å•çš„æ–‡æ¡£åŠ è½½å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory: Path):\n",
    "        self.directory = directory\n",
    "    \n",
    "    def load(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£\n",
    "        \n",
    "        Returns:\n",
    "            æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«contentå’Œmetadata\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å\n",
    "        supported_extensions = ['.txt', '.md']\n",
    "        \n",
    "        # éå†ç›®å½•\n",
    "        for file_path in self.directory.iterdir():\n",
    "            if file_path.is_file() and file_path.suffix in supported_extensions:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                documents.append({\n",
    "                    'content': content,\n",
    "                    'metadata': {\n",
    "                        'source': str(file_path),\n",
    "                        'filename': file_path.name\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = SimpleDocumentLoader(DATA_DIR)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"\\næ–‡æ¡£ {i}:\")\n",
    "    print(f\"  æ¥æº: {doc['metadata']['source']}\")\n",
    "    print(f\"  é¢„è§ˆ: {doc['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ–‡æœ¬åˆ†å—\n",
    "\n",
    "### 3.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    \"\"\"\n",
    "    æ–‡æœ¬åˆ†å—å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 100, chunk_overlap: int = 20):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬åˆ‡åˆ†æˆå—\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "        \n",
    "        Returns:\n",
    "            æ–‡æœ¬å—åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            if chunk:\n",
    "                chunks.append(chunk.strip())\n",
    "            \n",
    "            # ç§»åŠ¨çª—å£ï¼Œä¿ç•™overlap\n",
    "            start = end - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# æµ‹è¯•åˆ†å—å™¨\n",
    "splitter = TextSplitter(chunk_size=150, chunk_overlap=30)\n",
    "\n",
    "print(\"æµ‹è¯•æ–‡æœ¬åˆ†å—:\\n\")\n",
    "test_text = documents[0]['content']\n",
    "chunks = splitter.split(test_text)\n",
    "\n",
    "print(f\"åŸæ–‡é•¿åº¦: {len(test_text)} å­—ç¬¦\")\n",
    "print(f\"åˆ†å—æ•°é‡: {len(chunks)} å—\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"å— {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç®€å•çš„å‘é‡æ£€ç´¢\n",
    "\n",
    "### 4.1 æ¨¡æ‹ŸåµŒå…¥å’Œæ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    ç®€åŒ–çš„å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨TF-IDFï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectors = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_documents(self, docs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨\n",
    "        \"\"\"\n",
    "        texts = [doc['content'] for doc in docs]\n",
    "        self.documents = docs\n",
    "        self.vectors = self.vectorizer.fit_transform(texts)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        æœç´¢ç›¸å…³æ–‡æ¡£\n",
    "        \"\"\"\n",
    "        if self.vectors is None:\n",
    "            return []\n",
    "        \n",
    "        # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "        similarities = cosine_similarity(query_vector, self.vectors)[0]\n",
    "        \n",
    "        # æ’åºå¹¶è¿”å›top_k\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx]['content'],\n",
    "                'metadata': self.documents[idx]['metadata'],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# åˆ›å»ºå‘é‡å­˜å‚¨å¹¶æ·»åŠ æ–‡æ¡£\n",
    "vector_store = SimpleVectorStore()\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "print(\"å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æµ‹è¯•æ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ£€ç´¢\n",
    "test_queries = [\n",
    "    \"Pythonæ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"RAGæŠ€æœ¯çš„ä¼˜åŠ¿\",\n",
    "    \"å‘é‡æ•°æ®åº“\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æŸ¥è¯¢: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = vector_store.search(query, top_k=2)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nç»“æœ {i} (ç›¸å…³åº¦: {result['score']:.3f}):\")\n",
    "            print(f\"{result['document'][:100]}...\")\n",
    "    else:\n",
    "        print(\"æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å®Œæ•´çš„RAGæŸ¥è¯¢ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    ç®€å•çš„RAGç³»ç»Ÿ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Dict[str, Any]]):\n",
    "        self.vector_store = SimpleVectorStore()\n",
    "        self.vector_store.add_documents(documents)\n",
    "        \n",
    "        # æ¨¡æ‹ŸLLMå“åº”\n",
    "        self.llm_prompt_template = \"\"\"\n",
    "åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n",
    "\n",
    "{context}\n",
    "\n",
    "é—®é¢˜: {question}\n",
    "\n",
    "ç­”æ¡ˆ:\n",
    "        \"\"\"\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 2) -> Dict:\n",
    "        \"\"\"\n",
    "        RAGæŸ¥è¯¢\n",
    "        \n",
    "        Args:\n",
    "            question: ç”¨æˆ·é—®é¢˜\n",
    "            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«ç­”æ¡ˆå’Œæ¥æºçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        # æ­¥éª¤1ï¼šæ£€ç´¢\n",
    "        print(f\"\\nğŸ” æ­¥éª¤1: æ£€ç´¢ç›¸å…³æ–‡æ¡£\")\n",
    "        retrieved_docs = self.vector_store.search(question, top_k=top_k)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': 'æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚',\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "        \n",
    "        # æ­¥éª¤2ï¼šæ„å»ºä¸Šä¸‹æ–‡\n",
    "        print(f\"\\nğŸ“ æ­¥éª¤2: æ„å»ºä¸Šä¸‹æ–‡\")\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"æ–‡æ¡£{i+1}: {doc['document'][:100]}...\"\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "        \n",
    "        # æ­¥éª¤3ï¼šç”Ÿæˆç­”æ¡ˆ\n",
    "        print(f\"\\nğŸ¤– æ­¥éª¤3: ç”Ÿæˆç­”æ¡ˆ\")\n",
    "        answer = self._generate_answer(question, context)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': [\n",
    "                {\n",
    "                    'file': doc['metadata']['filename'],\n",
    "                    'score': doc['score']\n",
    "                }\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _generate_answer(self, question: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "        \"\"\"\n",
    "        # ç®€åŒ–å®ç°ï¼šåŸºäºè§„åˆ™ç”Ÿæˆç­”æ¡ˆ\n",
    "        if \"Python\" in question and \"Python\" in context:\n",
    "            return \"æ ¹æ®æ–‡æ¡£ï¼ŒPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½ã€‚\"\n",
    "        elif \"RAG\" in question:\n",
    "            return \"RAGï¼ˆRetrieval-Augmented Generationï¼‰ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å‡å°‘LLMçš„å¹»è§‰é—®é¢˜ï¼Œæé«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€‚\"\n",
    "        else:\n",
    "            return f\"æ ¹æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œ{context[:100]}...\"\n",
    "\n",
    "# åˆ›å»ºRAGç³»ç»Ÿ\n",
    "rag_system = SimpleRAG(documents)\n",
    "print(\"RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æµ‹è¯•RAGç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•RAGç³»ç»Ÿ\n",
    "test_questions = [\n",
    "    \"Pythonæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ\",\n",
    "    \"ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ\",\n",
    "    \"å¦‚ä½•ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    result = rag_system.query(q)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"é—®é¢˜: {result['question']}\")\n",
    "    print(f\"\\nç­”æ¡ˆ: {result['answer']}\")\n",
    "    print(f\"\\næ¥æº: {len(result['sources'])} ä¸ªæ–‡æ¡£\")\n",
    "    for source in result['sources']:\n",
    "        print(f\"  - {source['file']} (ç›¸å…³åº¦: {source['score']:.3f})\")\n",
    "    print()\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹ 1ï¼šæ”¹è¿›åˆ†å—ç­–ç•¥\n",
    "\n",
    "å½“å‰çš„åˆ†å—å™¨æ˜¯æŒ‰å­—ç¬¦æ•°ç®€å•åˆ‡åˆ†ï¼Œè¯·ä½ æ”¹è¿›å®ƒï¼š\n",
    "- æŒ‰å¥å­åˆ‡åˆ†\n",
    "- ä¿ç•™å®Œæ•´çš„è¯­ä¹‰å•å…ƒ\n",
    "- é¿å…åœ¨å¥å­ä¸­é—´åˆ‡åˆ†\n",
    "\n",
    "**æç¤º**ï¼šå¯ä»¥ä½¿ç”¨nltkæˆ–spaCyè¿›è¡Œå¥å­åˆ†å‰²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®ç°æ”¹è¿›çš„åˆ†å—å™¨\n",
    "class SmartTextSplitter(TextSplitter):\n",
    "    \n",
    "    def split_by_sentence(self, text: str) -> List[str]:\n",
    "        # æŒ‰å¥å­åˆ†å‰²\n",
    "        # TODO: å®ç°ä»£ç \n",
    "        pass\n",
    "\n",
    "# æµ‹è¯•æ–°çš„åˆ†å—å™¨\n",
    "# smart_splitter = SmartTextSplitter()\n",
    "# chunks = smart_splitter.split_by_sentence(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 2ï¼šæ·»åŠ å…ƒæ•°æ®\n",
    "\n",
    "ä¸ºæ–‡æ¡£æ·»åŠ æ›´å¤šå…ƒæ•°æ®ï¼ˆå¦‚ç±»åˆ«ã€æ—¥æœŸã€ä½œè€…ï¼‰ï¼Œå¹¶åœ¨æ£€ç´¢æ—¶æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: æ·»åŠ å…ƒæ•°æ®æ”¯æŒ\n",
    "class DocumentWithMetadata:\n",
    "    def __init__(self, content: str, category: str, author: str):\n",
    "        self.content = content\n",
    "        self.category = category\n",
    "        self.author = author\n",
    "        # ... å®ç°ä»£ç "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æ€»ç»“\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **æ–‡æ¡£åŠ è½½**ï¼šä»å„ç§æ•°æ®æºåŠ è½½æ–‡æ¡£\n",
    "2. **æ–‡æœ¬åˆ†å—**ï¼šåˆç†åˆ‡åˆ†æ–‡æ¡£å¾ˆé‡è¦\n",
    "3. **å‘é‡æ£€ç´¢**ï¼šåŸºäºç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "4. **RAGæµç¨‹**ï¼šæ£€ç´¢ + ç”Ÿæˆ = å‡†ç¡®ç­”æ¡ˆ\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- ç¬¬4ç« ï¼šRAGè¯„ä¼°åŸºç¡€\n",
    "- å­¦ä¹ å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿè´¨é‡\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆç¬¬3ç« çš„å­¦ä¹ ï¼** ğŸ‰\n",
    "\n",
    "ä½ å·²ç»å®ç°äº†ç¬¬ä¸€ä¸ªRAGç³»ç»Ÿï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
