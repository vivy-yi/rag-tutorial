{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - RAGè¯„ä¼°åŸºç¡€\n",
    "\n",
    "æœ¬ç« ä»‹ç»RAGç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡ï¼Œå¸®åŠ©æ‚¨ç†è§£å’Œè¡¡é‡RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAGè¯„ä¼°çš„é‡è¦æ€§\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°ï¼Ÿ\n",
    "\n",
    "- **å‡†ç¡®æ€§**: æ£€ç´¢çš„å†…å®¹æ˜¯å¦ç›¸å…³\n",
    "- **å®Œæ•´æ€§**: æ˜¯å¦æ£€ç´¢åˆ°äº†æ‰€æœ‰å¿…è¦ä¿¡æ¯\n",
    "- **å¯é æ€§**: ç³»ç»Ÿè¾“å‡ºæ˜¯å¦ç¨³å®šä¸€è‡´\n",
    "- **æ€§èƒ½**: å“åº”æ—¶é—´å’Œèµ„æºæ¶ˆè€—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "!pip install langchain langchain-openai chromadb numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ£€ç´¢è´¨é‡è¯„ä¼°\n",
    "\n",
    "### 2.1 åŸºç¡€æ£€ç´¢æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "def calculate_hit_rate(retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n",
    "    \"\"\"è®¡ç®—å‘½ä¸­ç‡ï¼ˆHit Rateï¼‰\n",
    "    \n",
    "    å‘½ä¸­ç‡ = è‡³å°‘æ£€ç´¢åˆ°ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢æ•° / æ€»æŸ¥è¯¢æ•°\n",
    "    \"\"\"\n",
    "    return 1.0 if any(doc in retrieved_docs for doc in relevant_docs) else 0.0\n",
    "\n",
    "def calculate_mrr(retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n",
    "    \"\"\"è®¡ç®—å¹³å‡å€’æ•°æ’åï¼ˆMean Reciprocal Rankï¼‰\n",
    "    \n",
    "    MRR = 1 / ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        if doc in relevant_docs:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_precision(retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n",
    "    \"\"\"è®¡ç®—ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰\n",
    "    \n",
    "    Precision = æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£æ•° / æ£€ç´¢åˆ°çš„æ–‡æ¡£æ€»æ•°\n",
    "    \"\"\"\n",
    "    if not retrieved_docs:\n",
    "        return 0.0\n",
    "    relevant_retrieved = len(set(retrieved_docs) & set(relevant_docs))\n",
    "    return relevant_retrieved / len(retrieved_docs)\n",
    "\n",
    "def calculate_recall(retrieved_docs: List[str], relevant_docs: List[str], total_relevant: int) -> float:\n",
    "    \"\"\"è®¡ç®—å¬å›ç‡ï¼ˆRecallï¼‰\n",
    "    \n",
    "    Recall = æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£æ•° / æ€»ç›¸å…³æ–‡æ¡£æ•°\n",
    "    \"\"\"\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    relevant_retrieved = len(set(retrieved_docs) & set(relevant_docs))\n",
    "    return relevant_retrieved / total_relevant\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "retrieved = ['doc1', 'doc3', 'doc5', 'doc7']\n",
    "relevant = ['doc2', 'doc3', 'doc4', 'doc7', 'doc8']\n",
    "\n",
    "print(f\"Hit Rate: {calculate_hit_rate(retrieved, relevant):.2f}\")\n",
    "print(f\"MRR: {calculate_mrr(retrieved, relevant):.2f}\")\n",
    "print(f\"Precision@4: {calculate_precision(retrieved, relevant):.2f}\")\n",
    "print(f\"Recall@4: {calculate_recall(retrieved, relevant, len(relevant)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç”Ÿæˆè´¨é‡è¯„ä¼°\n",
    "\n",
    "### 3.1 å¿ å®åº¦ï¼ˆFaithfulnessï¼‰\n",
    "\n",
    "è¯„ä¼°ç”Ÿæˆçš„å†…å®¹æ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(generated_answer: str, context: List[str]) -> Dict:\n",
    "    \"\"\"è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„å¿ å®åº¦\n",
    "    \n",
    "    æ£€æŸ¥ç­”æ¡ˆä¸­çš„äº‹å®æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®\n",
    "    \"\"\"\n",
    "    # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘\n",
    "    context_text = ' '.join(context)\n",
    "    \n",
    "    # æå–ç­”æ¡ˆä¸­çš„å…³é”®é™ˆè¿°ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "    statements = generated_answer.split('ã€‚')\n",
    "    \n",
    "    supported = 0\n",
    "    for statement in statements:\n",
    "        if any(word in context_text for word in statement.split()):\n",
    "            supported += 1\n",
    "    \n",
    "    faithfulness_score = supported / len(statements) if statements else 0\n",
    "    \n",
    "    return {\n",
    "        'score': faithfulness_score,\n",
    "        'supported_statements': supported,\n",
    "        'total_statements': len(statements)\n",
    "    }\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "answer = \"RAGç»“åˆäº†æ£€ç´¢å’Œç”ŸæˆæŠ€æœ¯ã€‚å®ƒå¯ä»¥æé«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€‚\"\n",
    "context = [\n",
    "    \"RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯\",\n",
    "    \"RAGå¯ä»¥é€šè¿‡å¤–éƒ¨çŸ¥è¯†åº“æé«˜LLMçš„å‡†ç¡®æ€§\"\n",
    "]\n",
    "\n",
    "result = evaluate_faithfulness(answer, context)\n",
    "print(f\"å¿ å®åº¦: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ç›¸å…³æ€§ï¼ˆRelevancyï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevancy(query: str, answer: str) -> float:\n",
    "    \"\"\"è¯„ä¼°ç­”æ¡ˆä¸æŸ¥è¯¢çš„ç›¸å…³æ€§\n",
    "    \n",
    "    ç®€åŒ–ç‰ˆï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«æŸ¥è¯¢çš„å…³é”®è¯\n",
    "    \"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    answer_words = set(answer.lower().split())\n",
    "    \n",
    "    overlap = len(query_words & answer_words)\n",
    "    relevancy = overlap / len(query_words) if query_words else 0\n",
    "    \n",
    "    return relevancy\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "query = \"ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ\"\n",
    "answer = \"RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆçš„æ–¹æ³•ã€‚\"\n",
    "\n",
    "score = evaluate_relevancy(query, answer)\n",
    "print(f\"ç›¸å…³æ€§å¾—åˆ†: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç«¯åˆ°ç«¯è¯„ä¼°\n",
    "\n",
    "### 4.1 RAGASè¯„ä¼°æ¡†æ¶ç®€ä»‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGEvaluator:\n",
    "    \"\"\"ç®€åŒ–çš„RAGè¯„ä¼°å™¨\n",
    "    \n",
    "    å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨RAGASåº“ï¼š\n",
    "    !pip install ragas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved_docs: List[str],\n",
    "        relevant_docs: List[str],\n",
    "        generated_answer: str,\n",
    "        context: List[str]\n",
    "    ) -> Dict:\n",
    "        \"\"\"è¯„ä¼°å•ä¸ªæŸ¥è¯¢\"\"\"\n",
    "        \n",
    "        # æ£€ç´¢æŒ‡æ ‡\n",
    "        hit_rate = calculate_hit_rate(retrieved_docs, relevant_docs)\n",
    "        mrr = calculate_mrr(retrieved_docs, relevant_docs)\n",
    "        precision = calculate_precision(retrieved_docs, relevant_docs)\n",
    "        \n",
    "        # ç”ŸæˆæŒ‡æ ‡\n",
    "        faithfulness = evaluate_faithfulness(generated_answer, context)\n",
    "        relevancy = evaluate_relevancy(query, generated_answer)\n",
    "        \n",
    "        result = {\n",
    "            'query': query,\n",
    "            'hit_rate': hit_rate,\n",
    "            'mrr': mrr,\n",
    "            'precision': precision,\n",
    "            'faithfulness': faithfulness['score'],\n",
    "            'relevancy': relevancy\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def get_average_scores(self) -> Dict:\n",
    "        \"\"\"è®¡ç®—å¹³å‡å¾—åˆ†\"\"\"\n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        metrics = self.results[0].keys()\n",
    "        averages = {}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric == 'query':\n",
    "                continue\n",
    "            values = [r[metric] for r in self.results]\n",
    "            averages[metric] = np.mean(values)\n",
    "        \n",
    "        return averages\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "evaluator = SimpleRAGEvaluator()\n",
    "\n",
    "# è¯„ä¼°å‡ ä¸ªæŸ¥è¯¢\n",
    "test_queries = [\n",
    "    {\n",
    "        'query': 'ä»€ä¹ˆæ˜¯RAGï¼Ÿ',\n",
    "        'retrieved': ['doc1', 'doc2', 'doc3'],\n",
    "        'relevant': ['doc1', 'doc3'],\n",
    "        'answer': 'RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯',\n",
    "        'context': ['RAGç»“åˆæ£€ç´¢å’Œç”Ÿæˆ', 'æé«˜LLMå‡†ç¡®æ€§']\n",
    "    },\n",
    "    {\n",
    "        'query': 'å¦‚ä½•ä¼˜åŒ–RAGï¼Ÿ',\n",
    "        'retrieved': ['doc4', 'doc5', 'doc6'],\n",
    "        'relevant': ['doc4'],\n",
    "        'answer': 'å¯ä»¥é€šè¿‡æ··åˆæ£€ç´¢ä¼˜åŒ–',\n",
    "        'context': ['æ··åˆæ£€ç´¢ç»“åˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢']\n",
    "    }\n",
    "]\n",
    "\n",
    "for tq in test_queries:\n",
    "    result = evaluator.evaluate_query(\n",
    "        tq['query'],\n",
    "        tq['retrieved'],\n",
    "        tq['relevant'],\n",
    "        tq['answer'],\n",
    "        tq['context']\n",
    "    )\n",
    "    print(f\"\\næŸ¥è¯¢: {tq['query']}\")\n",
    "    print(f\"å‘½ä¸­ç‡: {result['hit_rate']:.2f}, MRR: {result['mrr']:.2f}\")\n",
    "    print(f\"å¿ å®åº¦: {result['faithfulness']:.2f}, ç›¸å…³æ€§: {result['relevancy']:.2f}\")\n",
    "\n",
    "print(\"\\n=== å¹³å‡å¾—åˆ† ===\")\n",
    "averages = evaluator.get_average_scores()\n",
    "for metric, score in averages.items():\n",
    "    print(f\"{metric}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è¯„ä¼°æœ€ä½³å®è·µ\n",
    "\n",
    "### 5.1 åˆ›å»ºæµ‹è¯•æ•°æ®é›†\n",
    "\n",
    "```python\n",
    "# åˆ›å»ºæµ‹è¯•æ•°æ®é›†\n",
    "test_dataset = [\n",
    "    {\n",
    "        'query': 'é—®é¢˜1',\n",
    "        'relevant_docs': ['doc1', 'doc2'],\n",
    "        'expected_answer': 'é¢„æœŸç­”æ¡ˆ'\n",
    "    },\n",
    "    # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹\n",
    "]\n",
    "```\n",
    "\n",
    "### 5.2 å®šæœŸè¯„ä¼°\n",
    "- åœ¨å¼€å‘é˜¶æ®µé¢‘ç¹è¯„ä¼°\n",
    "- æ¯æ¬¡é‡å¤§æ›´æ”¹åé‡æ–°è¯„ä¼°\n",
    "- ç›‘æ§ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½\n",
    "\n",
    "### 5.3 A/Bæµ‹è¯•\n",
    "- æ¯”è¾ƒä¸åŒRAGé…ç½®çš„æ•ˆæœ\n",
    "- æµ‹è¯•æ–°çš„æ£€ç´¢ç­–ç•¥\n",
    "- è¯„ä¼°ä¸åŒçš„LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹ 1: å®ç°F1-score\n",
    "\n",
    "```python\n",
    "def calculate_f1_score(precision: float, recall: float) -> float:\n",
    "    \"\"\"è®¡ç®—F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰\"\"\"\n",
    "    # TODO: å®ç°F1-scoreè®¡ç®—\n",
    "    pass\n",
    "```\n",
    "\n",
    "### ç»ƒä¹ 2: å®ç°NDCGï¼ˆNormalized Discounted Cumulative Gainï¼‰\n",
    "\n",
    "```python\n",
    "def calculate_ndcg(retrieved_docs: List[str], relevant_docs: List[str], k: int = 10) -> float:\n",
    "    \"\"\"è®¡ç®—NDCG@k\n",
    "    \n",
    "    NDCGè€ƒè™‘æ–‡æ¡£åœ¨ç»“æœåˆ—è¡¨ä¸­çš„ä½ç½®å’Œç›¸å…³ç¨‹åº¦\n",
    "    \"\"\"\n",
    "    # TODO: å®ç°NDCGè®¡ç®—\n",
    "    pass\n",
    "```\n",
    "\n",
    "### ç»ƒä¹ 3: å¯è§†åŒ–è¯„ä¼°ç»“æœ\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evaluation_results(results: List[Dict]):\n",
    "    \"\"\"ç»˜åˆ¶è¯„ä¼°ç»“æœå¯¹æ¯”å›¾\"\"\"\n",
    "    # TODO: åˆ›å»ºå›¾è¡¨æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½\n",
    "    pass\n",
    "```\n",
    "\n",
    "å‚è€ƒç­”æ¡ˆè¯·æŸ¥çœ‹ `exercises/04_rag_evaluation_answers.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ€»ç»“\n",
    "\n",
    "æœ¬ç« å­¦ä¹ äº†ï¼š\n",
    "\n",
    "âœ… **æ£€ç´¢è´¨é‡è¯„ä¼°**\n",
    "- Hit Rate, MRR, Precision, Recall\n",
    "\n",
    "âœ… **ç”Ÿæˆè´¨é‡è¯„ä¼°**\n",
    "- Faithfulness, Relevancy\n",
    "\n",
    "âœ… **ç«¯åˆ°ç«¯è¯„ä¼°**\n",
    "- æ„å»ºè¯„ä¼°æ¡†æ¶\n",
    "- è®¡ç®—ç»¼åˆæŒ‡æ ‡\n",
    "\n",
    "âœ… **æœ€ä½³å®è·µ**\n",
    "- åˆ›å»ºæµ‹è¯•æ•°æ®é›†\n",
    "- å®šæœŸè¯„ä¼°å’Œç›‘æ§\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**: ç¬¬5ç« å°†ç»¼åˆå‰é¢æ‰€å­¦ï¼Œå®Œæˆä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆç¬¬4ç« çš„å­¦ä¹ ï¼** ğŸ‰\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†RAGè¯„ä¼°çš„åŸºç¡€çŸ¥è¯†ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
