{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬16ç« ï¼šå¤šæ¨¡æ€RAGå®è·µ\n",
    "\n",
    "> æ¢ç´¢å›¾åƒ+æ–‡æœ¬çš„è·¨æ¨¡æ€æ£€ç´¢ä¸ç”Ÿæˆ\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "æœ¬Notebookå°†å¸¦ä½ ï¼š\n",
    "- âœ… ç†è§£å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼ˆCLIPï¼‰\n",
    "- âœ… å®ç°å›¾åƒæ£€ç´¢ï¼ˆä»¥å›¾æœå›¾ï¼‰\n",
    "- âœ… å®ç°è·¨æ¨¡æ€æ£€ç´¢ï¼ˆä»¥æ–‡æœå›¾ï¼‰\n",
    "- âœ… é›†æˆè§†è§‰ç†è§£ï¼ˆGPT-4Vï¼‰\n",
    "- âœ… æ„å»ºå¤šæ¨¡æ€RAGç³»ç»Ÿ\n",
    "\n",
    "## é¢„è®¡æ—¶é—´\n",
    "- CLIPåŸºç¡€ï¼š25åˆ†é’Ÿ\n",
    "- å›¾åƒæ£€ç´¢ï¼š25åˆ†é’Ÿ\n",
    "- è·¨æ¨¡æ€æ£€ç´¢ï¼š30åˆ†é’Ÿ\n",
    "- GPT-4Vé›†æˆï¼š20åˆ†é’Ÿ\n",
    "- å®Œæ•´ç³»ç»Ÿï¼š30åˆ†é’Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¤šæ¨¡æ€åµŒå…¥åŸºç¡€\n",
    "\n",
    "### 1.1 ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€RAGï¼Ÿ\n",
    "\n",
    "**ä¼ ç»ŸRAG**ï¼šæ–‡æœ¬ â†’ æ–‡æœ¬\n",
    "```\n",
    "Query (æ–‡æœ¬) â†’ æ£€ç´¢ â†’ æ–‡æ¡£ (æ–‡æœ¬) â†’ ç”Ÿæˆ â†’ ç­”æ¡ˆ (æ–‡æœ¬)\n",
    "```\n",
    "\n",
    "**å¤šæ¨¡æ€RAG**ï¼šæ–‡æœ¬/å›¾åƒ â†’ è·¨æ¨¡æ€æ£€ç´¢ â†’ æ–‡æœ¬/å›¾åƒ â†’ ç­”æ¡ˆ\n",
    "```\n",
    "Query (æ–‡æœ¬/å›¾åƒ) â†’ æ£€ç´¢ â†’ æ–‡æ¡£ (æ–‡æœ¬/å›¾åƒ) â†’ ç†è§£ â†’ ç­”æ¡ˆ (æ–‡æœ¬/å›¾åƒ)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CLIPæ¨¡å‹ç®€ä»‹\n",
    "\n",
    "**CLIP** (Contrastive Language-Image Pre-training) by OpenAI\n",
    "\n",
    "æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "- å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°**åŒä¸€ä¸ªå‘é‡ç©ºé—´**\n",
    "- ç›¸ä¼¼çš„å›¾åƒå’Œæ–‡æœ¬åœ¨ç©ºé—´ä¸­**è·ç¦»æ›´è¿‘**\n",
    "- å®ç°**è·¨æ¨¡æ€è¯­ä¹‰ç†è§£**\n",
    "\n",
    "```python\n",
    "# CLIPå·¥ä½œåŸç†\n",
    "Image Encoder: å›¾åƒ â†’ 512ç»´å‘é‡\n",
    "Text Encoder: æ–‡æœ¬ â†’ 512ç»´å‘é‡\n",
    "\n",
    "# ç›¸ä¼¼åº¦è®¡ç®—\n",
    "similarity = cosine_similarity(image_vector, text_vector)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ç¯å¢ƒé…ç½®\n",
    "\n",
    "å®‰è£…å¿…è¦çš„åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q torch torchvision transformers pillow matplotlib\n",
    "!pip install -q openai clip-by-openai  # CLIPæ¨¡å‹\n",
    "!pip install -q sentence-transformers  # å¤‡é€‰æ–¹æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥åº“\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "\n",
    "# æ£€æŸ¥GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. CLIPæ¨¡å‹å®è·µ\n",
    "\n",
    "### 2.1 åŠ è½½CLIPæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "# åŠ è½½CLIPæ¨¡å‹\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(f\"Model loaded: ViT-B/32\")\n",
    "print(f\"Image input resolution: {model.visual.input_resolution}\")\n",
    "print(f\"Context length: {model.context_length}\")\n",
    "print(f\"Vocabulary size: {model.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 å›¾åƒå’Œæ–‡æœ¬ç¼–ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨CLIPç¼–ç å›¾åƒ\n",
    "    \n",
    "    Args:\n",
    "        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "        å›¾åƒåµŒå…¥å‘é‡ï¼ˆ512ç»´ï¼‰\n",
    "    \"\"\"\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    return image_features.cpu().numpy()[0]\n",
    "\n",
    "def encode_text(text: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨CLIPç¼–ç æ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        text: è¾“å…¥æ–‡æœ¬\n",
    "    \n",
    "    Returns:\n",
    "        æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆ512ç»´ï¼‰\n",
    "    \"\"\"\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "    return text_features.cpu().numpy()[0]\n",
    "\n",
    "# æµ‹è¯•ç¼–ç \n",
    "sample_text = \"ä¸€åªå¯çˆ±çš„å°çŒ«\"\n",
    "text_embedding = encode_text(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Embedding shape: {text_embedding.shape}\")\n",
    "print(f\"Embedding norm: {np.linalg.norm(text_embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 è®¡ç®—è·¨æ¨¡æ€ç›¸ä¼¼åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    \n",
    "    Args:\n",
    "        vec1, vec2: å‘é‡\n",
    "    \n",
    "    Returns:\n",
    "        ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# ç¤ºä¾‹ï¼šè®¡ç®—æ–‡æœ¬-æ–‡æœ¬ç›¸ä¼¼åº¦\n",
    "texts = [\n",
    "    \"ä¸€åªå¯çˆ±çš„å°çŒ«\",\n",
    "    \"ä¸€åªå¯çˆ±çš„å°ç‹—\",\n",
    "    \"ä»Šå¤©çš„å¤©æ°”å¾ˆå¥½\",\n",
    "    \"ç¬”è®°æœ¬ç”µè„‘\"\n",
    "]\n",
    "\n",
    "query_text = \"å°çŒ«åœ¨ç©è€\"\n",
    "query_embedding = encode_text(query_text)\n",
    "\n",
    "print(f\"\\nQuery: {query_text}\\n\")\n",
    "print(\"ç›¸ä¼¼åº¦æ’å:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "similarities = []\n",
    "for text in texts:\n",
    "    text_embedding = encode_text(text)\n",
    "    sim = cosine_similarity(query_embedding, text_embedding)\n",
    "    similarities.append((text, sim))\n",
    "\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for text, sim in similarities:\n",
    "    print(f\"{text:30s} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å›¾åƒæ£€ç´¢ç³»ç»Ÿ\n",
    "\n",
    "### 3.1 åˆ›å»ºç¤ºä¾‹å›¾åƒæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¤ºä¾‹å›¾åƒé›†\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "\n",
    "# ç¤ºä¾‹å›¾åƒURLï¼ˆä½¿ç”¨å…¬å¼€çš„ç¤ºä¾‹å›¾åƒï¼‰\n",
    "sample_images = {\n",
    "    \"cat\": \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\",\n",
    "    \"dog\": \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=400\",\n",
    "    \"car\": \"https://images.unsplash.com/photo-1503376780353-7e6692767b70?w=400\",\n",
    "    \"food\": \"https://images.unsplash.com/photo-1476224203421-9ac39bcb3327?w=400\",\n",
    "    \"landscape\": \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\"\n",
    "}\n",
    "\n",
    "# ä¸‹è½½å›¾åƒ\n",
    "os.makedirs(\"data/images\", exist_ok=True)\n",
    "\n",
    "image_paths = {}\n",
    "for name, url in sample_images.items():\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, f\"data/images/{name}.jpg\")\n",
    "        image_paths[name] = f\"data/images/{name}.jpg\"\n",
    "        print(f\"Downloaded: {name}.jpg\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ„å»ºå›¾åƒç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIndex:\n",
    "    \"\"\"å›¾åƒç´¢å¼•ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = []\n",
    "        self.paths = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_image(self, image_path: str, metadata: dict = None):\n",
    "        \"\"\"æ·»åŠ å›¾åƒåˆ°ç´¢å¼•\"\"\"\n",
    "        embedding = encode_image(image_path)\n",
    "        self.embeddings.append(embedding)\n",
    "        self.paths.append(image_path)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[dict]:\n",
    "        \"\"\"æœç´¢æœ€ç›¸ä¼¼çš„å›¾åƒ\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_embedding, emb)\n",
    "            similarities.append({\n",
    "                'path': self.paths[i],\n",
    "                'similarity': sim,\n",
    "                'metadata': self.metadata[i]\n",
    "            })\n",
    "        \n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "# æ„å»ºå›¾åƒç´¢å¼•\n",
    "image_index = ImageIndex()\n",
    "\n",
    "for name, path in image_paths.items():\n",
    "    image_index.add_image(path, metadata={'category': name})\n",
    "    print(f\"Indexed: {name}\")\n",
    "\n",
    "print(f\"\\nTotal images indexed: {len(image_index.embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ä»¥å›¾æœå›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_image(query_image_path: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŸ¥è¯¢å›¾åƒæœç´¢ç›¸ä¼¼å›¾åƒ\n",
    "    \n",
    "    Args:\n",
    "        query_image_path: æŸ¥è¯¢å›¾åƒè·¯å¾„\n",
    "        top_k: è¿”å›å‰kä¸ªç»“æœ\n",
    "    \"\"\"\n",
    "    # ç¼–ç æŸ¥è¯¢å›¾åƒ\n",
    "    query_embedding = encode_image(query_image_path)\n",
    "    \n",
    "    # æœç´¢\n",
    "    results = image_index.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    fig, axes = plt.subplots(1, top_k + 1, figsize=(15, 3))\n",
    "    \n",
    "    # æŸ¥è¯¢å›¾åƒ\n",
    "    query_img = Image.open(query_image_path)\n",
    "    axes[0].imshow(query_img)\n",
    "    axes[0].set_title(\"Query Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # æœç´¢ç»“æœ\n",
    "    for i, result in enumerate(results):\n",
    "        img = Image.open(result['path'])\n",
    "        axes[i + 1].imshow(img)\n",
    "        axes[i + 1].set_title(f\"Sim: {result['similarity']:.3f}\")\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æµ‹è¯•ï¼šä½¿ç”¨ç¬¬ä¸€å¼ å›¾ä½œä¸ºæŸ¥è¯¢\n",
    "if image_paths:\n",
    "    query_path = list(image_paths.values())[0]\n",
    "    results = search_by_image(query_path, top_k=3)\n",
    "    \n",
    "    print(\"\\næœç´¢ç»“æœ:\")\n",
    "    for r in results:\n",
    "        print(f\"  {r['path']}: {r['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. è·¨æ¨¡æ€æ£€ç´¢\n",
    "\n",
    "### 4.1 ä»¥æ–‡æœå›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_text(query_text: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢æœç´¢å›¾åƒ\n",
    "    \n",
    "    Args:\n",
    "        query_text: æŸ¥è¯¢æ–‡æœ¬\n",
    "        top_k: è¿”å›å‰kä¸ªç»“æœ\n",
    "    \"\"\"\n",
    "    # ç¼–ç æŸ¥è¯¢æ–‡æœ¬\n",
    "    query_embedding = encode_text(query_text)\n",
    "    \n",
    "    # æœç´¢\n",
    "    results = image_index.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(12, 4))\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        img = Image.open(result['path'])\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"{result['metadata']['category']}\\nSim: {result['similarity']:.3f}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Query: {query_text}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æµ‹è¯•æ–‡æœ¬æŸ¥è¯¢\n",
    "queries = [\n",
    "    \"ä¸€åªå°çŒ«\",\n",
    "    \"æ±½è½¦\",\n",
    "    \"ç¾é£Ÿ\",\n",
    "    \"è‡ªç„¶é£æ™¯\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\næŸ¥è¯¢: {query}\")\n",
    "    results = search_by_text(query, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. é›†æˆGPT-4Vè§†è§‰ç†è§£\n",
    "\n",
    "### 5.1 OpenAI Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
    "client = OpenAI(api_key=\"your-api-key\")  # æ›¿æ¢ä¸ºä½ çš„API key\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"å°†å›¾åƒç¼–ç ä¸ºbase64\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def analyze_image_with_gpt4v(image_path: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨GPT-4Våˆ†æå›¾åƒ\n",
    "    \n",
    "    Args:\n",
    "        image_path: å›¾åƒè·¯å¾„\n",
    "        question: å…³äºå›¾åƒçš„é—®é¢˜\n",
    "    \n",
    "    Returns:\n",
    "        GPT-4Vçš„å›ç­”\n",
    "    \"\"\"\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "# if image_paths:\n",
    "#     test_image = list(image_paths.values())[0]\n",
    "#     answer = analyze_image_with_gpt4v(\n",
    "#         test_image,\n",
    "#         \"è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿè¯·è¯¦ç»†æè¿°ã€‚\"\n",
    "#     )\n",
    "#     print(f\"GPT-4Vå›ç­”: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 è§†è§‰é—®ç­”ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQASystem:\n",
    "    \"\"\"è§†è§‰é—®ç­”ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, image_index: ImageIndex):\n",
    "        self.image_index = image_index\n",
    "        self.client = OpenAI(api_key=\"your-api-key\")\n",
    "    \n",
    "    def query(self, text_query: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"\n",
    "        æ–‡æœ¬æŸ¥è¯¢ï¼šæ£€ç´¢ç›¸å…³å›¾åƒå¹¶å›ç­”\n",
    "        \n",
    "        Args:\n",
    "            text_query: æ–‡æœ¬æŸ¥è¯¢\n",
    "            top_k: æ£€ç´¢çš„å›¾åƒæ•°é‡\n",
    "        \n",
    "        Returns:\n",
    "            åŒ…å«æ£€ç´¢ç»“æœå’Œå›ç­”çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        # 1. ä½¿ç”¨CLIPæ£€ç´¢ç›¸å…³å›¾åƒ\n",
    "        query_embedding = encode_text(text_query)\n",
    "        image_results = self.image_index.search(query_embedding, top_k=top_k)\n",
    "        \n",
    "        # 2. ä½¿ç”¨GPT-4Våˆ†ææ£€ç´¢åˆ°çš„å›¾åƒ\n",
    "        analysis_results = []\n",
    "        for result in image_results:\n",
    "            analysis = self._analyze_image(\n",
    "                result['path'],\n",
    "                f\"ç”¨æˆ·æŸ¥è¯¢ï¼š{text_query}ã€‚è¿™å¼ å›¾ç‰‡ä¸æŸ¥è¯¢ç›¸å…³å—ï¼Ÿè¯·è¯´æ˜åŸå› ã€‚\"\n",
    "            )\n",
    "            analysis_results.append({\n",
    "                'image_path': result['path'],\n",
    "                'similarity': result['similarity'],\n",
    "                'analysis': analysis\n",
    "            })\n",
    "        \n",
    "        # 3. ç”Ÿæˆç»¼åˆå›ç­”\n",
    "        answer = self._generate_answer(text_query, analysis_results)\n",
    "        \n",
    "        return {\n",
    "            'query': text_query,\n",
    "            'retrieved_images': analysis_results,\n",
    "            'answer': answer\n",
    "        }\n",
    "    \n",
    "    def _analyze_image(self, image_path: str, question: str) -> str:\n",
    "        \"\"\"åˆ†æå•å¼ å›¾åƒ\"\"\"\n",
    "        try:\n",
    "            base64_image = encode_image_to_base64(image_path)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4-vision-preview\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": question},\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=200\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"åˆ†æå¤±è´¥: {str(e)}\"\n",
    "    \n",
    "    def _generate_answer(self, query: str, results: list) -> str:\n",
    "        \"\"\"åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”\"\"\"\n",
    "        # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è¿”å›æœ€ç›¸å…³å›¾åƒçš„åˆ†æ\n",
    "        if results:\n",
    "            best_result = max(results, key=lambda x: x['similarity'])\n",
    "            return f\"æ‰¾åˆ° {len(results)} å¼ ç›¸å…³å›¾ç‰‡ã€‚æœ€ç›¸å…³çš„æ˜¯ï¼š{best_result['analysis']}\"\n",
    "        return \"æœªæ‰¾åˆ°ç›¸å…³å›¾ç‰‡ã€‚\"\n",
    "\n",
    "# åˆ›å»ºè§†è§‰é—®ç­”ç³»ç»Ÿ\n",
    "# vqa_system = VisualQASystem(image_index)\n",
    "\n",
    "# æµ‹è¯•\n",
    "# result = vqa_system.query(\"å¯çˆ±çš„å°åŠ¨ç‰©\")\n",
    "# print(f\"æŸ¥è¯¢: {result['query']}\")\n",
    "# print(f\"å›ç­”: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. å®Œæ•´å¤šæ¨¡æ€RAGç³»ç»Ÿ\n",
    "\n",
    "### 6.1 ç³»ç»Ÿæ¶æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    "source": [
    "class MultimodalRAG:\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„å¤šæ¨¡æ€RAGç³»ç»Ÿ\n",
    "    \n",
    "    æ”¯æŒä¸¤ç§æŸ¥è¯¢æ¨¡å¼ï¼š\n",
    "    1. ä»¥æ–‡æœå›¾ï¼šæ–‡æœ¬æŸ¥è¯¢ â†’ å›¾åƒæ£€ç´¢ â†’ è§†è§‰ç†è§£ â†’ å›ç­”\n",
    "    2. ä»¥å›¾æœå›¾ï¼šå›¾åƒæŸ¥è¯¢ â†’ å›¾åƒæ£€ç´¢ â†’ è§†è§‰ç†è§£ â†’ å›ç­”\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_vision: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_vision: æ˜¯å¦ä½¿ç”¨GPT-4Vè¿›è¡Œè§†è§‰ç†è§£\n",
    "        \"\"\"\n",
    "        self.image_index = ImageIndex()\n",
    "        self.use_vision = use_vision\n",
    "        \n",
    "        if use_vision:\n",
    "            self.client = OpenAI(api_key=\"your-api-key\")\n",
    "    \n",
    "    def index_images(self, image_paths: List[str], metadata: List[dict] = None):\n",
    "        \"\"\"æ‰¹é‡ç´¢å¼•å›¾åƒ\"\"\"\n",
    "        for i, path in enumerate(image_paths):\n",
    "            meta = metadata[i] if metadata else {}\n",
    "            self.image_index.add_image(path, meta)\n",
    "        print(f\"Indexed {len(image_paths)} images\")\n",
    "    \n",
    "    def query_with_text(self, query: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"æ–‡æœ¬æŸ¥è¯¢\"\"\"\n",
    "        # 1. æ£€ç´¢\n",
    "        query_embedding = encode_text(query)\n",
    "        results = self.image_index.search(query_embedding, top_k=top_k)\n",
    "        \n",
    "        # 2. è§†è§‰ç†è§£ï¼ˆå¯é€‰ï¼‰\n",
    "        if self.use_vision:\n",
    "            enhanced_results = self._enhance_with_vision(query, results)\n",
    "        else:\n",
    "            enhanced_results = results\n",
    "        \n",
    "        # 3. è¿”å›\n",
    "        return {\n",
    "            'query': query,\n",
    "            'mode': 'text-to-image',\n",
    "            'results': enhanced_results\n",
    "        }\n",
    "    \n",
    "    def query_with_image(self, query_image_path: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"å›¾åƒæŸ¥è¯¢\"\"\"\n",
    "        # 1. æ£€ç´¢\n",
    "        query_embedding = encode_image(query_image_path)\n",
    "        results = self.image_index.search(query_embedding, top_k=top_k)\n",
    "        \n",
    "        # 2. è¿”å›\n",
    "        return {\n",
    "            'query': query_image_path,\n",
    "            'mode': 'image-to-image',\n",
    "            'results': results\n",
    "        }\n",
    "    \n",
    "    def _enhance_with_vision(self, query: str, results: list) -> list:\n",
    "        \"\"\"ä½¿ç”¨GPT-4Vå¢å¼ºç»“æœ\"\"\"\n",
    "        enhanced = []\n",
    "        for result in results:\n",
    "            analysis = self._analyze_image(result['path'], query)\n",
    "            enhanced.append({\n",
    "                **result,\n",
    "                'analysis': analysis\n",
    "            })\n",
    "        return enhanced\n",
    "    \n",
    "    def _analyze_image(self, image_path: str, query: str) -> str:\n",
    "        \"\"\"åˆ†æå›¾åƒ\"\"\"\n",
    "        try:\n",
    "            base64_image = encode_image_to_base64(image_path)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4-vision-preview\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": f\"ç”¨æˆ·æŸ¥è¯¢ï¼š{query}\\nè¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸æŸ¥è¯¢çš„å…³ç³»ã€‚\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            return \"è§†è§‰åˆ†æå¤±è´¥\"\n",
    "\n",
    "# åˆ›å»ºç³»ç»Ÿ\n",
    "rag_system = MultimodalRAG(use_vision=False)  # æš‚æ—¶å…³é—­GPT-4V\n",
    "\n",
    "# ç´¢å¼•å›¾åƒ\n",
    "if image_paths:\n",
    "    paths = list(image_paths.values())\n",
    "    metadatas = [{'category': k} for k in image_paths.keys()]\n",
    "    rag_system.index_images(paths, metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 æµ‹è¯•ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ–‡æœ¬æŸ¥è¯¢\n",
    "test_queries = [\n",
    "    \"å¯çˆ±çš„å® ç‰©\",\n",
    "    \"äº¤é€šå·¥å…·\",\n",
    "    \"ç¾é£Ÿ\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = rag_system.query_with_text(query, top_k=2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æŸ¥è¯¢: {result['query']}\")\n",
    "    print(f\"æ¨¡å¼: {result['mode']}\")\n",
    "    print(f\"\\nç»“æœ:\")\n",
    "    \n",
    "    for i, r in enumerate(result['results'], 1):\n",
    "        print(f\"  {i}. {r['metadata']['category']}: {r['similarity']:.4f}\")\n",
    "        if 'analysis' in r:\n",
    "            print(f\"     åˆ†æ: {r['analysis'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹ 1ï¼šæ”¹è¿›å›¾åƒæ£€ç´¢\n",
    "\n",
    "æ·»åŠ å›¾åƒå…ƒæ•°æ®ï¼ˆå¦‚æ ‡ç­¾ã€æè¿°ï¼‰ï¼Œå¹¶å®ç°åŸºäºå…ƒæ•°æ®çš„è¿‡æ»¤æœç´¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®ç°å¸¦è¿‡æ»¤çš„å›¾åƒæ£€ç´¢\n",
    "# æç¤ºï¼šä¿®æ”¹ ImageIndex ç±»ï¼Œæ·»åŠ  filter å‚æ•°\n",
    "\n",
    "class ImageIndexWithFilter(ImageIndex):\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 5, \n",
    "               filters: dict = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        æ·»åŠ è¿‡æ»¤åŠŸèƒ½\n",
    "        \n",
    "        Args:\n",
    "            filters: è¿‡æ»¤æ¡ä»¶ï¼Œå¦‚ {'category': 'animal'}\n",
    "        \"\"\"\n",
    "        # å®ç°è¿‡æ»¤é€»è¾‘\n",
    "        pass\n",
    "\n",
    "# æµ‹è¯•\n",
    "# filtered_index = ImageIndexWithFilter()\n",
    "# ... æ·»åŠ å¸¦å…ƒæ•°æ®çš„å›¾åƒ\n",
    "# results = filtered_index.search(query_emb, filters={'category': 'animal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 2ï¼šå›¾æ–‡æ··åˆæ£€ç´¢\n",
    "\n",
    "å®ç°ä¸€ä¸ªåŒæ—¶æ¥å—å›¾åƒ+æ–‡æœ¬ä½œä¸ºè¾“å…¥çš„æ··åˆæ£€ç´¢ç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®ç°å›¾æ–‡æ··åˆæ£€ç´¢\n",
    "# æç¤ºï¼šå°†å›¾åƒå’Œæ–‡æœ¬åµŒå…¥åŠ æƒèåˆ\n",
    "\n",
    "def hybrid_query(rag_system: MultimodalRAG, \n",
    "                image_path: str, \n",
    "                text: str, \n",
    "                image_weight: float = 0.5,\n",
    "                top_k: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    æ··åˆæŸ¥è¯¢ï¼šç»“åˆå›¾åƒå’Œæ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        image_path: å‚è€ƒå›¾åƒ\n",
    "        text: æ–‡æœ¬æè¿°\n",
    "        image_weight: å›¾åƒæƒé‡ï¼ˆ0-1ï¼‰\n",
    "        top_k: è¿”å›ç»“æœæ•°\n",
    "    \n",
    "    Returns:\n",
    "        æ£€ç´¢ç»“æœ\n",
    "    \"\"\"\n",
    "    # 1. ç¼–ç \n",
    "    image_emb = encode_image(image_path)\n",
    "    text_emb = encode_text(text)\n",
    "    \n",
    "    # 2. åŠ æƒèåˆ\n",
    "    # hybrid_emb = image_emb * image_weight + text_emb * (1 - image_weight)\n",
    "    # æˆ–ä½¿ç”¨å…¶ä»–èåˆç­–ç•¥\n",
    "    \n",
    "    # 3. æ£€ç´¢\n",
    "    # results = rag_system.image_index.search(hybrid_emb, top_k)\n",
    "    \n",
    "    # return results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 3ï¼šäº§å“æ¨èç³»ç»Ÿ\n",
    "\n",
    "ä¸ºç”µå•†å¹³å°æ„å»ºä¸€ä¸ªäº§å“å›¾åƒæ¨èç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®ç°äº§å“æ¨èç³»ç»Ÿ\n",
    "# åŠŸèƒ½ï¼š\n",
    "# 1. ä»¥å›¾æœç›¸ä¼¼äº§å“\n",
    "# 2. ä»¥æ–‡æœäº§å“\n",
    "# 3. äº§å“å¯¹æ¯”\n",
    "\n",
    "class ProductRecommender:\n",
    "    \"\"\"äº§å“æ¨èç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rag = MultimodalRAG()\n",
    "    \n",
    "    def find_similar_products(self, product_image: str, top_k: int = 5):\n",
    "        \"\"\"æ‰¾ç›¸ä¼¼äº§å“\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def search_by_description(self, description: str, top_k: int = 5):\n",
    "        \"\"\"æŒ‰æè¿°æœç´¢\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compare_products(self, product_images: List[str]) -> dict:\n",
    "        \"\"\"å¯¹æ¯”å¤šä¸ªäº§å“\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æ€»ç»“\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **CLIPæ¨¡å‹**ï¼šå®ç°å›¾åƒå’Œæ–‡æœ¬çš„è·¨æ¨¡æ€ç†è§£\n",
    "2. **å›¾åƒæ£€ç´¢**ï¼šä»¥å›¾æœå›¾ã€ä»¥æ–‡æœå›¾\n",
    "3. **è§†è§‰ç†è§£**ï¼šGPT-4Væä¾›æ·±åº¦åˆ†æ\n",
    "4. **å¤šæ¨¡æ€RAG**ï¼šç»“åˆæ£€ç´¢å’Œè§†è§‰ç†è§£\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- æ¢ç´¢å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLLaVA, BLIPï¼‰\n",
    "- ä¼˜åŒ–æ£€ç´¢æ€§èƒ½ï¼ˆFaisså‘é‡ç´¢å¼•ï¼‰\n",
    "- æ„å»ºæ›´å¤æ‚çš„åº”ç”¨\n",
    "\n",
    "### å‚è€ƒèµ„æº\n",
    "\n",
    "- [CLIPè®ºæ–‡](https://arxiv.org/abs/2103.00020)\n",
    "- [OpenAI Vision API](https://platform.openai.com/docs/guides/vision)\n",
    "- [LangChainå¤šæ¨¡æ€RAG](https://python.langchain.com/docs/use_cases/multimodal/)\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆç¬¬16ç« çš„å­¦ä¹ ï¼** ğŸ‰\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†å¤šæ¨¡æ€RAGçš„æ ¸å¿ƒæŠ€æœ¯ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
