{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬7ç« ï¼šé«˜çº§åˆ†å—ç­–ç•¥\n",
    "\n",
    "> åˆé€‚çš„åˆ†å—ç­–ç•¥å¯ä»¥è®©æ£€ç´¢è´¨é‡æå‡5-10%ã€‚æœ¬ç« å°†å¸¦ä½ æŒæ¡å¤šç§é«˜çº§åˆ†å—æŠ€æœ¯ã€‚\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "æœ¬Notebookå°†å¸¦ä½ ï¼š\n",
    "- âœ… ç†è§£åˆ†å—ç­–ç•¥çš„é‡è¦æ€§\n",
    "- âœ… æŒæ¡5+ç§é«˜çº§åˆ†å—æ–¹æ³•\n",
    "- âœ… å­¦ä¼šä¼˜åŒ–åˆ†å—å‚æ•°\n",
    "- âœ… å¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœ\n",
    "- âœ… å®æˆ˜åº”ç”¨æœ€ä½³åˆ†å—æ–¹æ¡ˆ\n",
    "\n",
    "## é¢„è®¡æ—¶é—´\n",
    "\n",
    "- åˆ†å—ç­–ç•¥åŸç†ï¼š30åˆ†é’Ÿ\n",
    "- é«˜çº§åˆ†å—æ–¹æ³•ï¼š60åˆ†é’Ÿ\n",
    "- æ•ˆæœå¯¹æ¯”ï¼š40åˆ†é’Ÿ\n",
    "- å®æˆ˜åº”ç”¨ï¼š30åˆ†é’Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(\"æ£€æŸ¥ç¯å¢ƒ...\")\n",
    "print(f\"NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(\"\\nç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆ†å—ç­–ç•¥çš„é‡è¦æ€§\n",
    "\n",
    "### 2.1 ä¸ºä»€ä¹ˆåˆ†å—è¿™ä¹ˆé‡è¦ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹ï¼šä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”\n",
    "\n",
    "sample_text = \"\"\"\n",
    "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚\n",
    "\n",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸï¼Œä¸“æ³¨äºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚\n",
    "æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚\n",
    "\n",
    "è¿™äº›æŠ€æœ¯å·²ç»å¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚\n",
    "\"\"\"\n",
    "\n",
    "print(\"ç¤ºä¾‹æ–‡æ¡£ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "print(sample_text)\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ€»å­—ç¬¦æ•°: {len(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŸºç¡€åˆ†å—ç­–ç•¥\n",
    "\n",
    "### 3.1 å›ºå®šé•¿åº¦åˆ†å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"\n",
    "    æ–‡æ¡£å—\n",
    "    \"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class FixedLengthChunker:\n",
    "    \"\"\"\n",
    "    å›ºå®šé•¿åº¦åˆ†å—å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 200, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split(self, text: str) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        æŒ‰å›ºå®šé•¿åº¦åˆ†å—\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append(Chunk(\n",
    "                    content=chunk_text,\n",
    "                    metadata={\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"start\": start,\n",
    "                        \"end\": end,\n",
    "                        \"length\": len(chunk_text)\n",
    "                    }\n",
    "                ))\n",
    "                chunk_id += 1\n",
    "            \n",
    "            start = end - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# æµ‹è¯•å›ºå®šé•¿åº¦åˆ†å—\n",
    "chunker = FixedLengthChunker(chunk_size=100, chunk_overlap=20)\n",
    "chunks = chunker.split(sample_text)\n",
    "\n",
    "print(\"\\nå›ºå®šé•¿åº¦åˆ†å—ç»“æœï¼š\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"åˆ†å—æ•°é‡: {len(chunks)}\\n\")\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"å— {chunk.metadata['chunk_id']}: \")\n",
    "    print(f\"  é•¿åº¦: {chunk.metadata['length']} å­—ç¬¦\")\n",
    "    print(f\"  å†…å®¹: {chunk.content[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é«˜çº§åˆ†å—ç­–ç•¥\n",
    "\n",
    "### 4.1 æŒ‰æ®µè½åˆ†å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParagraphChunker:\n",
    "    \"\"\"\n",
    "    æŒ‰æ®µè½åˆ†å—\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_length: int = 50):\n",
    "        self.min_length = min_length\n",
    "    \n",
    "    def split(self, text: str) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        æŒ‰æ®µè½åˆ‡åˆ†\n",
    "        \"\"\"\n",
    "        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†æ®µ\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n",
    "        \n",
    "        chunks = []\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            para = para.strip()\n",
    "            if len(para) >= self.min_length:\n",
    "                chunks.append(Chunk(\n",
    "                    content=para,\n",
    "                    metadata={\n",
    "                        \"chunk_id\": i,\n",
    "                        \"type\": \"paragraph\",\n",
    "                        \"length\": len(para)\n",
    "                    }\n",
    "                ))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# æµ‹è¯•æ®µè½åˆ†å—\n",
    "para_chunker = ParagraphChunker(min_length=30)\n",
    "para_chunks = para_chunker.split(sample_text)\n",
    "\n",
    "print(\"\\næŒ‰æ®µè½åˆ†å—ç»“æœï¼š\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"åˆ†å—æ•°é‡: {len(para_chunks)}\\n\")\n",
    "\n",
    "for chunk in para_chunks:\n",
    "    print(f\"å— {chunk.metadata['chunk_id']}:\")\n",
    "    print(f\"  ç±»å‹: {chunk.metadata['type']}\")\n",
    "    print(f\"  é•¿åº¦: {chunk.metadata['length']} å­—ç¬¦\")\n",
    "    print(f\"  å†…å®¹: {chunk.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æŒ‰å¥å­åˆ†å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunker:\n",
    "    \"\"\"\n",
    "    æŒ‰å¥å­åˆ†å—\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sentences_per_chunk: int = 3, overlap: int = 1):\n",
    "        self.sentences_per_chunk = sentences_per_chunk\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def split(self, text: str) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        æŒ‰å¥å­åˆ‡åˆ†ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´ï¼‰\n",
    "        \"\"\"\n",
    "        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰\n",
    "        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', text)\n",
    "        \n",
    "        # é‡ç»„å¥å­\n",
    "        full_sentences = []\n",
    "        for i in range(0, len(sentences) - 1, 2):\n",
    "            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                full_sentences.append(sentence)\n",
    "        \n",
    "        # æŒ‰æŒ‡å®šå¥æ•°åˆ†å—\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for i in range(0, len(full_sentences), self.sentences_per_chunk - self.overlap):\n",
    "            chunk_sentences = full_sentences[i:i + self.sentences_per_chunk]\n",
    "            if chunk_sentences:\n",
    "                chunk_text = ' '.join(chunk_sentences)\n",
    "                chunks.append(Chunk(\n",
    "                    content=chunk_text,\n",
    "                    metadata={\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"type\": \"sentence_group\",\n",
    "                        \"num_sentences\": len(chunk_sentences)\n",
    "                    }\n",
    "                ))\n",
    "                chunk_id += 1\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# æµ‹è¯•å¥å­åˆ†å—\n",
    "sent_chunker = SentenceChunker(sentences_per_chunk=2, overlap=0)\n",
    "sent_chunks = sent_chunker.split(sample_text)\n",
    "\n",
    "print(\"\\næŒ‰å¥å­åˆ†å—ç»“æœï¼š\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"åˆ†å—æ•°é‡: {len(sent_chunks)}\\n\")\n",
    "\n",
    "for chunk in sent_chunks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª\n",
    "    print(f\"å— {chunk.metadata['chunk_id']}:\")\n",
    "    print(f\"  å¥å­æ•°: {chunk.metadata['num_sentences']}\")\n",
    "    print(f\"  å†…å®¹: {chunk.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 é€’å½’åˆ†å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveChunker:\n",
    "    \"\"\"\n",
    "    é€’å½’åˆ†å—å™¨\n",
    "    \n",
    "    å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦ï¼Œä¼˜å…ˆä½¿ç”¨æ›´é«˜çº§åˆ«çš„åˆ†éš”ç¬¦\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 200, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        # æŒ‰ä¼˜å…ˆçº§æ’åºçš„åˆ†éš”ç¬¦\n",
    "        self.separators = [\"\\n\\n\", \"\\n\", \"ã€‚\", \"\", \" \"]\n",
    "    \n",
    "    def split(self, text: str, separators: List[str] = None) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        é€’å½’åˆ†å—\n",
    "        \"\"\"\n",
    "        if separators is None:\n",
    "            separators = self.separators\n",
    "        \n",
    "        # æœ€åçš„å›é€€ï¼šå›ºå®šé•¿åº¦åˆ†å—\n",
    "        if not separators:\n",
    "            return FixedLengthChunker(self.chunk_size, self.chunk_overlap).split(text)\n",
    "        \n",
    "        # ä½¿ç”¨å½“å‰åˆ†éš”ç¬¦\n",
    "        separator = separators[0]\n",
    "        remaining_separators = separators[1:]\n",
    "        \n",
    "        # åˆ†å‰²æ–‡æœ¬\n",
    "        splits = text.split(separator)\n",
    "        \n",
    "        # æ£€æŸ¥æ¯æ®µå¤§å°\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for split in splits:\n",
    "            # åŠ ä¸Šåˆ†éš”ç¬¦\n",
    "            if current_chunk:\n",
    "                split_with_sep = separator + split\n",
    "            else:\n",
    "                split_with_sep = split\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡chunk_size\n",
    "            if len(current_chunk) + len(split_with_sep) < self.chunk_size:\n",
    "                current_chunk += split_with_sep\n",
    "            else:\n",
    "                # ä¿å­˜å½“å‰å—\n",
    "                if current_chunk:\n",
    "                    chunks.append(Chunk(\n",
    "                        content=current_chunk.strip(),\n",
    "                        metadata={\"chunk_id\": chunk_id, \"separator\": separator}\n",
    "                    ))\n",
    "                    chunk_id += 1\n",
    "                \n",
    "                # å¤„ç†è¿‡é•¿çš„splitï¼ˆé€’å½’ä½¿ç”¨ä¸‹ä¸€çº§åˆ†éš”ç¬¦ï¼‰\n",
    "                if len(split) > self.chunk_size:\n",
    "                    sub_chunks = self.split(split, remaining_separators)\n",
    "                    chunks.extend(sub_chunks)\n",
    "                    current_chunk = \"\"\n",
    "                else:\n",
    "                    current_chunk = split\n",
    "        \n",
    "        # ä¿å­˜æœ€åä¸€å—\n",
    "        if current_chunk:\n",
    "            chunks.append(Chunk(\n",
    "                content=current_chunk.strip(),\n",
    "                metadata={\"chunk_id\": chunk_id, \"separator\": separator}\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# æµ‹è¯•é€’å½’åˆ†å—\n",
    "recursive_chunker = RecursiveChunker(chunk_size=150, chunk_overlap=30)\n",
    "recursive_chunks = recursive_chunker.split(sample_text)\n",
    "\n",
    "print(\"\\né€’å½’åˆ†å—ç»“æœï¼š\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"åˆ†å—æ•°é‡: {len(recursive_chunks)}\\n\")\n",
    "\n",
    "for chunk in recursive_chunks:\n",
    "    print(f\"å— {chunk.metadata['chunk_id']}:\")\n",
    "    print(f\"  åˆ†éš”ç¬¦: {repr(chunk.metadata['separator'])}\")\n",
    "    print(f\"  å†…å®¹: {chunk.content[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åˆ†å—ç­–ç•¥å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥\n",
    "\n",
    "chunkers = {\n",
    "    \"å›ºå®šé•¿åº¦\": FixedLengthChunker(chunk_size=100, chunk_overlap=20),\n",
    "    \"æŒ‰æ®µè½\": ParagraphChunker(min_length=30),\n",
    "    \"æŒ‰å¥å­\": SentenceChunker(sentences_per_chunk=2, overlap=0),\n",
    "    \"é€’å½’åˆ†å—\": RecursiveChunker(chunk_size=150, chunk_overlap=30),\n",
    "}\n",
    "\n",
    "print(\"\\nåˆ†å—ç­–ç•¥å¯¹æ¯”ï¼š\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'ç­–ç•¥':<15} {'å—æ•°é‡':<10} {'å¹³å‡é•¿åº¦':<15} {'æœ€å°é•¿åº¦':<15} {'æœ€å¤§é•¿åº¦':<15}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, chunker in chunkers.items():\n",
    "    chunks = chunker.split(sample_text)\n",
    "    \n",
    "    if chunks:\n",
    "        lengths = [c.metadata.get('length', len(c.content)) for c in chunks]\n",
    "        avg_length = np.mean(lengths)\n",
    "        min_length = np.min(lengths)\n",
    "        max_length = np.max(lengths)\n",
    "        \n",
    "        print(f\"{name:<15} {len(chunks):<10} {avg_length:<15.1f} {min_length:<15.0f} {max_length:<15.0f}\")\n",
    "    else:\n",
    "        print(f\"{name:<15} {'æ— å—':<10} {'-':<15} {'-':<15} {'-':<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å®æˆ˜ï¼šé€‰æ‹©æœ€ä½³åˆ†å—ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_chunking_strategy(\n",
    "    text_type: str,\n",
    "    query_type: str,\n",
    "    doc_length: int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    æ ¹æ®åœºæ™¯æ¨èåˆ†å—ç­–ç•¥\n",
    "    \n",
    "    Args:\n",
    "        text_type: æ–‡æœ¬ç±»å‹ (article/code/conversation/report)\n",
    "        query_type: æŸ¥è¯¢ç±»å‹ (specific/broad)\n",
    "        doc_length: æ–‡æ¡£é•¿åº¦\n",
    "    \n",
    "    Returns:\n",
    "        æ¨èç­–ç•¥\n",
    "    \"\"\"\n",
    "    # ä»£ç æ–‡æ¡£ï¼šæŒ‰å‡½æ•°/ç±»åˆ†å—\n",
    "    if text_type == \"code\":\n",
    "        return \"ä½¿ç”¨ä»£ç ç‰¹å®šçš„ASTåˆ†å—\"\n",
    "    \n",
    "    # å¯¹è¯ï¼šæŒ‰å¯¹è¯è½®æ¬¡\n",
    "    if text_type == \"conversation\":\n",
    "        return \"æŒ‰å¯¹è¯è½®æ¬¡åˆ†å—\"\n",
    "    \n",
    "    # æŠ€æœ¯æŠ¥å‘Šï¼šæŒ‰ç« èŠ‚\n",
    "    if text_type == \"report\":\n",
    "        return \"æŒ‰ç« èŠ‚/æ®µè½åˆ†å—ï¼ˆé€’å½’åˆ†å—ï¼‰\"\n",
    "    \n",
    "    # æ–‡ç« \n",
    "    if text_type == \"article\":\n",
    "        if query_type == \"specific\":\n",
    "            # å…·ä½“æŸ¥è¯¢ï¼šå°å—\n",
    "            return \"æŒ‰å¥å­åˆ†å—ï¼Œæ¯å—2-3å¥\"\n",
    "        else:\n",
    "            # å¹¿æ³›æŸ¥è¯¢ï¼šå¤§å—\n",
    "            return \"æŒ‰æ®µè½åˆ†å—ï¼Œä¿æŒä¸Šä¸‹æ–‡\"\n",
    "    \n",
    "    # é»˜è®¤\n",
    "    return \"é€’å½’åˆ†å—ï¼Œchunk_size=512, overlap=50\"\n",
    "\n",
    "# æµ‹è¯•æ¨è\n",
    "scenarios = [\n",
    "    {\"text_type\": \"article\", \"query_type\": \"specific\", \"doc_length\": 2000},\n",
    "    {\"text_type\": \"code\", \"query_type\": \"specific\", \"doc_length\": 5000},\n",
    "    {\"text_type\": \"report\", \"query_type\": \"broad\", \"doc_length\": 10000},\n",
    "]\n",
    "\n",
    "print(\"\\nåˆ†å—ç­–ç•¥æ¨èï¼š\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, scenario in enumerate(scenarios, 1):\n",
    "    recommendation = recommend_chunking_strategy(**scenario)\n",
    "    print(f\"\\nåœºæ™¯{i}: {scenario}\")\n",
    "    print(f\"æ¨è: {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹ 1ï¼šè¯­ä¹‰åˆ†å—\n",
    "\n",
    "å®ç°ä¸€ä¸ªåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†å—å™¨ï¼Œå½“ç›¸é‚»å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼æ—¶è¿›è¡Œåˆ‡åˆ†ã€‚\n",
    "\n",
    "**æç¤º**ï¼š\n",
    "- è®¡ç®—å¥å­çš„åµŒå…¥å‘é‡\n",
    "- è®¡ç®—ç›¸é‚»å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "- åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ‡åˆ†\n",
    "\n",
    "### ç»ƒä¹ 2ï¼šåŠ¨æ€chunk_size\n",
    "\n",
    "æ”¹è¿›é€’å½’åˆ†å—å™¨ï¼Œæ ¹æ®æ–‡æ¡£ç±»å‹è‡ªåŠ¨è°ƒæ•´chunk_sizeã€‚\n",
    "\n",
    "### ç»ƒä¹ 3ï¼šåˆ†å—è´¨é‡è¯„ä¼°\n",
    "\n",
    "å®ç°ä¸€ä¸ªè¯„ä¼°å‡½æ•°ï¼Œå¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥çš„æ£€ç´¢è´¨é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æ€»ç»“\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **åˆ†å—é‡è¦æ€§**ï¼šç›´æ¥å½±å“æ£€ç´¢è´¨é‡\n",
    "2. **å›ºå®šé•¿åº¦åˆ†å—**ï¼šç®€å•ä½†å¯èƒ½ç ´åè¯­ä¹‰\n",
    "3. **æŒ‰æ®µè½/å¥å­**ï¼šä¿æŒè¯­ä¹‰å®Œæ•´æ€§\n",
    "4. **é€’å½’åˆ†å—**ï¼šçµæ´»é€‚åº”ä¸åŒæ–‡æ¡£ç»“æ„\n",
    "5. **åœºæ™¯é€‰æ‹©**ï¼šæ ¹æ®æ–‡æœ¬ç±»å‹å’ŒæŸ¥è¯¢ç±»å‹é€‰æ‹©\n",
    "\n",
    "### åˆ†å—ç­–ç•¥é€‰æ‹©æŒ‡å—\n",
    "\n",
    "| åœºæ™¯ | æ¨èç­–ç•¥ |\n",
    "|------|----------|\n",
    "| æ–°é—»æ–‡ç«  | æŒ‰æ®µè½åˆ†å— |\n",
    "| æŠ€æœ¯æ–‡æ¡£ | é€’å½’åˆ†å—ï¼ˆæ ‡é¢˜ï¼‰ |\n",
    "| ä»£ç  | ASTåˆ†å— |\n",
    "| å¯¹è¯ | æŒ‰è½®æ¬¡åˆ†å— |\n",
    "| é•¿æ–‡æ¡£ | å›ºå®šé•¿åº¦+é‡å  |\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- ç¬¬8ç« ï¼šæŸ¥è¯¢å¢å¼ºæŠ€æœ¯\n",
    "- å­¦ä¹ å¦‚ä½•ä¼˜åŒ–æŸ¥è¯¢è´¨é‡\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆç¬¬7ç« çš„å­¦ä¹ ï¼** ğŸ‰\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†å¤šç§é«˜çº§åˆ†å—ç­–ç•¥ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
