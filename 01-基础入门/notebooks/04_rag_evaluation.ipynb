{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬4ç« ï¼šRAGè¯„ä¼°åŸºç¡€\n",
    "\n",
    "> ç”¨ç§‘å­¦çš„æ–¹æ³•è¡¡é‡å’Œæ”¹è¿›RAGç³»ç»Ÿ\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "æœ¬Notebookå°†å¸¦ä½ ï¼š\n",
    "- âœ… ç†è§£RAGè¯„ä¼°çš„ä¸‰ä¸ªç»´åº¦\n",
    "- âœ… å®ç°æ£€ç´¢è´¨é‡è¯„ä¼°æŒ‡æ ‡\n",
    "- âœ… å®ç°ç”Ÿæˆè´¨é‡è¯„ä¼°æŒ‡æ ‡\n",
    "- âœ… å»ºç«‹å®Œæ•´çš„è¯„ä¼°æµç¨‹\n",
    "- âœ… åŸºäºè¯„ä¼°ç»“æœä¼˜åŒ–ç³»ç»Ÿ\n",
    "\n",
    "## é¢„è®¡æ—¶é—´\n",
    "\n",
    "- æ£€ç´¢è´¨é‡è¯„ä¼°ï¼š30åˆ†é’Ÿ\n",
    "- ç”Ÿæˆè´¨é‡è¯„ä¼°ï¼š30åˆ†é’Ÿ\n",
    "- å®Œæ•´è¯„ä¼°æµç¨‹ï¼š30åˆ†é’Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAGè¯„ä¼°çš„ä¸‰ä¸ªç»´åº¦\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          RAGè¯„ä¼°ç»´åº¦                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                         â”‚\n",
    "â”‚  1. æ£€ç´¢è´¨é‡ (Retrieval Quality)       â”‚\n",
    "â”‚     - æ‰¾åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³ï¼Ÿ              â”‚\n",
    "â”‚     - ç›¸å…³æ–‡æ¡£æ˜¯å¦æ’åœ¨å‰é¢ï¼Ÿ            â”‚\n",
    "â”‚     æŒ‡æ ‡ï¼šHit Rate, MRR, Precision@K    â”‚\n",
    "â”‚                                         â”‚\n",
    "â”‚  2. ç”Ÿæˆè´¨é‡ (Generation Quality)      â”‚\n",
    "â”‚     - ç­”æ¡ˆæ˜¯å¦å‡†ç¡®ï¼Ÿ                    â”‚\n",
    "â”‚     - æ˜¯å¦åŸºäºæ£€ç´¢æ–‡æ¡£ï¼Ÿ                â”‚\n",
    "â”‚     æŒ‡æ ‡ï¼šFaithfulness, Relevancy       â”‚\n",
    "â”‚                                         â”‚\n",
    "â”‚  3. ç³»ç»Ÿæ€§èƒ½ (System Performance)      â”‚\n",
    "â”‚     - å“åº”é€Ÿåº¦å¦‚ä½•ï¼Ÿ                    â”‚\n",
    "â”‚     - èƒ½å¤„ç†å¤šå°‘è¯·æ±‚ï¼Ÿ                  â”‚\n",
    "â”‚     æŒ‡æ ‡ï¼šå»¶è¿Ÿ, QPS, æˆæœ¬               â”‚\n",
    "â”‚                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ£€ç´¢è´¨é‡è¯„ä¼°\n",
    "\n",
    "### 2.1 å‡†å¤‡æµ‹è¯•æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®é›†\n",
    "test_dataset = [\n",
    "    {\n",
    "        'query': 'Pythonæ˜¯ä»€ä¹ˆï¼Ÿ',\n",
    "        'relevant_docs': ['doc1.txt'],  # ç›¸å…³æ–‡æ¡£çš„æ–‡ä»¶å\n",
    "        'answer': 'Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€'\n",
    "    },\n",
    "    {\n",
    "        'query': 'RAGæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ',\n",
    "        'relevant_docs': ['doc2.txt'],\n",
    "        'answer': 'RAGå¯ä»¥å‡å°‘å¹»è§‰'\n",
    "    },\n",
    "    {\n",
    "        'query': 'å¦‚ä½•ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼Ÿ',\n",
    "        'relevant_docs': ['doc3.txt'],\n",
    "        'answer': 'ä½¿ç”¨HNSWç®—æ³•'\n",
    "    },\n",
    "    {\n",
    "        'query': 'æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«',\n",
    "        'relevant_docs': [],  # æ²¡æœ‰ç›¸å…³æ–‡æ¡£\n",
    "        'answer': 'çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ­¤ä¿¡æ¯'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)} ä¸ªæŸ¥è¯¢\")\n",
    "for i, item in enumerate(test_dataset, 1):\n",
    "    print(f\"\\n{i}. {item['query']}\")\n",
    "    print(f\"   ç›¸å…³æ–‡æ¡£: {item['relevant_docs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hit Rateï¼ˆå‘½ä¸­ç‡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(\n",
    "    retrieved_docs_list: List[List[str]],\n",
    "    relevant_docs_list: List[List[str]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—Hit Rate\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs_list: æ¯ä¸ªæŸ¥è¯¢æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨\n",
    "        relevant_docs_list: æ¯ä¸ªæŸ¥è¯¢çš„çœŸå®ç›¸å…³æ–‡æ¡£åˆ—è¡¨\n",
    "    \n",
    "    Returns:\n",
    "        Hit Rate (0-1)\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    \n",
    "    for retrieved, relevant in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªç›¸å…³æ–‡æ¡£\n",
    "        if any(doc in retrieved for doc in relevant):\n",
    "            hits += 1\n",
    "    \n",
    "    return hits / len(retrieved_docs_list)\n",
    "\n",
    "# æµ‹è¯•Hit Rate\n",
    "retrieved_docs_list = [\n",
    "    ['doc1.txt', 'doc2.txt'],  # ç¬¬1ä¸ªæŸ¥è¯¢ç»“æœ\n",
    "    ['doc2.txt'],              # ç¬¬2ä¸ªæŸ¥è¯¢ç»“æœ\n",
    "    ['doc3.txt', 'doc1.txt'],  # ç¬¬3ä¸ªæŸ¥è¯¢ç»“æœ\n",
    "    []                       # ç¬¬4ä¸ªæŸ¥è¯¢ç»“æœ\n",
    "]\n",
    "relevant_docs_list = [\n",
    "    ['doc1.txt'],\n",
    "    ['doc2.txt'],\n",
    "    ['doc3.txt'],\n",
    "    []\n",
    "]\n",
    "hit_rate = calculate_hit_rate(retrieved_docs_list, relevant_docs_list)\n",
    "\n",
    "print(f\"\\nHit Rateè®¡ç®—ç»“æœ:\")\n",
    "print(f\"  æŸ¥è¯¢æ€»æ•°: {len(retrieved_docs_list)}\")\n",
    "print(f\"  å‘½ä¸­æ•°: {sum(1 for r, rel in zip(retrieved_docs_list, relevant_docs_list) if any(d in r for d in rel))}\")\n",
    "print(f\"  Hit Rate: {hit_rate:.2%}\")\n",
    "print(f\"\\nè§£è¯»: æ¯ {1/hit_rate:.0f} ä¸ªæŸ¥è¯¢ä¸­ï¼Œæœ‰1ä¸ªæŸ¥è¯¢æ‰¾ä¸åˆ°ç›¸å…³æ–‡æ¡£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 MRRï¼ˆå¹³å‡å€’æ•°æ’åï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(\n",
    "    retrieved_docs_list: List[List[str]],\n",
    "    relevant_docs_list: List[List[str]]\n",
    "    top_k: int = 10\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—MRR (Mean Reciprocal Rank)\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs_list: æ£€ç´¢ç»“æœåˆ—è¡¨\n",
    "        relevant_docs_list: ç›¸å…³æ–‡æ¡£åˆ—è¡¨\n",
    "        top_k: è€ƒè™‘å‰Kä¸ªç»“æœ\n",
    "    \n",
    "    Returns:\n",
    "        MRRåˆ†æ•° (0-1)\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for retrieved, relevant in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        if not relevant:\n",
    "            continue  # æ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œè·³è¿‡\n",
    "        \n",
    "        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®\n",
    "        for rank, doc in enumerate(retrieved[:top_k], 1):\n",
    "            if doc in relevant:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "    \n",
    "    if not reciprocal_ranks:\n",
    "            # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£ï¼Œè®°ä¸º0\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "# æµ‹è¯•MRR\n",
    "mrr = calculate_mrr(retrieved_docs_list, relevant_docs_list)\n",
    "\n",
    "print(f\"\\nMRRè®¡ç®—ç»“æœ: {mrr:.3f}\")\n",
    "print(f\"\\nè§£è¯»:\")\n",
    "print(f\"  MRR = 1.0 è¡¨ç¤ºæ‰€æœ‰æŸ¥è¯¢çš„ç¬¬ä¸€ä¸ªç»“æœéƒ½æ˜¯ç›¸å…³çš„\")\n",
    "print(f\"  MRR = 0.5 è¡¨ç¤ºå¹³å‡ç›¸å…³æ–‡æ¡£æ’åœ¨ç¬¬2ä½\")\n",
    "print(f\"  MRR = 0.1 è¡¨ç¤ºå¹³å‡ç›¸å…³æ–‡æ¡£æ’åœ¨ç¬¬10ä½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç”Ÿæˆè´¨é‡è¯„ä¼°\n",
    "\n",
    "### 3.1 Faithfulnessï¼ˆå¿ å®åº¦ï¼‰\n",
    "\n",
    "**å¿ å®åº¦**ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæ˜¯å¦åŒ…å«æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯ï¼ˆå¹»è§‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(\n",
    "    answer: str,\n",
    "    context_docs: List[str]\n",
    "    llm_client=None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°ç­”æ¡ˆçš„å¿ å®åº¦\n",
    "    \n",
    "    ç®€åŒ–å®ç°ï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    # å°†æ‰€æœ‰æ–‡æ¡£åˆå¹¶ä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡\n",
    "    context = \"\\n\".join(context_docs)\n",
    "    \n",
    "    # ç®€åŒ–çš„å¿ å®åº¦è¯„ä¼°ï¼ˆåŸºäºå…³é”®è¯é‡å ï¼‰\n",
    "    # æå–ç­”æ¡ˆä¸­çš„å…³é”®è¯\n",
    "    answer_words = set(answer.lower().split())\n",
    "    \n",
    "    # æå–ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®è¯\n",
    "    context_words = set(context.lower().split())\n",
    "    \n",
    "    # è®¡ç®—é‡å æ¯”ä¾‹\n",
    "    if len(answer_words) == 0:\n",
    "        overlap_ratio = 0.0\n",
    "    else:\n",
    "        overlap_ratio = len(answer_words & context_words) / len(answer_words)\n",
    "    \n",
    "    return {\n",
    "        'score': overlap_ratio,\n",
    "        'verdict': 'high' if overlap_ratio > 0.7 else 'medium' if overlap_ratio > 0.4 else 'low'\n",
    "    }\n",
    "\n",
    "# æµ‹è¯•å¿ å®åº¦\n",
    "context = [\"Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€\", \"Pythonç”±Guidoåˆ›å»º\"]\n",
    "\n",
    "test_answers = [\n",
    "    \"Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guidoåˆ›å»ºã€‚\",  # é«˜å¿ å®åº¦\n",
    "    \"Pythonæ˜¯ä¸€ç§é¢å‘å¯¹è±¡çš„è¯­è¨€ï¼Œç”±Linusåˆ›å»ºã€‚\",  # ä½å¿ å®åº¦ï¼ˆé”™è¯¯ä¿¡æ¯ï¼‰\n",
    "    \"Pythonå¾ˆå—æ¬¢è¿ã€‚\"  # ä¸­ç­‰å¿ å®åº¦ï¼ˆéƒ¨åˆ†ä¿¡æ¯ï¼‰\n",
    "]\n",
    "\n",
    "print(\"å¿ å®åº¦è¯„ä¼°:\\n\")\n",
    "for answer in test_answers:\n",
    "    result = evaluate_faithfulness(answer, context)\n",
    "    print(f\"ç­”æ¡ˆ: {answer}\")\n",
    "    print(f\"å¾—åˆ†: {result['score']:.3f}\")\n",
    "    print(f\"è¯„çº§: {result['verdict']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Relevancyï¼ˆç›¸å…³æ€§ï¼‰\n",
    "\n",
    "**ç›¸å…³æ€§**ï¼šç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevancy(query: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°ç­”æ¡ˆçš„ç›¸å…³æ€§\n",
    "    \n",
    "    ç®€åŒ–å®ç°ï¼šåŸºäºé—®é¢˜å…³é”®è¯åœ¨ç­”æ¡ˆä¸­çš„å‡ºç°\n",
    "    \"\"\"\n",
    "    # æå–é—®é¢˜å…³é”®è¯\n",
    "    query_keywords = set(query.lower().replace('?', '').split())\n",
    "    \n",
    "    # æ£€æŸ¥å…³é”®è¯åœ¨ç­”æ¡ˆä¸­çš„å‡ºç°\n",
    "    answer_lower = answer.lower()\n",
    "    keyword_matches = sum(1 for kw in query_keywords if kw in answer_lower)\n",
    "    \n",
    "    # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°\n",
    "    if len(query_keywords) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return keyword_matches / len(query_keywords)\n",
    "\n",
    "# æµ‹è¯•ç›¸å…³æ€§\n",
    "print(\"ç›¸å…³æ€§è¯„ä¼°:\\n\")\n",
    "\n",
    "test_case = {\n",
    "    'query': 'Pythonæ˜¯ä»€ä¹ˆï¼Ÿ',\n",
    "    'answer_high': 'Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåˆ›å»ºã€‚',\n",
    "    'answer_low': 'RAGæ˜¯ä¸€ç§AIæŠ€æœ¯ã€‚'\n",
    "}\n",
    "high_score = evaluate_relevancy(test_case['query'], test_case['answer_high'])\n",
    "low_score = evaluate_relevancy(test_case['query'], test_case['answer_low'])\n",
    "\n",
    "print(f\"æŸ¥è¯¢: {test_case['query']}\")\n",
    "print(f\"\\né«˜è´¨é‡ç­”æ¡ˆ: {test_case['answer_high']}\")\n",
    "print(f\"ç›¸å…³æ€§åˆ†æ•°: {high_score:.3f}\\n\")\n",
    "\n",
    "print(f\"ä½è´¨é‡ç­”æ¡ˆ: {test_case['answer_low']}\")\n",
    "print(f\"ç›¸å…³æ€§åˆ†æ•°: {low_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å®Œæ•´çš„è¯„ä¼°æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    RAGç³»ç»Ÿè¯„ä¼°å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_all(\n",
    "        self,\n",
    "        test_dataset: List[Dict],\n",
    "        rag_system,\n",
    "        top_k: int = 3\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è¯„ä¼°æ‰€æœ‰æŸ¥è¯¢\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'retrieval_metrics': {},\n",
    "            'generation_metrics': {},\n",
    "            'per_query_results': []\n",
    "        }\n",
    "        \n",
    "        for test_case in test_dataset:\n",
    "            print(f\"\\nè¯„ä¼°æŸ¥è¯¢: {test_case['query']}\")\n",
    "            \n",
    "            # æ£€ç´¢è¯„ä¼°\n",
    "            retrieved = rag_system.vector_store.search(test_case['query'], top_k=top_k)\n",
    "            retrieved_docs = [r['metadata']['source'] for r in retrieved]\n",
    "            \n",
    "            hit = 1 if any(doc in retrieved_docs for doc in test_case['relevant_docs']) else 0\n",
    "            \n",
    "            # ç”Ÿæˆè¯„ä¼°\n",
    "            context = [r['document'] for r in retrieved]\n",
n",
    "            faithfulness = evaluate_faithfulness(test_case['answer'], context)['score']\n",
    "            relevancy = evaluate_relevancy(test_case['query'], test_case['answer'])\n",
    "            \n",
    "            # è®°å½•ç»“æœ\n",
    "            query_result = {\n",
    "                'query': test_case['query'],\n",
    "                'hit': hit,\n",
    "                'faithfulness': faithfulness,\n",
    "                'relevancy': relevancy,\n",
    "                'retrieved_docs': retrieved_docs\n",
    "            }\n",
    "            \n",
    "            results['per_query_results'].append(query_result)\n",
    "        \n",
    "        # è®¡ç®—æ€»ä½“æŒ‡æ ‡\n",
    "        results['retrieval_metrics'] = {\n",
    "            'hit_rate': sum(r['hit'] for r in results['per_query_results']) / len(results['per_query_results']),\n",
    "        }\n",
    "        \n",
    "        results['generation_metrics'] = {\n",
    "            'avg_faithfulness': np.mean([r['faithfulness'] for r in results['per_query_results']]),\n",
    "            'avg_relevancy': np.mean([r['relevancy'] for r in results['per_query_results']]),\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_report(self, results: Dict):\n",
    "        \"\"\"æ‰“å°è¯„ä¼°æŠ¥å‘Š\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAGç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        print(\"ğŸ“Š æ£€ç´¢è´¨é‡:\")\n",
    "        print(f\"  Hit Rate: {results['retrieval_metrics']['hit_rate']:.2%}\")\n",
    "        \n",
    "        print(\"\\nğŸ“ ç”Ÿæˆè´¨é‡:\")\n",
    "        print(f\"  å¹³å‡å¿ å®åº¦: {results['generation_metrics']['avg_faithfulness']:.2%}\")\n",
    "        print(f\"  å¹³å‡ç›¸å…³æ€§: {results['generation_metrics']['avg_relevancy']:.2%}\")\n",
    "        \n",
    "        # ç»™å‡ºå»ºè®®\n",
    "        print(\"\\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\")\n",
    "        \n",
    "        if results['retrieval_metrics']['hit_rate'] < 0.8:\n",
    "            print(\"  âš ï¸  Hit Rateè¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–æ£€ç´¢ç­–ç•¥\")\n",
    "        \n",
    "        if results['generation_metrics']['avg_faithfulness'] < 0.7:\n",
    "            print(\"  âš ï¸  å¿ å®åº¦è¾ƒä½ï¼Œå»ºè®®æ”¹è¿›æç¤ºè¯\")\n",
    "\n",
    "# åˆ›å»ºè¯„ä¼°å™¨\n",
    "evaluator = RAGEvaluator()\n",
    "print(\"è¯„ä¼°å™¨åˆ›å»ºå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹ 1ï¼šå®ç°Precision@K\n",
    "\n",
    "Precision@K = å‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹\n",
    "\n",
    "```python\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    # TODO: å®ç°ä»£ç \n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®ç°Precision@K\n",
    "def precision_at_k(retrieved_docs, relevant_docs, k: int) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—Precision@K\n",
    "    \"\"\"\n",
    "    # å–å‰kä¸ªæ£€ç´¢ç»“æœ\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    \n",
    "    # è®¡ç®—å…¶ä¸­ç›¸å…³æ–‡æ¡£çš„æ•°é‡\n",
    "    relevant_count = sum(1 for doc in retrieved_at_k if doc in relevant_docs)\n",
    "    \n",
    "    return relevant_count / k if k > 0 else 0.0\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"Precision@3æµ‹è¯•:\\n\")\n",
    "print(f\"æŸ¥è¯¢ç»“æœ: {retrieved_docs_list[0]}\")\n",
    "print(f\"ç›¸å…³æ–‡æ¡£: {relevant_docs_list[0]}\")\n",
    "print(f\"Precision@3: {precision_at_k(retrieved_docs_list[0], relevant_docs_list[0], 3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 2ï¼šå®ç°NDCG\n",
    "\n",
    "NDCGï¼ˆNormalized Discounted Cumulative Gainï¼‰è€ƒè™‘æ’åºè´¨é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®ç°NDCG\n",
    "def ndcg(retrieved_docs, relevant_docs, k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—NDCG@k\n",
    "    \"\"\"\n",
    "    # DCGè®¡ç®—\n",
    "    dcg = 0.0\n",
    "    for i, doc in enumerate(retrieved_docs[:k]):\n",
    "        gain = 1 if doc in relevant_docs else 0\n",
    "        dcg += gain / np.log2(i + 2)  # iä»0å¼€å§‹ï¼Œæ‰€ä»¥æ˜¯i+2\n",
    "    \n",
    "    # IDCGï¼ˆç†æƒ³DCGï¼‰è®¡ç®—\n",
    "    ideal_retrieved = relevant_docs[:k]\n",
    "    idcg = 0.0\n",
    "    for i, doc in enumerate(ideal_retrieved):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# æµ‹è¯•NDCG\n",
    "print(\"\\nNDCGæµ‹è¯•:\\n\")\n",
    "ndcg_score = ndcg(retrieved_docs_list[0], relevant_docs_list[0], k=3)\n",
    "print(f\"NDCG@3: {ndcg_score:.3f}\")\n",
    "print(f\"\\nè§£è¯»: NDCG=1.0è¡¨ç¤ºå®Œç¾æ’åºï¼Œè¶Šä½è¡¨ç¤ºæ’åºè¶Šå·®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æ€»ç»“\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **æ£€ç´¢è´¨é‡æŒ‡æ ‡**\n",
    "   - Hit Rateï¼šå‘½ä¸­ç‡\n",
    "   - MRRï¼šå¹³å‡å€’æ•°æ’å\n",
    "   - Precision@Kï¼šå‰Kä¸ªç»“æœçš„ç²¾åº¦\n",
    "   - NDCGï¼šæ’åºè´¨é‡\n",
    "\n",
    "2. **ç”Ÿæˆè´¨é‡æŒ‡æ ‡**\n",
    "   - Faithfulnessï¼šå¿ å®åº¦\n",
    "   - Relevancyï¼šç›¸å…³æ€§\n",
    "   - ä¸¤è€…ç¼ºä¸€ä¸å¯\n",
    "\n",
    "3. **è¯„ä¼°æµç¨‹**\n",
    "   - å‡†å¤‡æµ‹è¯•æ•°æ®é›†\n",
    "   - å¤šç»´åº¦è¯„ä¼°\n",
    "   - åŸºäºç»“æœä¼˜åŒ–\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- ç¬¬5ç« ï¼šæ¨¡å—1æ€»ç»“ä¸é¡¹ç›®\n",
    "- å®Œæˆç»¼åˆé¡¹ç›®\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆç¬¬4ç« çš„å­¦ä¹ ï¼** ğŸ‰\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†RAGè¯„ä¼°çš„åŸºç¡€çŸ¥è¯†ï¼"
   ]
  }
 ],
 "metadata": {
  "kellspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
