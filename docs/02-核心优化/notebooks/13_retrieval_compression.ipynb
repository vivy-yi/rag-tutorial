{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第13章：检索压缩优化\n",
        "\n",
        "**本章目标**:\n",
        "- 理解检索压缩的原理和价值\n",
        "- 掌握多种压缩策略\n",
        "- 实现语义保留压缩\n",
        "- 应用段落级压缩技术\n",
        "- 评估压缩效果\n",
        "\n",
        "---\n",
        "\n",
        "## 什么是检索压缩?\n",
        "\n",
        "检索压缩(Retrieval Compression)是在保留关键信息的前提下，大幅减少检索结果的token消耗的技术。\n",
        "\n",
        "**核心价值**:\n",
        "- 节省API成本\n",
        "- 提升响应速度\n",
        "- 提高答案质量\n",
        "- 扩展检索容量\n",
        "\n",
        "```python\n",
        "# 压缩效果示例\n",
        "原文档: 6000 tokens\n",
        "压缩后: 500 tokens\n",
        "压缩率: 8.3%\n",
        "关键信息保留: 85%+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 为什么需要检索压缩?\n",
        "\n",
        "### 问题场景"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例：传统检索的问题\n",
        "\n",
        "query = \"Python的异步编程如何使用？\"\n",
        "\n",
        "# 传统检索结果\n",
        "retrieved_docs = [\n",
        "    \"\"\"Python异步编程使用async/await语法...\n",
        "    [此处省略2000字详细说明]\n",
        "    \"\"\",\n",
        "    \"\"\"asyncio是Python的标准库...\n",
        "    [此处省略2000字API文档]\n",
        "    \"\"\",\n",
        "    \"\"\"异步编程可以提高性能...\n",
        "    [此处省略2000字性能分析]\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "total_tokens = sum(len(doc.split()) for doc in retrieved_docs)\n",
        "print(f\"检索到{len(retrieved_docs)}个文档\")\n",
        "print(f\"总长度: {total_tokens} tokens\")\n",
        "print(\"\\n问题:\")\n",
        "print(\"  ✓ 可能超出模型上下文限制\")\n",
        "print(\"  ✓ 大量冗余和无关信息\")\n",
        "print(\"  ✓ API调用成本高\")\n",
        "print(\"  ✓ 响应速度慢\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 简单压缩技术\n",
        "\n",
        "### 2.1 固定长度截断"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def truncate_by_length(text: str, max_length: int) -> str:\n",
        "    \"\"\"\n",
        "    按固定长度截断\n",
        "\n",
        "    保留开头80%和结尾20%\n",
        "    \"\"\"\n",
        "    if len(text) <= max_length:\n",
        "        return text\n",
        "\n",
        "    head_length = int(max_length * 0.8)\n",
        "    tail_length = max_length - head_length\n",
        "\n",
        "    head = text[:head_length]\n",
        "    tail = text[-tail_length:] if tail_length > 0 else \"\"\n",
        "\n",
        "    return f\"{head}\\n...[省略]...\\n{tail}\"\n",
        "\n",
        "# 示例\n",
        "long_text = \"Python是一门高级编程语言。\" * 100\n",
        "\n",
        "compressed = truncate_by_length(long_text, max_length=200)\n",
        "\n",
        "print(f\"原文长度: {len(long_text)} 字符\")\n",
        "print(f\"压缩后: {len(compressed)} 字符\")\n",
        "print(f\"压缩率: {len(compressed)/len(long_text)*100:.1f}%\")\n",
        "print(\"\\n压缩结果:\")\n",
        "print(compressed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 去除格式和冗余"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    清理文本：去除格式标记和冗余内容\n",
        "    \"\"\"\n",
        "    # 去除多余空白\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 去除Markdown格式\n",
        "    text = re.sub(r'#{1,6}\\s', '', text)  # 标题\n",
        "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # 粗体\n",
        "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)  # 斜体\n",
        "    text = re.sub(r'`(.*?)`', r'\\1', text)  # 代码\n",
        "\n",
        "    # 去除HTML标签\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def remove_redundant_sentences(text: str) -> str:\n",
        "    \"\"\"\n",
        "    去除冗余句子（简化版）\n",
        "    \"\"\"\n",
        "    sentences = [s.strip() for s in text.split('。') if s.strip()]\n",
        "    unique_sentences = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        # 检查是否与已有句子重复\n",
        "        is_duplicate = any(\n",
        "            sent in existing or existing in sent\n",
        "            for existing in unique_sentences\n",
        "        )\n",
        "        if not is_duplicate:\n",
        "            unique_sentences.append(sent)\n",
        "\n",
        "    return '。'.join(unique_sentences) + '。'\n",
        "\n",
        "# 示例\n",
        "text_with_format = \"\"\"\n",
        "## Python简介\n",
        "\n",
        "**Python**是一门*高级*编程语言。\n",
        "Python的语法非常简洁。\n",
        "Python的代码很容易阅读。\n",
        "Python的语法非常简洁。\n",
        "\"\"\"\n",
        "\n",
        "cleaned = clean_text(text_with_format)\n",
        "deduped = remove_redundant_sentences(cleaned)\n",
        "\n",
        "print(\"原始文本:\")\n",
        "print(text_with_format)\n",
        "print(\"\\n清理去重后:\")\n",
        "print(deduped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 语义保留压缩 ⭐\n",
        "\n",
        "### 3.1 SemanticCompressor实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class SemanticCompressor:\n",
        "    \"\"\"\n",
        "    语义压缩器\n",
        "\n",
        "    特点:\n",
        "    1. 保留关键信息\n",
        "    2. 简化表达\n",
        "    3. 去除冗余\n",
        "    4. 保持可读性\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm=None):\n",
        "        self.llm = llm\n",
        "\n",
        "    def compress_single_document(self,\n",
        "                                  document: str,\n",
        "                                  compression_ratio: float = 0.25) -> str:\n",
        "        \"\"\"\n",
        "        压缩单个文档\n",
        "\n",
        "        Args:\n",
        "            document: 原始文档\n",
        "            compression_ratio: 压缩比例 (0-1)\n",
        "\n",
        "        Returns:\n",
        "            压缩后的文档\n",
        "        \"\"\"\n",
        "        target_length = int(len(document) * compression_ratio)\n",
        "\n",
        "        # 如果没有LLM，使用简单压缩\n",
        "        if not self.llm:\n",
        "            return self._simple_compress(document, target_length)\n",
        "\n",
        "        # 使用LLM压缩\n",
        "        prompt = f\"\"\"请将以下文档压缩到{target_length}字符左右。\n",
        "\n",
        "要求:\n",
        "1. 保留所有关键信息\n",
        "2. 去除冗余和修饰内容\n",
        "3. 简化表达但保持原意\n",
        "4. 保持逻辑结构清晰\n",
        "\n",
        "原文档:\n",
        "{document}\n",
        "\n",
        "请输出压缩后的文档:\"\"\"\n",
        "\n",
        "        return self.llm.predict(prompt)\n",
        "\n",
        "    def compress_multiple_documents(self,\n",
        "                                     documents: List[str],\n",
        "                                     query: str,\n",
        "                                     max_total_length: int = 1000) -> str:\n",
        "        \"\"\"\n",
        "        压缩并合并多个文档\n",
        "        \"\"\"\n",
        "        print(f\"压缩{len(documents)}个文档，目标长度: {max_total_length}\")\n",
        "\n",
        "        # 逐个压缩\n",
        "        compressed_docs = []\n",
        "        for i, doc in enumerate(documents, 1):\n",
        "            budget = max_total_length // len(documents)\n",
        "            compressed = self._compress_with_context(doc, query, budget)\n",
        "            compressed_docs.append(compressed)\n",
        "\n",
        "        # 合并\n",
        "        merged = \"\\n\\n\".join(compressed_docs)\n",
        "\n",
        "        # 如果超长，整体压缩\n",
        "        if len(merged) > max_total_length:\n",
        "            merged = self._simple_compress(merged, max_total_length)\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def _compress_with_context(self, document: str, query: str, max_length: int) -> str:\n",
        "        \"\"\"基于查询上下文压缩文档\"\"\"\n",
        "        if not self.llm:\n",
        "            return self._simple_compress(document, max_length)\n",
        "\n",
        "        prompt = f\"\"\"用户查询: {query}\n",
        "\n",
        "请将以下文档压缩到{max_length}字符以内，要求:\n",
        "1. 优先保留与查询相关的内容\n",
        "2. 保留关键细节和数据\n",
        "3. 去除与查询无关的内容\n",
        "\n",
        "文档:\n",
        "{document}\n",
        "\n",
        "压缩结果:\"\"\"\n",
        "\n",
        "        return self.llm.predict(prompt)\n",
        "\n",
        "    def _simple_compress(self, text: str, max_length: int) -> str:\n",
        "        \"\"\"简单压缩（不使用LLM）\"\"\"\n",
        "        if len(text) <= max_length:\n",
        "            return text\n",
        "\n",
        "        # 保留关键句子\n",
        "        sentences = [s.strip() for s in text.split('。') if s.strip()]\n",
        "\n",
        "        # 简单策略：保留前面的句子（通常包含更多关键信息）\n",
        "        result_sentences = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            if current_length + len(sent) <= max_length * 0.9:\n",
        "                result_sentences.append(sent)\n",
        "                current_length += len(sent)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return '。'.join(result_sentences) + '。'\n",
        "\n",
        "# 创建压缩器\n",
        "compressor = SemanticCompressor()\n",
        "print(\"语义压缩器创建成功!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试语义压缩\n",
        "documents = [\n",
        "    \"Python异步编程使用async/await语法，可以显著提高IO密集型任务的性能。asyncio是Python的标准库，提供了事件循环、协程、Future等组件。\",\n",
        "    \"异步编程允许程序在等待IO操作时执行其他任务，从而提高并发性能。FastAPI、aiohttp等框架充分利用了异步特性。\",\n",
        "    \"Python的异步生态系统包括aiohttp、asyncpg等库。异步编程的挑战包括调试困难、并发控制等。\"\n",
        "]\n",
        "\n",
        "query = \"Python异步编程的优势是什么？\"\n",
        "\n",
        "compressed = compressor.compress_multiple_documents(\n",
        "    documents=documents,\n",
        "    query=query,\n",
        "    max_total_length=200\n",
        ")\n",
        "\n",
        "print(\"原始文档总长度:\", sum(len(d) for d in documents))\n",
        "print(\"压缩后长度:\", len(compressed))\n",
        "print(f\"压缩率: {len(compressed)/sum(len(d) for d in documents)*100:.1f}%\")\n",
        "print(\"\\n压缩结果:\")\n",
        "print(compressed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 段落级压缩 ⭐⭐\n",
        "\n",
        "### 4.1 ParagraphCompressor实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ParagraphCompressor:\n",
        "    \"\"\"\n",
        "    段落级压缩器\n",
        "\n",
        "    特点:\n",
        "    1. 段落级别的细粒度控制\n",
        "    2. 相关性筛选\n",
        "    3. 智能合并\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm=None):\n",
        "        self.llm = llm\n",
        "\n",
        "    def compress(self,\n",
        "                 documents: List[str],\n",
        "                 query: str,\n",
        "                 target_length: int) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        段落级压缩主方法\n",
        "\n",
        "        Returns:\n",
        "            (压缩后的文档, 统计信息)\n",
        "        \"\"\"\n",
        "        stats = {\n",
        "            \"original_length\": sum(len(d) for d in documents),\n",
        "            \"num_paragraphs\": 0,\n",
        "            \"num_kept\": 0,\n",
        "            \"num_discarded\": 0,\n",
        "        }\n",
        "\n",
        "        # 步骤1: 拆分段落\n",
        "        paragraphs = self._split_into_paragraphs(documents)\n",
        "        stats[\"num_paragraphs\"] = len(paragraphs)\n",
        "        print(f\"拆分得到 {len(paragraphs)} 个段落\")\n",
        "\n",
        "        # 步骤2: 评估相关性\n",
        "        scored_paragraphs = self._score_relevance(paragraphs, query)\n",
        "\n",
        "        # 步骤3: 选择段落\n",
        "        selected = self._select_paragraphs(scored_paragraphs, target_length)\n",
        "        stats[\"num_kept\"] = len(selected)\n",
        "        stats[\"num_discarded\"] = len(paragraphs) - len(selected)\n",
        "\n",
        "        # 步骤4: 压缩选中段落\n",
        "        compressed_paragraphs = [\n",
        "            self._compress_paragraph(para, query)\n",
        "            for para, score in selected\n",
        "        ]\n",
        "\n",
        "        # 步骤5: 合并\n",
        "        final_doc = \"\\n\\n\".join(compressed_paragraphs)\n",
        "\n",
        "        stats[\"final_length\"] = len(final_doc)\n",
        "        stats[\"compression_ratio\"] = final_doc[\"final_length\"] / stats[\"original_length\"]\n",
        "\n",
        "        return final_doc, stats\n",
        "\n",
        "    def _split_into_paragraphs(self, documents: List[str]) -> List[str]:\n",
        "        \"\"\"将文档拆分为段落\"\"\"\n",
        "        paragraphs = []\n",
        "        for doc in documents:\n",
        "            doc_paragraphs = doc.split('\\n\\n')\n",
        "            paragraphs.extend([p.strip() for p in doc_paragraphs if p.strip()])\n",
        "        return paragraphs\n",
        "\n",
        "    def _score_relevance(self, paragraphs: List[str], query: str) -> List[Tuple[str, float]]:\n",
        "        \"\"\"评估段落相关性\"\"\"\n",
        "        scored = []\n",
        "        for para in paragraphs:\n",
        "            score = self._calculate_relevance(para, query)\n",
        "            scored.append((para, score))\n",
        "\n",
        "        # 按相关性排序\n",
        "        scored.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scored\n",
        "\n",
        "    def _calculate_relevance(self, paragraph: str, query: str) -> float:\n",
        "        \"\"\"计算段落与查询的相关性（简化版）\"\"\"\n",
        "        query_words = set(query.lower().split())\n",
        "        para_words = set(paragraph.lower().split())\n",
        "        overlap = len(query_words & para_words)\n",
        "        return min(overlap / len(query_words), 1.0) if query_words else 0.0\n",
        "\n",
        "    def _select_paragraphs(self,\n",
        "                            scored_paragraphs: List[Tuple[str, float]],\n",
        "                            target_length: int) -> List[Tuple[str, float]]:\n",
        "        \"\"\"选择段落\"\"\"\n",
        "        selected = []\n",
        "        current_length = 0\n",
        "\n",
        "        for para, score in scored_paragraphs:\n",
        "            estimated_length = len(para) * 0.4\n",
        "            if current_length + estimated_length <= target_length:\n",
        "                selected.append((para, score))\n",
        "                current_length += estimated_length\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return selected\n",
        "\n",
        "    def _compress_paragraph(self, paragraph: str, query: str) -> str:\n",
        "        \"\"\"压缩单个段落\"\"\"\n",
        "        if not self.llm:\n",
        "            # 简单压缩\n",
        "            target_length = max(len(paragraph) * 0.5, 50)\n",
        "            sentences = paragraph.split('。')\n",
        "            result = []\n",
        "            current_len = 0\n",
        "            for sent in sentences:\n",
        "                if current_len + len(sent) <= target_length:\n",
        "                    result.append(sent.strip())\n",
        "                    current_len += len(sent)\n",
        "                else:\n",
        "                    break\n",
        "            return '。'.join(result) + '。'\n",
        "\n",
        "        # 使用LLM压缩\n",
        "        target_length = max(len(paragraph) * 0.4, 50)\n",
        "        prompt = f\"\"\"用户查询: {query}\\n\\n请将以下段落压缩到{int(target_length)}字符:\\n\\n{paragraph}\"\"\"\n",
        "        return self.llm.predict(prompt)\n",
        "\n",
        "# 创建段落压缩器\n",
        "para_compressor = ParagraphCompressor()\n",
        "print(\"段落压缩器创建成功!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试段落压缩\n",
        "docs = [\n",
        "    \"\"\"Python的异步编程是一个重要特性。\n",
        "\n",
        "async/await语法是Python 3.5引入的，用于简化异步代码的编写。\n",
        "\n",
        "asyncio是Python的标准库，提供了事件循环、协程、Future等组件。\"\"\",\n",
        "\n",
        "    \"\"\"异步编程可以显著提高IO密集型应用的性能。\n",
        "\n",
        "在Web开发中，异步框架如FastAPI、aiohttp可以利用异步特性处理大量并发请求。\"\"\"\n",
        "]\n",
        "\n",
        "query = \"Python异步编程的特点和优势\"\n",
        "\n",
        "result, stats = para_compressor.compress(docs, query, target_length=300)\n",
        "\n",
        "print(\"压缩结果:\")\n",
        "print(result)\n",
        "print(\"\\n统计信息:\")\n",
        "for k, v in stats.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 压缩效果评估\n",
        "\n",
        "### 5.1 评估指标实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CompressionEvaluator:\n",
        "    \"\"\"\n",
        "    压缩效果评估器\n",
        "    \"\"\"\n",
        "\n",
        "    def evaluate(self, original: str, compressed: str, query: str) -> Dict:\n",
        "        \"\"\"\n",
        "        评估压缩效果\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"compression_ratio\": self._compression_ratio(original, compressed),\n",
        "            \"token_savings\": self._token_savings(original, compressed),\n",
        "            \"semantic_similarity\": self._semantic_similarity(original, compressed),\n",
        "            \"query_relevance\": self._query_relevance(compressed, query),\n",
        "        }\n",
        "\n",
        "    def _compression_ratio(self, original: str, compressed: str) -> float:\n",
        "        \"\"\"压缩比例\"\"\"\n",
        "        return len(compressed) / len(original)\n",
        "\n",
        "    def _token_savings(self, original: str, compressed: str) -> Dict:\n",
        "        \"\"\"Token节省（简化估算）\"\"\"\n",
        "        original_tokens = len(original) / 2\n",
        "        compressed_tokens = len(compressed) / 2\n",
        "\n",
        "        return {\n",
        "            \"original_tokens\": int(original_tokens),\n",
        "            \"compressed_tokens\": int(compressed_tokens),\n",
        "            \"saved_tokens\": int(original_tokens - compressed_tokens),\n",
        "            \"savings_ratio\": (original_tokens - compressed_tokens) / original_tokens,\n",
        "        }\n",
        "\n",
        "    def _semantic_similarity(self, original: str, compressed: str) -> float:\n",
        "        \"\"\"语义相似度（简化版）\"\"\"\n",
        "        orig_words = set(original.lower().split())\n",
        "        comp_words = set(compressed.lower().split())\n",
        "        overlap = len(orig_words & comp_words)\n",
        "        union = len(orig_words | comp_words)\n",
        "        return overlap / union if union > 0 else 0.0\n",
        "\n",
        "    def _query_relevance(self, compressed: str, query: str) -> float:\n",
        "        \"\"\"查询相关性\"\"\"\n",
        "        query_words = set(query.lower().split())\n",
        "        comp_words = set(compressed.lower().split())\n",
        "        overlap = len(query_words & comp_words)\n",
        "        return overlap / len(query_words) if query_words else 0.0\n",
        "\n",
        "\n",
        "# 测试评估\n",
        "evaluator = CompressionEvaluator()\n",
        "\n",
        "original = \"Python异步编程使用async/await语法。asyncio是Python的标准库，提供了事件循环机制。异步编程可以提高程序的并发性能，特别适合IO密集型任务。\"\n",
        "compressed = \"Python异步编程用async/await语法，通过asyncio的事件循环机制提升IO密集型任务的并发性能。\"\n",
        "query = \"Python异步编程\"\n",
        "\n",
        "results = evaluator.evaluate(original, compressed, query)\n",
        "\n",
        "print(\"压缩效果评估:\")\n",
        "for metric, value in results.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"\\n{metric}:\")\n",
        "        for k, v in value.items():\n",
        "            print(f\"  {k}: {v:.2f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 实战练习\n",
        "\n",
        "### 练习1: 对比不同压缩策略\n",
        "\n",
        "尝试对比:\n",
        "1. 固定长度截断\n",
        "2. 语义压缩\n",
        "3. 段落级压缩\n",
        "\n",
        "评估它们的:\n",
        "- 压缩率\n",
        "- 语义保留度\n",
        "- 查询相关性\n",
        "\n",
        "### 练习2: 集成真实LLM\n",
        "\n",
        "1. 集成OpenAI或其他LLM\n",
        "2. 实现完整的语义压缩\n",
        "3. 对比使用LLM前后的效果\n",
        "\n",
        "### 练习3: 优化相关性计算\n",
        "\n",
        "1. 使用embedding计算相似度\n",
        "2. 实现更智能的段落选择\n",
        "3. 测试对压缩效果的影响\n",
        "\n",
        "---\n",
        "\n",
        "## 总结\n",
        "\n",
        "本节学习了检索压缩的核心技术:\n",
        "\n",
        "1. **压缩价值**: 节省成本、提升速度、提高质量\n",
        "2. **压缩策略**: 简单截断、语义压缩、段落级压缩\n",
        "3. **实现方法**: SemanticCompressor、ParagraphCompressor\n",
        "4. **效果评估**: 多维度评估压缩质量\n",
        "\n",
        "**应用建议**:\n",
        "- 简单场景: 规则压缩\n",
        "- 复杂场景: 语义压缩\n",
        "- 高质量要求: 段落级压缩\n",
        "\n",
        "**下一步**:\n",
        "- 尝试在实际RAG系统中应用检索压缩\n",
        "- 根据具体场景优化压缩策略\n",
        "- 监控压缩对最终答案质量的影响"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
