# ç¬¬7ç« ï¼šé«˜çº§åˆ†å—ç­–ç•¥

> åˆé€‚çš„åˆ†å—ç­–ç•¥å¯ä»¥è®©æ£€ç´¢è´¨é‡æå‡5-10%ã€‚æœ¬ç« å°†å¸¦ä½ æŒæ¡å¤šç§é«˜çº§åˆ†å—æŠ€æœ¯ã€‚

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£åˆ†å—ç­–ç•¥çš„é‡è¦æ€§
- [ ] æŒæ¡5+ç§é«˜çº§åˆ†å—æ–¹æ³•
- [ ] çŸ¥é“å¦‚ä½•ä¼˜åŒ–åˆ†å—å‚æ•°
- [ ] èƒ½å¤Ÿæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚ç­–ç•¥
- [ ] å®Œæˆåˆ†å—æ•ˆæœå¯¹æ¯”å®éªŒ

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š3å°æ—¶
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â˜†â˜†

---

## å‰ç½®çŸ¥è¯†

- [ ] å®Œæˆæ¨¡å—1ç¬¬3ç« ï¼ˆåŸºç¡€åˆ†å—ï¼‰
- [ ] ç†è§£æ–‡æ¡£ç»“æ„åˆ†æ
- [ ] æŒæ¡åŸºç¡€åˆ†å—æ–¹æ³•

**ç¯å¢ƒè¦æ±‚**ï¼š
- LlamaIndex
- llama-hubï¼ˆé«˜çº§åˆ†å—å™¨ï¼‰

---

## 7.1 åˆ†å—ç­–ç•¥çš„é‡è¦æ€§

### ä¸ºä»€ä¹ˆåˆ†å—è¿™ä¹ˆé‡è¦ï¼Ÿ

**æ ¸å¿ƒé—®é¢˜**ï¼šæ–‡æ¡£åˆ‡åˆ†æ–¹å¼ç›´æ¥å½±å“æ£€ç´¢è´¨é‡

```
ç¤ºä¾‹ï¼šåˆ†æä¸€æ®µé•¿æ–‡æ¡£

åŸæ–‡ï¼ˆ2000å­—ï¼‰ï¼š
â”œâ”€ æ®µè½1ï¼šä»‹ç»AIå†å²ï¼ˆ300å­—ï¼‰
â”œâ”€ æ®µè½2ï¼šæœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆ500å­—ï¼‰
â”œâ”€ æ®µè½3ï¼šæ·±åº¦å­¦ä¹ åŸç†ï¼ˆ800å­—ï¼‰
â””â”€ æ®µè½4ï¼šå®é™…åº”ç”¨ï¼ˆ400å­—ï¼‰

é—®é¢˜æŸ¥è¯¢ï¼š"æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"

ç­–ç•¥1ï¼ˆå›ºå®šé•¿åº¦500å­—ï¼‰ï¼š
  å—1: "...AIå†å²...æœºå™¨å­¦ä¹ åŸºç¡€[åˆ‡æ–­]..."
  å—2: "...åŸºç¡€...æ·±åº¦å­¦ä¹ åŸç†..."
  âŒ é—®é¢˜ï¼šä¸Šä¸‹æ–‡ä¿¡æ¯è¢«åˆ‡æ–­

ç­–ç•¥2ï¼ˆæŒ‰æ®µè½ï¼‰ï¼š
  å—1: "AIå†å²"ï¼ˆ300å­—ï¼‰
  å—2: "æœºå™¨å­¦ä¹ åŸºç¡€"ï¼ˆ500å­—ï¼‰
  å—3: "æ·±åº¦å­¦ä¹ åŸç†"ï¼ˆ800å­—ï¼‰
  âœ… ä¼˜åŠ¿ï¼šä¿æŒè¯­ä¹‰å®Œæ•´ï¼Œæ£€ç´¢ç²¾å‡†
```

### åˆ†å—å¯¹æ€§èƒ½çš„å½±å“

```
å®éªŒï¼šä¸åŒåˆ†å—ç­–ç•¥çš„Hit Rate

å›ºå®šé•¿åº¦åˆ†å—:  62%
å›ºå®šé•¿åº¦+é‡å :  68%
æŒ‰æ®µè½åˆ†å—:      75%
è¯­ä¹‰åˆ†å—:        78%
ä¸Šä¸‹æ–‡åˆ†å—å¤´:    82%

â†’ ä¼˜åŒ–åˆ†å—å¯ä»¥æå‡20%ï¼
```

---

## 7.2 åˆ†å—ç­–ç•¥è¯¦è§£

### ç­–ç•¥1ï¼šè¯­ä¹‰åˆ†å—

#### åŸç†

æ ¹æ®è¯­ä¹‰è¾¹ç•Œï¼ˆå¥å­ã€æ®µè½ï¼‰è¿›è¡Œåˆ‡åˆ†ï¼Œè€Œä¸æ˜¯å›ºå®šå­—ç¬¦æ•°ã€‚

**ä¼˜åŠ¿**ï¼š
- âœ… ä¿æŒè¯­ä¹‰å®Œæ•´
- âœ… é¿å…åˆ‡æ–­é‡è¦ä¿¡æ¯
- âœ… æå‡æ£€ç´¢ç›¸å…³æ€§

#### å®ç°ä»£ç 

```python
# æ–‡ä»¶åï¼š07_01_semantic_chunking.py
"""
è¯­ä¹‰åˆ†å—å®ç°
"""

from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding

def semantic_chunking(documents, breakpoint_threshold=0.6):
    """
    è¯­ä¹‰åˆ†å—

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        breakpoint_threshold: æ–­ç‚¹é˜ˆå€¼

    Returns:
        åˆ†å—åˆ—è¡¨
    """
    print("="*60)
    print("è¯­ä¹‰åˆ†å—æ¼”ç¤º")
    print("="*60 + "\n")

    # åˆ›å»ºåµŒå…¥æ¨¡å‹
    embed_model = OpenAIEmbedding()

    # åˆ›å»ºè¯­ä¹‰åˆ†å—å™¨
    splitter = SemanticSplitterNodeParser(
        buffer_size=1,
        breakpoint_threshold=breakpoint_threshold,
        embed_model=embed_model
    )

    # åˆ†å—
    print("å¼€å§‹è¯­ä¹‰åˆ†å—...")
    nodes = splitter.get_nodes_from_documents(documents)

    print(f"âœ… ç”Ÿæˆäº† {len(nodes)} ä¸ªå—\n")

    # æ˜¾ç¤ºåˆ†å—ä¿¡æ¯
    for i, node in enumerate(nodes[:5], 1):
        print(f"å— {i}:")
        print(f"  é•¿åº¦: {len(node.text)} å­—ç¬¦")
        print(f"  å†…å®¹: {node.text[:100]}...")
        print()

    return nodes

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from llama_index.core import Document

    # ç¤ºä¾‹æ–‡æ¡£
    doc = Document(text="""
    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚

    å®ƒè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

    æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚

    é€šè¿‡ç®—æ³•ï¼Œè®¡ç®—æœºå¯ä»¥ä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹ã€‚

    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚

    å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚

    æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
    """)

    nodes = semantic_chunking([doc])
```

### ç­–ç•¥2ï¼šä¸Šä¸‹æ–‡åˆ†å—å¤´

#### åŸç†

ä¸ºæ¯ä¸ªå—æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯´æ˜å…¶åœ¨åŸæ–‡ä¸­çš„ä½ç½®å’Œå…³ç³»ã€‚

**ç»“æ„**ï¼š

```
åŸå§‹æ–‡æ¡£ï¼š
  ç¬¬1ç« ï¼šä»‹ç»
  ç¬¬2ç« ï¼šæ–¹æ³•
  ç¬¬3ç« ï¼šå®éªŒ

åˆ†å—åæ·»åŠ ä¸Šä¸‹æ–‡ï¼š

å—1: "ç¬¬1ç« å†…å®¹..."
     å…ƒæ•°æ®ï¼š{"chapter": "1", "title": "ä»‹ç»"}

å—2: "ç¬¬2ç« çš„ç®—æ³•éƒ¨åˆ†..."
     å…ƒæ•°æ®ï¼š{"chapter": "2", "section": "ç®—æ³•", "before": "ç¬¬2ç« æ–¹æ³•æ¦‚è¿°", "after": "ç¬¬2ç« çš„å®éªŒéƒ¨åˆ†"}
```

#### å®ç°ä»£ç 

```python
# æ–‡ä»¶åï¼š07_02_contextual_headers.py
"""
ä¸Šä¸‹æ–‡åˆ†å—å¤´å®ç°
"""

from llama_index.core.node_parser import (
    MarkdownNodeParser,
    CodeSplitter
)

def contextual_chunking_markdown(file_path: str):
    """
    Markdownæ–‡æ¡£çš„ä¸Šä¸‹æ–‡åˆ†å—

    Args:
        file_path: Markdownæ–‡ä»¶è·¯å¾„
    """
    print("="*60)
    print("ä¸Šä¸‹æ–‡åˆ†å—å¤´æ¼”ç¤º")
    print("="*60 + "\n")

    # ä½¿ç”¨Markdownè§£æå™¨
    parser = MarkdownNodeParser(
        include_prev_next_rel=True,
        include_metadata=True
    )

    # åŠ è½½å¹¶è§£æ
    from llama_index.core import SimpleDirectoryReader
    reader = SimpleDirectoryReader(input_files=[file_path])
    docs = reader.load_data()

    nodes = parser.get_nodes_from_documents(docs)

    print(f"âœ… ç”Ÿæˆäº† {len(nodes)} ä¸ªå—")

    # æ˜¾ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯
    for i, node in enumerate(nodes[:3], 1):
        print(f"\nå— {i}:")
        print(f"  å†…å®¹: {node.text[:80]}...")
        print(f"  å…ƒæ•°æ®: {node.metadata}")
        if hasattr(node, 'relationships'):
            print(f"  å…³ç³»: {node.relationships}")

    return nodes

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹Markdownæ–‡ä»¶
    sample_md = """
# ç¬¬ä¸€ç« ï¼šç®€ä»‹

è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ã€‚

## 1.1 èƒŒæ™¯

èƒŒæ™¯ä»‹ç»...

## 1.2 ç›®æ ‡

ä¸»è¦ç›®æ ‡...

# ç¬¬äºŒç« ï¼šæ–¹æ³•

æ–¹æ³•è®ºè¯´æ˜...
"""

    with open("sample.md", "w", encoding="utf-8") as f:
        f.write(sample_md)

    nodes = contextual_chunking_markdown("sample.md")
```

### ç­–ç•¥3ï¼šé€’å½’åˆ†å—

#### åŸç†

å°è¯•å¤šç§åˆ†éš”ç¬¦ï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ‡åˆ†ã€‚

**åˆ†éš”ç¬¦ä¼˜å…ˆçº§**ï¼š

```
1. æ®µè½åˆ†éš”ç¬¦: \n\n
2. å¥å­åˆ†éš”ç¬¦: ã€‚ï¼ï¼Ÿ.!?ç­‰
3. è¯åˆ†éš”ç¬¦: ç©ºæ ¼
4. å­—ç¬¦åˆ†éš”ç¬¦: é€å­—ç¬¦
```

#### å®ç°ä»£ç 

```python
# æ–‡ä»¶åï¼š07_03_recursive_chunking.py
"""
é€’å½’åˆ†å—å®ç°
"""

from llama_index.core.node_parser import RecursiveCharacterTextSplitter

def recursive_chunking(
    documents,
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "ã€‚", " ", ""]
):
    """
    é€’å½’åˆ†å—

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        chunk_size: å—å¤§å°
        chunk_overlap: é‡å å¤§å°
        separators: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

    Returns:
        åˆ†å—åˆ—è¡¨
    """
    print("="*60)
    print("é€’å½’åˆ†å—æ¼”ç¤º")
    print("="*60 + "\n")

    print("åˆ†éš”ç¬¦ä¼˜å…ˆçº§:")
    for i, sep in enumerate(separators, 1):
        sep_repr = repr(sep) if sep else "(æ— )"
        print(f"  {i}. {sep_repr}")

    # åˆ›å»ºåˆ†å—å™¨
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators
    )

    # åˆ†å—
    nodes = splitter.get_nodes_from_documents(documents)

    print(f"\nâœ… ç”Ÿæˆäº† {len(nodes)} ä¸ªå—")

    return nodes

# å¯¹æ¯”å®éªŒ
if __name__ == "__main__":
    from llama_index.core import Document

    doc = Document(text="""
    ç¬¬1æ®µå†…å®¹ã€‚ç¬¬2æ®µå†…å®¹ï¼

    ç¬¬3æ®µå†…å®¹ï¼Ÿç¬¬4æ®µå†…å®¹ã€‚
    ç¬¬5æ®µå†…å®¹ï¼Œç¬¬6æ®µå†…å®¹ï¼Œç¬¬7æ®µå†…å®¹...

    å¾ˆé•¿çš„æ®µè½ï¼Œæ²¡æœ‰æ˜æ˜¾çš„åˆ†éš”ç¬¦ï¼Œéœ€è¦ç»§ç»­åˆ‡åˆ†ã€‚è¿™ä¸ªæ®µè½åŒ…å«äº†å¾ˆå¤šä¿¡æ¯ï¼Œéœ€è¦åˆç†åœ°åˆ†æˆå¤šä¸ªå—ã€‚
    """)

    print("æµ‹è¯•1: åªæœ‰æ®µè½åˆ†éš”ç¬¦")
    nodes1 = recursive_chunking([doc], separators=["\n\n", "\n"])
    print(f"å—æ•°: {len(nodes1)}")

    print("\næµ‹è¯•2: æ®µè½+å¥å­åˆ†éš”ç¬¦")
    nodes2 = recursive_chunking([doc], separators=["\n\n", "\n", "ã€‚"])
    print(f"å—æ•°: {len(nodes2)}")

    print("\næµ‹è¯•3: æ‰€æœ‰åˆ†éš”ç¬¦")
    nodes3 = recursive_chunking([doc])
    print(f"å—æ•°: {len(nodes3)}")
```

### ç­–ç•¥4ï¼šçˆ¶æ–‡æ¡£æ£€ç´¢

#### åŸç†

åˆ†å—æ—¶ä¿æŒçˆ¶å­å…³ç³»ï¼Œæ£€ç´¢æ—¶è¿”å›å°å—ï¼Œä½†ä½¿ç”¨çˆ¶æ–‡æ¡£çš„ä¸Šä¸‹æ–‡ã€‚

```
æ–‡æ¡£ç»“æ„ï¼š
  çˆ¶æ–‡æ¡£ï¼ˆå®Œæ•´ç« èŠ‚ï¼‰
    â”œâ”€ å­å—1ï¼ˆç¬¬1æ®µï¼‰
    â”œâ”€ å­å—2ï¼ˆç¬¬2æ®µï¼‰
    â””â”€ å­å—3ï¼ˆç¬¬3æ®µï¼‰

æ£€ç´¢æµç¨‹ï¼š
  1. æ£€ç´¢å­å—ï¼ˆç²¾å‡†ï¼‰
  2. è¿”å›çˆ¶æ–‡æ¡£ï¼ˆå®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
```

#### å®ç°ä»£ç 

```python
# æ–‡ä»¶åï¼š07_04_parent_document.py
"""
çˆ¶æ–‡æ¡£æ£€ç´¢å®ç°
"""

from llama_index.core.node_parser import (
    SentenceSplitter,
    HierarchicalNodeParser
)
from llama_index.core import StorageContext, load_index_from_storage

def parent_document_retrieval(documents, chunk_size=500):
    """
    çˆ¶æ–‡æ¡£æ£€ç´¢

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        chunk_size: å­å—å¤§å°

    Returns:
        ç´¢å¼•
    """
    print("="*60)
    print("çˆ¶æ–‡æ¡£æ£€ç´¢æ¼”ç¤º")
    print("="*60 + "\n")

    # 1. åˆ›å»ºçˆ¶æ–‡æ¡£è§£æå™¨
    node_parser = HierarchicalNodeParser.from_defaults(
        chunk_sizes=[2048, 512, 128]  # çˆ¶ã€å­ã€å­™å—å¤§å°
    )

    # 2. ç”Ÿæˆå±‚æ¬¡èŠ‚ç‚¹
    nodes = node_parser.get_nodes_from_documents(documents)

    print(f"âœ… ç”Ÿæˆäº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
    print(f"   çˆ¶èŠ‚ç‚¹: {sum(1 for n in nodes if n.metadata.get('chunk_type') == 'parent')}")
    print(f"   å­èŠ‚ç‚¹: {sum(1 for n in nodes if n.metadata.get('chunk_type') == 'child')}")

    # 3. æ„å»ºç´¢å¼•ï¼ˆåŒ…å«çˆ¶å­å…³ç³»ï¼‰
    storage_context = StorageContext.from_defaults()

    index = VectorStoreIndex(
        nodes=nodes,
        storage_context=storage_context
    )

    print("\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    # 4. ä½¿ç”¨çˆ¶æ–‡æ¡£æ£€ç´¢å™¨
    from llama_index.core.postprocessor import ParentDocumentRetrieverPostprocessor

    # è¿™é‡Œéœ€è¦é…ç½®æ£€ç´¢å™¨ä½¿ç”¨çˆ¶æ–‡æ¡£
    # ...

    return index

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from llama_index.core import Document

    doc = Document(text="""
    è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æ¡£ï¼ŒåŒ…å«å¤šä¸ªä¸»é¢˜ã€‚

    ç¬¬ä¸€ä¸ªä¸»é¢˜æ˜¯å…³äºäººå·¥æ™ºèƒ½çš„è¯¦ç»†ä»‹ç»ï¼ŒåŒ…æ‹¬å†å²ã€å‘å±•å’Œåº”ç”¨ã€‚

    ç¬¬äºŒä¸ªä¸»é¢˜è½¬å‘æœºå™¨å­¦ä¹ ï¼Œè®²è§£åŸºæœ¬æ¦‚å¿µå’Œå¸¸ç”¨ç®—æ³•ã€‚

    æœ€åä¸€ä¸ªä¸»é¢˜è®¨è®ºæ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ç¥ç»ç½‘ç»œå’Œå·ç§¯ç½‘ç»œã€‚
    """)

    index = parent_document_retrieval([doc])
```

### ç­–ç•¥5ï¼šä»£ç æ–‡æ¡£åˆ†å—

#### åŸç†

ä»£ç æ–‡æ¡£éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¿æŒå‡½æ•°å’Œç±»çš„å®Œæ•´æ€§ã€‚

```python
# ä¸å¥½çš„åˆ†å—ï¼š
def complex_function(
    param1, param2,
    param3):  # â† åˆ‡æ–­ï¼
    """å‡½æ•°è¯´æ˜"""
    pass

# å¥½çš„åˆ†å—ï¼š
# å—1: å®Œæ•´çš„å‡½æ•°å®šä¹‰
def complex_function(param1, param2, param3):
    """å‡½æ•°è¯´æ˜"""
    pass
```

#### å®ç°ä»£ç 

```python
# æ–‡ä»¶åï¼š07_05_code_chunking.py
"""
ä»£ç æ–‡æ¡£åˆ†å—
"""

from llama_index.core.node_parser import CodeSplitter

def code_chunking(code_files):
    """
    ä»£ç æ–‡æ¡£åˆ†å—

    Args:
        code_files: ä»£ç æ–‡ä»¶åˆ—è¡¨

    Returns:
        åˆ†å—åˆ—è¡¨
    """
    print("="*60)
    print("ä»£ç æ–‡æ¡£åˆ†å—æ¼”ç¤º")
    print("="*60 + "\n")

    # æ”¯æŒçš„è¯­è¨€
    language = "python"  # æˆ– "javascript", "go", "java"ç­‰

    # åˆ›å»ºä»£ç åˆ†å—å™¨
    splitter = CodeSplitter(
        language=language,
        chunk_lines=40,        # æ¯å—çº¦40è¡Œ
        chunk_lines_overlap=15, # é‡å 15è¡Œ
        max_chars=1500,        # æœ€å¤§å­—ç¬¦æ•°
    )

    # åŠ è½½ä»£ç 
    from llama_index.core import SimpleDirectoryReader
    reader = SimpleDirectoryReader(
        input_files=code_files,
        required_exts=[".py"]
    )
    documents = reader.load_data()

    # åˆ†å—
    nodes = splitter.get_nodes_from_documents(documents)

    print(f"âœ… ç”Ÿæˆäº† {len(nodes)} ä¸ªä»£ç å—")

    # æ˜¾ç¤ºç¤ºä¾‹
    for i, node in enumerate(nodes[:3], 1):
        print(f"\nå— {i}:")
        print(f"  ç±»å‹: {node.metadata.get('language')}")
        print(f"  å†…å®¹: {node.text[:150]}...")

    return nodes

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹ä»£ç 
    sample_code = """
class Calculator:
    \"\"\"è®¡ç®—å™¨ç±»\"\"\"

    def __init__(self):
        self.result = 0

    def add(self, a, b):
        \"\"\"åŠ æ³•\"\"\"
        self.result = a + b
        return self

    def subtract(self, a, b):
        \"\"\"å‡æ³•\"\"\"
        self.result = a - b
        return self

    def multiply(self, a, b):
        \"\"\"ä¹˜æ³•\"\"\"
        self.result = a * b
        return self

    def divide(self, a, b):
        \"\"\"é™¤æ³•\"\"\"
        if b == 0:
            raise ValueError("ä¸èƒ½é™¤ä»¥é›¶")
        return a / b
"""

    with open("sample.py", "w") as f:
        f.write(sample_code)

    nodes = code_chunking(["sample.py"])
```

---

## 7.3 åˆ†å—å‚æ•°ä¼˜åŒ–

### chunk_sizeä¼˜åŒ–

#### å®éªŒè®¾è®¡

```python
# æ–‡ä»¶åï¼š07_06_chunk_size_optimization.py
"""
chunk_sizeä¼˜åŒ–å®éªŒ
"""

import matplotlib.pyplot as plt
import numpy as np

def optimize_chunk_size(documents, queries, relevant_docs):
    """
    ä¼˜åŒ–chunk_sizeå‚æ•°

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        queries: æŸ¥è¯¢åˆ—è¡¨
        relevant_docs: çœŸå®ç›¸å…³æ–‡æ¡£
    """
    print("="*60)
    print("chunk_sizeä¼˜åŒ–å®éªŒ")
    print("="*60 + "\n")

    # æµ‹è¯•ä¸åŒçš„chunk_size
    chunk_sizes = [200, 400, 600, 800, 1000, 1200, 1500]

    results = {
        "chunk_size": [],
        "hit_rate": [],
        "avg_chunks": [],
        "avg_length": []
    }

    for size in chunk_sizes:
        print(f"æµ‹è¯• chunk_size={size}")

        # åˆ†å—
        splitter = SentenceSplitter(
            chunk_size=size,
            chunk_overlap=int(size * 0.1)
        )
        nodes = splitter.get_nodes_from_documents(documents)

        # è¯„ä¼°
        metrics = evaluate_retrieval(nodes, queries, relevant_docs)

        # è®°å½•
        results["chunk_size"].append(size)
        results["hit_rate"].append(metrics["hit_rate"])
        results["avg_chunks"].append(len(nodes) / len(documents))
        results["avg_length"].append(
            sum(len(n.text) for n in nodes) / len(nodes)
        )

        print(f"  Hit Rate: {metrics['hit_rate']:.2%}\n")

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 4))

    # Hit Rate
    plt.subplot(1, 3, 1)
    plt.plot(results["chunk_size"], results["hit_rate"], marker='o')
    plt.xlabel("chunk_size")
    plt.ylabel("Hit Rate")
    plt.title("Hit Rate vs chunk_size")
    plt.grid(True, alpha=0.3)

    # å¹³å‡å—æ•°
    plt.subplot(1, 3, 2)
    plt.plot(results["chunk_size"], results["avg_chunks"], marker='s', color='orange')
    plt.xlabel("chunk_size")
    plt.ylabel("å¹³å‡å—æ•°")
    plt.title("å¹³å‡å—æ•° vs chunk_size")
    plt.grid(True, alpha=0.3)

    # å¹³å‡é•¿åº¦
    plt.subplot(1, 3, 3)
    plt.plot(results["chunk_size"], results["avg_length"], marker='^', color='green')
    plt.xlabel("chunk_size")
    plt.ylabel("å¹³å‡é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰")
    plt.title("å¹³å‡é•¿åº¦ vs chunk_size")
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("./outputs/chunk_size_optimization.png", dpi=300)
    plt.show()

    return results

def evaluate_retrieval(nodes, queries, relevant_docs):
    """è¯„ä¼°æ£€ç´¢è´¨é‡"""
    # ç®€åŒ–ç‰ˆè¯„ä¼°
    # å®é™…åº”ä½¿ç”¨çœŸå®æ£€ç´¢å™¨
    return {"hit_rate": 0.7}  # å ä½
```

### chunk_overlapä¼˜åŒ–

```python
# æ–‡ä»¶åï¼š07_07_overlap_optimization.py
"""
chunk_overlapä¼˜åŒ–å®éªŒ
"""

def optimize_overlap(documents, chunk_size=1000):
    """
    ä¼˜åŒ–chunk_overlapå‚æ•°

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        chunk_size: å—å¤§å°

    Returns:
        æœ€ä¼˜overlapå€¼
    """
    print("="*60)
    print("chunk_overlapä¼˜åŒ–å®éªŒ")
    print("="*60 + "\n")

    # æµ‹è¯•ä¸åŒçš„overlap
    overlaps = [0, 50, 100, 150, 200, 250, 300]

    results = []

    for overlap in overlaps:
        print(f"æµ‹è¯• overlap={overlap}")

        # åˆ†å—
        splitter = SentenceSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap
        )
        nodes = splitter.get_nodes_from_documents(documents)

        # è¯„ä¼°
        # è¿™é‡Œç®€åŒ–è¯„ä¼°ï¼Œå®é™…éœ€è¦æ£€ç´¢æµ‹è¯•
        score = len(nodes) / overlap if overlap > 0 else len(nodes)

        results.append({
            "overlap": overlap,
            "num_chunks": len(nodes),
            "score": score
        })

        print(f"  å—æ•°: {len(nodes)}")

    # é€‰æ‹©æœ€ä¼˜
    best = max(results, key=lambda x: x["score"])
    print(f"\nâœ… æœ€ä¼˜ overlap: {best['overlap']}")

    return best["overlap"]
```

---

## 7.4 åˆ†å—ç­–ç•¥å¯¹æ¯”å®éªŒ

### å®Œæ•´å¯¹æ¯”

```python
# æ–‡ä»¶åï¼š07_08_strategy_comparison.py
"""
åˆ†å—ç­–ç•¥å®Œæ•´å¯¹æ¯”
"""

from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
    RecursiveCharacterTextSplitter
)

def compare_strategies(document, queries):
    """
    å¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥

    Args:
        document: æµ‹è¯•æ–‡æ¡£
        queries: æµ‹è¯•æŸ¥è¯¢
    """
    print("="*70)
    print("åˆ†å—ç­–ç•¥å®Œæ•´å¯¹æ¯”")
    print("="*70 + "\n")

    strategies = {
        "å›ºå®šé•¿åº¦": SentenceSplitter(
            chunk_size=500,
            chunk_overlap=0
        ),
        "å›ºå®š+é‡å ": SentenceSplitter(
            chunk_size=500,
            chunk_overlap=50
        ),
        "é€’å½’åˆ†å—": RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", " ", ""]
        )
        # è¯­ä¹‰åˆ†å—éœ€è¦APIå¯†é’¥
    }

    comparison = {}

    for name, splitter in strategies.items():
        print(f"\n{'='*70}")
        print(f"ç­–ç•¥: {name}")
        print('='*70)

        # åˆ†å—
        nodes = splitter.get_nodes_from_documents([document])

        # æ˜¾ç¤ºä¿¡æ¯
        print(f"å—æ•°: {len(nodes)}")
        print(f"å¹³å‡é•¿åº¦: {np.mean([len(n.text) for n in nodes]):.0f}")
        print(f"é•¿åº¦æ ‡å‡†å·®: {np.std([len(n.text) for n in nodes]):.0f}")

        # è¯„ä¼°ï¼ˆç®€åŒ–ï¼‰
        # å®é™…åº”è¯¥è¿›è¡ŒçœŸå®æ£€ç´¢æµ‹è¯•
        score = len(nodes) / (1 + np.std([len(n.text) for n in nodes]))

        comparison[name] = {
            "num_chunks": len(nodes),
            "avg_length": np.mean([len(n.text) for n in nodes]),
            "score": score
        }

    # æ€»ç»“å¯¹æ¯”
    print("\n" + "="*70)
    print("ç­–ç•¥å¯¹æ¯”æ€»ç»“")
    print("="*70)

    print(f"\n{'ç­–ç•¥':<12} {'å—æ•°':<8} {'å¹³å‡é•¿åº¦':<12} {'è¯„åˆ†':<10}")
    print("-" * 70)

    for name, metrics in comparison.items():
        print(f"{name:<12} {metrics['num_chunks']:<8} "
              f"{metrics['avg_length']:<12.0f} {metrics['score']:<10.2f}")

    return comparison

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from llama_index.core import Document

    doc = Document(text="""
    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯ã€‚

    å®ƒçš„ç ”ç©¶ç›®æ ‡æ˜¯åˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿã€‚

    æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚

    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚

    è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„é‡è¦åº”ç”¨é¢†åŸŸã€‚

    è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ"çœ‹æ‡‚"å›¾åƒå’Œè§†é¢‘ã€‚

    è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆä¹Ÿæ˜¯AIçš„é‡è¦åº”ç”¨ã€‚

    AIæŠ€æœ¯æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚
    """)

    queries = ["AIçš„æ ¸å¿ƒæŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ", "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ"]

    results = compare_strategies(doc, queries)
```

---

## æ€»ç»“

### æœ¬ç« è¦ç‚¹å›é¡¾

1. **åˆ†å—çš„é‡è¦æ€§**
   - å½±å“æ£€ç´¢è´¨é‡20%+
   - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
   - é¿å…ä¿¡æ¯åˆ‡æ–­

2. **5ç§é«˜çº§ç­–ç•¥**
   - è¯­ä¹‰åˆ†å—ï¼šä¿æŒè¯­ä¹‰å®Œæ•´
   - ä¸Šä¸‹æ–‡åˆ†å—å¤´ï¼šæ·»åŠ ä½ç½®ä¿¡æ¯
   - é€’å½’åˆ†å—ï¼šå¤šçº§åˆ†éš”ç¬¦
   - çˆ¶æ–‡æ¡£æ£€ç´¢ï¼šä¿æŒçˆ¶å­å…³ç³»
   - ä»£ç åˆ†å—ï¼šä¿æŒå‡½æ•°å®Œæ•´

3. **å‚æ•°ä¼˜åŒ–**
   - chunk_size: æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©
   - chunk_overlap: é€šå¸¸ä¸º10-20%
   - åˆ†éš”ç¬¦: é€’å½’ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£åˆ†å—ç­–ç•¥çš„é‡è¦æ€§
- [ ] æŒæ¡5ç§é«˜çº§åˆ†å—æ–¹æ³•
- [ ] èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„ç­–ç•¥
- [ ] å®Œæˆå‚æ•°ä¼˜åŒ–å®éªŒ
- [ ] å®é™…åº”ç”¨åˆ°é¡¹ç›®ä¸­

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- **ä¸‹ä¸€ç« **ï¼š[ç¬¬8ç« ï¼šæŸ¥è¯¢å¢å¼ºæŠ€æœ¯](./08-æŸ¥è¯¢å¢å¼ºæŠ€æœ¯.md)
- **æç¤º**ï¼šä¼˜åŒ–å®Œæ•°æ®ï¼Œæ¥ä¸‹æ¥ä¼˜åŒ–æŸ¥è¯¢

### å®è·µç»ƒä¹ 

1. **åŸºç¡€ç»ƒä¹ **
   - åœ¨ä½ çš„æ•°æ®ä¸Šæµ‹è¯•5ç§åˆ†å—ç­–ç•¥
   - å¯¹æ¯”Hit Rateå·®å¼‚
   - é€‰æ‹©æœ€ä¼˜ç­–ç•¥

2. **è¿›é˜¶ç»ƒä¹ **
   - ç»„åˆå¤šç§ç­–ç•¥
   - åˆ›å»ºè‡ªå®šä¹‰åˆ†å—å™¨
   - å¯è§†åŒ–åˆ†å—æ•ˆæœ

3. **æŒ‘æˆ˜é¡¹ç›®**
   - å®ç°è‡ªé€‚åº”åˆ†å—
   - ä¼˜åŒ–é¢†åŸŸç‰¹å®šæ–‡æ¡£
   - è¾¾åˆ°Hit Rate > 0.80

---

**è¿”å›ç›®å½•** | **ä¸Šä¸€ç« ï¼šåµŒå…¥æ¨¡å‹æ·±å…¥** | **ä¸‹ä¸€ç« ï¼šæŸ¥è¯¢å¢å¼ºæŠ€æœ¯**

---

**æœ¬ç« ç»“æŸ**

> åˆé€‚çš„åˆ†å—æ˜¯æ•°æ®è´¨é‡çš„åŸºç¡€ã€‚é€‰æ‹©æ­£ç¡®çš„åˆ†å—ç­–ç•¥å¯ä»¥è®©ä½ çš„RAGç³»ç»Ÿæ€§èƒ½æå‡5-10%ï¼Œè¿™æ˜¯æœ€å®¹æ˜“å®ç°ä¹Ÿæ˜¯æœ€å®¹æ˜“è¢«å¿½è§†çš„ä¼˜åŒ–ï¼
