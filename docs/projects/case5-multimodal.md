# æ¡ˆä¾‹5ï¼šå¤šæ¨¡æ€äº§å“é—®ç­”

> **éš¾åº¦**: â­â­â­ é«˜çº§ | **æŠ€æœ¯æ ˆ**: CLIP, GPT-4V, å¤šæ¨¡æ€Embedding, å›¾æ–‡æ£€ç´¢

æ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬æ··åˆæ£€ç´¢çš„å¤šæ¨¡æ€RAGç³»ç»Ÿ

---

## ğŸ¯ æ¡ˆä¾‹ç‰¹ç‚¹

- âœ… **å›¾æ–‡æ£€ç´¢**: CLIPå¤šæ¨¡æ€åµŒå…¥
- âœ… **GPT-4Vé›†æˆ**: è§†è§‰ç†è§£èƒ½åŠ›
- âœ… **æ··åˆæŸ¥è¯¢**: æ–‡æœ¬+å›¾ç‰‡è”åˆæ£€ç´¢
- âœ… **äº§å“å±•ç¤º**: ç”µå•†åœºæ™¯ä¼˜åŒ–

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
cd projects/case5-multimodal
pip install -r requirements.txt
python main.py
```

---

## ğŸ”‘ æ ¸å¿ƒæŠ€æœ¯

### CLIPå¤šæ¨¡æ€åµŒå…¥

```python
import clip
from PIL import Image

class MultiModalEmbedding:
    def __init__(self):
        # åŠ è½½CLIPæ¨¡å‹
        self.model, self.preprocess = clip.load("ViT-B/32", device="cuda")

    def embed_text(self, text):
        """æ–‡æœ¬åµŒå…¥"""
        tokens = clip.tokenize([text])
        with torch.no_grad():
            text_features = self.model.encode_text(tokens)
        return text_features

    def embed_image(self, image_path):
        """å›¾åƒåµŒå…¥"""
        image = self.preprocess(Image.open(image_path)).unsqueeze(0)
        with torch.no_grad():
            image_features = self.model.encode_image(image)
        return image_features

    def similarity(self, text, image_path):
        """è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦"""
        text_feat = self.embed_text(text)
        image_feat = self.embed_image(image_path)
        return cosine_similarity(text_feat, image_feat)
```

### GPT-4Vç†è§£

```python
from openai import OpenAI

def understand_image(image_path, question):
    """ä½¿ç”¨GPT-4Vç†è§£å›¾ç‰‡"""
    client = OpenAI()

    # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
    with open(image_path, "rb") as f:
        image_data = f.read()

    # è°ƒç”¨GPT-4V
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64.b64encode(image_data).decode()}"}
                }
            ]
        }]
    )

    return response.choices[0].message.content
```

---

## ğŸ“Š åº”ç”¨åœºæ™¯

### åœºæ™¯1: ä»¥å›¾æœå›¾
ç”¨æˆ·ä¸Šä¼ äº§å“å›¾ â†’ ç³»ç»Ÿæ£€ç´¢ç›¸ä¼¼äº§å“ â†’ è¿”å›æ¨èåˆ—è¡¨

### åœºæ™¯2: å›¾æ–‡æ··åˆæŸ¥è¯¢
"æ‰¾ä¸€ä¸ªçº¢è‰²è¿è¡£è£™ï¼Œæ¬¾å¼å’Œè¿™å¼ å›¾ç‰‡ç±»ä¼¼" â†’ ç»“åˆè§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æè¿°

### åœºæ™¯3: è§†è§‰é—®ç­”
ä¸Šä¼ äº§å“ç…§ç‰‡ â†’ "è¿™ä¸ªäº§å“æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ" â†’ GPT-4Våˆ†æå¹¶å›ç­”

---

## ğŸ“ å­¦ä¹ è¦ç‚¹

1. **å¤šæ¨¡æ€åµŒå…¥**
   - CLIPæ¨¡å‹åŸç†
   - å›¾æ–‡å¯¹é½
   - è”åˆå‘é‡ç©ºé—´

2. **è§†è§‰ç†è§£**
   - GPT-4Våº”ç”¨
   - å›¾åƒæè¿°ç”Ÿæˆ
   - è§†è§‰é—®ç­”

3. **æ£€ç´¢ç­–ç•¥**
   - å¤šé˜¶æ®µæ£€ç´¢
   - ç‰¹å¾èåˆ
   - ç›¸å…³æ€§è®¡ç®—

---

**[æŸ¥çœ‹å®Œæ•´æºç  â†’](https://github.com/vivy-yi/rag-tutorial/tree/main/projects/case5-multimodal)**

**[â† è¿”å›æ¡ˆä¾‹åˆ—è¡¨](index.md)**
