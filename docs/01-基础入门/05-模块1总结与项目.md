# ç¬¬5ç« ï¼šæ¨¡å—1æ€»ç»“ä¸é¡¹ç›®

> æ­å–œä½ å®Œæˆäº†æ¨¡å—1çš„å­¦ä¹ ï¼ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„å®æˆ˜é¡¹ç›®ï¼Œç»¼åˆè¿ç”¨æ‰€å­¦çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªçœŸæ­£çš„æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹ã€‚

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] å›é¡¾å’Œå·©å›ºæ¨¡å—1çš„æ ¸å¿ƒæ¦‚å¿µ
- [ ] ç»¼åˆè¿ç”¨æ‰€å­¦çŸ¥è¯†æ„å»ºå®Œæ•´é¡¹ç›®
- [ ] å®ç°ä¸€ä¸ªå¯ç”¨çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
- [ ] å»ºç«‹é¡¹ç›®è¯„ä¼°å’Œä¼˜åŒ–æµç¨‹
- [ ] ä¸ºæ¨¡å—2çš„å­¦ä¹ åšå¥½å‡†å¤‡

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š1.5å°æ—¶
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â˜†â˜†â˜†

---

## 5.1 çŸ¥è¯†å›é¡¾

### æ ¸å¿ƒæ¦‚å¿µå¤ä¹ 

è®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾æ¨¡å—1çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š

#### 1. RAGæ˜¯ä»€ä¹ˆï¼Ÿ

**å®šä¹‰**ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆä¿¡æ¯æ£€ç´¢å’ŒLLMç”Ÿæˆ

**æ ¸å¿ƒä»·å€¼**ï¼š
- âœ… å‡å°‘å¹»è§‰
- âœ… çŸ¥è¯†å®æ—¶æ›´æ–°
- âœ… å¯è§£é‡Šæ€§å¼º
- âœ… æˆæœ¬ç›¸å¯¹è¾ƒä½

**å·¥ä½œæµç¨‹**ï¼š
```
ç”¨æˆ·é—®é¢˜ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ LLMç”Ÿæˆç­”æ¡ˆ â†’ è¿”å›ç»“æœ
```

#### 2. RAGçš„5å¤§ç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ | å…³é”®æŠ€æœ¯ |
|------|------|---------|
| **æ–‡æ¡£åŠ è½½å™¨** | åŠ è½½å„ç§æ ¼å¼æ–‡æ¡£ | SimpleDirectoryReader |
| **æ–‡æœ¬åˆ†å—å™¨** | åˆ‡åˆ†é•¿æ–‡æ¡£ | SentenceSplitter |
| **åµŒå…¥æ¨¡å‹** | æ–‡æœ¬å‘é‡åŒ– | OpenAI embeddings, BGE |
| **å‘é‡æ•°æ®åº“** | é«˜æ•ˆæ£€ç´¢ | Chroma, Qdrant, Milvus |
| **LLM** | ç”Ÿæˆç­”æ¡ˆ | GPT-3.5, GPT-4, Qwen |

#### 3. å…³é”®æŠ€æœ¯å†³ç­–

**åˆ†å—ç­–ç•¥**ï¼š
- chunk_size: 500-1000ï¼ˆé€šç”¨ï¼‰
- chunk_overlap: 10-20% of chunk_size
- ä¼˜å…ˆæŒ‰æ®µè½åˆ†å—

**åµŒå…¥æ¨¡å‹é€‰æ‹©**ï¼š
- å¿«é€Ÿ/ä¾¿å®œ: OpenAI text-embedding-3-small
- é«˜è´¨é‡: OpenAI text-embedding-3-large
- ç§æœ‰éƒ¨ç½²: BGE-small-zh

**å‘é‡æ•°æ®åº“é€‰æ‹©**ï¼š
- å­¦ä¹ /åŸå‹: Chroma
- ç”Ÿäº§ç¯å¢ƒ: Qdrant, Milvus

#### 4. è¯„ä¼°æŒ‡æ ‡

**æ£€ç´¢è´¨é‡**ï¼š
- Hit Rate: å‘½ä¸­ç‡ï¼ˆ>0.85ä¼˜ç§€ï¼‰
- MRR: å¹³å‡å€’æ•°æ’åï¼ˆ>0.7ä¼˜ç§€ï¼‰
- Precision@K: å‰Kä¸ªç»“æœçš„ç²¾ç¡®ç‡

**ç”Ÿæˆè´¨é‡**ï¼š
- Faithfulness: å¿ å®åº¦ï¼ˆ>0.8ä¼˜ç§€ï¼‰
- Relevancy: ç›¸å…³æ€§ï¼ˆ>0.8ä¼˜ç§€ï¼‰

### å¸¸è§é—®é¢˜è§£ç­”

#### Q1: chunk_sizeå¦‚ä½•é€‰æ‹©ï¼Ÿ

**åŸåˆ™**ï¼šæ ¹æ®å†…å®¹ç±»å‹å’ŒæŸ¥è¯¢ç‰¹ç‚¹

```
çŸ­æ–‡æ¡£/ç²¾å‡†æŸ¥è¯¢ï¼š
  chunk_size: 300-500
  â†’ ä¿¡æ¯å¯†åº¦é«˜ï¼Œæ£€ç´¢ç²¾å‡†

é•¿æ–‡æ¡£/å¹¿æ³›æŸ¥è¯¢ï¼š
  chunk_size: 1000-1500
  â†’ ä¿æŒä¸Šä¸‹æ–‡å®Œæ•´

ä»£ç æ–‡æ¡£ï¼š
  chunk_size: 500-800
  â†’ ä¿æŒä»£ç å®Œæ•´æ€§
```

#### Q2: æ£€ç´¢æ•ˆæœä¸å¥½æ€ä¹ˆåŠï¼Ÿ

**æ’æŸ¥æ­¥éª¤**ï¼š

1. **æ£€æŸ¥æ•°æ®è´¨é‡**
   ```python
   # æŸ¥çœ‹åˆ†å—æ•ˆæœ
   for i, node in enumerate(nodes[:5]):
       print(f"å—{i+1}: {node.text[:100]}...")
   ```

2. **è°ƒæ•´æ£€ç´¢å‚æ•°**
   ```python
   # å¢åŠ æ£€ç´¢æ•°é‡
   response = index.query(question, similarity_top_k=5)
   ```

3. **ä¼˜åŒ–æŸ¥è¯¢**
   ```python
   # æŸ¥è¯¢é‡å†™
   rewritten_query = rewrite_query(user_query)
   ```

4. **å°è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹**
   ```python
   # ä»OpenAIåˆ‡æ¢åˆ°BGE
   embed_model = OpenAIEmbedding(model="text-embedding-3-large")
   ```

#### Q3: å¦‚ä½•é™ä½APIæˆæœ¬ï¼Ÿ

**ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **ç¼“å­˜å¸¸è§æŸ¥è¯¢**
   ```python
   import functools

   @functools.lru_cache(maxsize=100)
   def cached_query(question):
       return rag.query(question)
   ```

2. **ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹**
   ```python
   # GPT-3.5ä»£æ›¿GPT-4
   llm = OpenAILLM(model="gpt-3.5-turbo")
   ```

3. **æ‰¹é‡å¤„ç†**
   ```python
   # æ‰¹é‡ç”ŸæˆåµŒå…¥
   embeddings = get_embeddings(texts)  # ä¸€æ¬¡è°ƒç”¨
   ```

4. **ä½¿ç”¨å¼€æºæ¨¡å‹**
   ```python
   # æœ¬åœ°éƒ¨ç½²BGE + Qwen
   embed_model = BGEEmbedding()
   llm = QwenLLM()
   ```

---

## 5.2 å®æˆ˜é¡¹ç›®ï¼šæ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹

### é¡¹ç›®æ¦‚è¿°

**é¡¹ç›®åç§°**ï¼šInteliKB-Liteï¼ˆæ™ºèƒ½çŸ¥è¯†åº“è½»é‡ç‰ˆï¼‰

**é¡¹ç›®ç›®æ ‡**ï¼šæ„å»ºä¸€ä¸ªèƒ½å¤Ÿå›ç­”æŠ€æœ¯æ–‡æ¡£é—®é¢˜çš„æ™ºèƒ½åŠ©æ‰‹

### é¡¹ç›®éœ€æ±‚

#### åŠŸèƒ½éœ€æ±‚

1. **æ–‡æ¡£ç®¡ç†**
   - [x] æ”¯æŒPDFã€TXTã€MDæ ¼å¼æ–‡æ¡£
   - [x] æ‰¹é‡ä¸Šä¼ æ–‡æ¡£
   - [x] æ–‡æ¡£åˆ—è¡¨å±•ç¤º

2. **æ™ºèƒ½é—®ç­”**
   - [x] è‡ªç„¶è¯­è¨€æé—®
   - [x] è¿”å›å‡†ç¡®ç­”æ¡ˆ
   - [x] æ˜¾ç¤ºå‚è€ƒæ¥æº
   - [x] æ˜¾ç¤ºç½®ä¿¡åº¦

3. **ç³»ç»Ÿè¯„ä¼°**
   - [x] Hit Rateè®¡ç®—
   - [x] MRRè®¡ç®—
   - [x] ç”¨æˆ·åé¦ˆæ”¶é›†

#### æ€§èƒ½è¦æ±‚

- å“åº”æ—¶é—´ï¼š< 3ç§’
- Hit Rateï¼š> 0.6
- ç”¨æˆ·æ»¡æ„åº¦ï¼š> 70%

### é¡¹ç›®æ¶æ„

```
InteliKB-Liteæ¶æ„

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Webç•Œé¢ (Streamlit)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ æ–‡æ¡£ä¸Šä¼     â”‚  â”‚  é—®ç­”ç•Œé¢    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                  â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         RAGå¼•æ“ (LlamaIndex)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. æ–‡æ¡£å¤„ç†                   â”‚ â”‚
â”‚  â”‚ 2. åˆ†å—                       â”‚ â”‚
â”‚  â”‚ 3. å‘é‡åŒ–                     â”‚ â”‚
â”‚  â”‚ 4. æ£€ç´¢                       â”‚ â”‚
â”‚  â”‚ 5. ç”Ÿæˆ                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æ•°æ®å±‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Chroma   â”‚  â”‚  æ–‡æ¡£å­˜å‚¨        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®Œæ•´å®ç°

#### æ­¥éª¤1ï¼šé¡¹ç›®ç»“æ„

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir intelikb_lite
cd intelikb_lite

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p data/{raw,processed,eval}
mkdir -p notebooks
mkdir -p src
mkdir -p outputs

# åˆ›å»ºæ–‡ä»¶
touch README.md
touch requirements.txt
touch src/__init__.py
touch src/rag_engine.py
touch src/evaluator.py
touch app.py  # Streamlitåº”ç”¨
```

#### æ­¥éª¤2ï¼šRAGå¼•æ“å®ç°

```python
# æ–‡ä»¶åï¼šsrc/rag_engine.py
"""
InteliKB-Lite RAGå¼•æ“
"""

import os
from dotenv import load_dotenv
from pathlib import Path
from typing import List, Dict, Tuple

from llama_index.core import SimpleDirectoryReader, Document, VectorStoreIndex, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI as OpenAILLM
import chromadb

load_dotenv()

class RAGEngine:
    """RAGå¼•æ“"""

    def __init__(self, persist_dir="./chroma_db"):
        """
        åˆå§‹åŒ–RAGå¼•æ“

        Args:
            persist_dir: å‘é‡åº“æŒä¹…åŒ–ç›®å½•
        """
        self.persist_dir = persist_dir
        self.index = None
        self.query_engine = None

    def load_documents(self, data_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£

        Args:
            data_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        reader = SimpleDirectoryReader(data_path)
        documents = reader.load_data()
        return documents

    def build_index(self, documents: List[Document]):
        """
        æ„å»ºå‘é‡ç´¢å¼•

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        # åˆ†å—
        splitter = SentenceSplitter(
            chunk_size=800,
            chunk_overlap=100
        )
        nodes = splitter.get_nodes_from_documents(documents)

        # åˆ›å»ºå‘é‡åº“
        chroma_client = chromadb.PersistentClient(path=self.persist_dir)
        collection = chroma_client.get_or_create_collection("intelikb")
        vector_store = ChromaVectorStore(chroma_collection=collection)

        storage_context = StorageContext.from_defaults(vector_store=vector_store)

        # æ„å»ºç´¢å¼•
        self.index = VectorStoreIndex(
            nodes=nodes,
            storage_context=storage_context,
            embed_model=OpenAIEmbedding(model="text-embedding-3-small")
        )

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        self.query_engine = self.index.as_query_engine(
            llm=OpenAILLM(model="gpt-3.5-turbo", temperature=0.7),
            similarity_top_k=3,
            vector_store_query_mode="default"
        )

    def query(self, question: str) -> Tuple[str, List[Dict]]:
        """
        æŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            (ç­”æ¡ˆ, æ¥æºæ–‡æ¡£åˆ—è¡¨)
        """
        if self.query_engine is None:
            raise ValueError("å¼•æ“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨build_index()")

        response = self.query_engine.query(question)

        # æå–æ¥æº
        sources = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                sources.append({
                    "content": node.node.text[:200] + "...",
                    "metadata": node.node.metadata,
                    "score": node.score if hasattr(node, 'score') else 0.0
                })

        return str(response), sources

    def add_documents(self, new_documents: List[Document]):
        """
        æ·»åŠ æ–°æ–‡æ¡£

        Args:
            new_documents: æ–°æ–‡æ¡£åˆ—è¡¨
        """
        if self.index is None:
            self.build_index(new_documents)
        else:
            for doc in new_documents:
                self.index.insert(doc)

    def save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        if self.index:
            self.index.storage_context.persist()

    @classmethod
    def load_index(cls, persist_dir: str = "./chroma_db"):
        """
        åŠ è½½å·²ä¿å­˜çš„ç´¢å¼•

        Args:
            persist_dir: æŒä¹…åŒ–ç›®å½•

        Returns:
            RAGå¼•æ“å®ä¾‹
        """
        engine = cls(persist_dir)

        # åŠ è½½å‘é‡åº“
        chroma_client = chromadb.PersistentClient(path=persist_dir)
        collection = chroma_client.get_collection("intelikb")
        vector_store = ChromaVectorStore(chroma_collection=collection)

        storage_context = StorageContext.from_defaults(vector_store=vector_store)

        # åŠ è½½ç´¢å¼•
        engine.index = VectorStoreIndex.from_documents(
            [],
            storage_context=storage_context,
            embed_model=OpenAIEmbedding(model="text-embedding-3-small")
        )

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        engine.query_engine = engine.index.as_query_engine(
            llm=OpenAILLM(model="gpt-3.5-turbo", temperature=0.7),
            similarity_top_k=3
        )

        return engine
```

#### æ­¥éª¤3ï¼šè¯„ä¼°å™¨å®ç°

```python
# æ–‡ä»¶åï¼šsrc/evaluator.py
"""
è¯„ä¼°å™¨
"""

import json
from typing import List, Dict
from openai import OpenAI

class RAGEvaluator:
    """RAGè¯„ä¼°å™¨"""

    def __init__(self):
        self.client = OpenAI()

    def evaluate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """
        è¯„ä¼°å¿ å®åº¦

        Args:
            answer: ç­”æ¡ˆ
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨

        Returns:
            å¿ å®åº¦åˆ†æ•°
        """
        context_text = "\n\n".join([f"æ–‡æ¡£{i+1}: {c}" for i, c in enumerate(contexts)])

        prompt = f"""
è¯„ä¼°ç­”æ¡ˆæ˜¯å¦åŸºäºå‚è€ƒæ–‡æ¡£ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context_text}

ç­”æ¡ˆï¼š
{answer}

è¯„åˆ†ï¼ˆ0-1ï¼‰ï¼š
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            score = float(response.choices[0].message.content.strip())
            return max(0, min(1, score))
        except:
            return 0.5

    def evaluate_relevancy(self, question: str, answer: str) -> float:
        """
        è¯„ä¼°ç›¸å…³æ€§

        Args:
            question: é—®é¢˜
            answer: ç­”æ¡ˆ

        Returns:
            ç›¸å…³æ€§åˆ†æ•°
        """
        prompt = f"""
è¯„ä¼°ç­”æ¡ˆæ˜¯å¦å›ç­”äº†é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š{answer}

è¯„åˆ†ï¼ˆ0-1ï¼‰ï¼š
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            score = float(response.choices[0].message.content.strip())
            return max(0, min(1, score))
        except:
            return 0.5

    def load_golden_dataset(self, path: str) -> List[Dict]:
        """åŠ è½½é»„é‡‘æ•°æ®é›†"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def run_evaluation(self, rag_engine, golden_dataset_path: str) -> Dict:
        """
        è¿è¡Œè¯„ä¼°

        Args:
            rag_engine: RAGå¼•æ“
            golden_dataset_path: é»„é‡‘æ•°æ®é›†è·¯å¾„

        Returns:
            è¯„ä¼°ç»“æœ
        """
        dataset = self.load_golden_dataset(golden_dataset_path)

        faithfulness_scores = []
        relevancy_scores = []

        for item in dataset:
            question = item["question"]
            ground_truth = item["answer"]

            # æŸ¥è¯¢
            answer, sources = rag_engine.query(question)
            contexts = [s["content"] for s in sources]

            # è¯„ä¼°
            faith = self.evaluate_faithfulness(answer, contexts)
            rel = self.evaluate_relevancy(question, answer)

            faithfulness_scores.append(faith)
            relevancy_scores.append(rel)

        # è®¡ç®—å¹³å‡åˆ†
        results = {
            "faithfulness": sum(faithfulness_scores) / len(faithfulness_scores),
            "relevancy": sum(relevancy_scores) / len(relevancy_scores),
            "num_evaluated": len(dataset)
        }

        return results
```

#### æ­¥éª¤4ï¼šStreamlitç•Œé¢

```python
# æ–‡ä»¶åï¼šapp.py
"""
InteliKB-Lite Streamlitåº”ç”¨
"""

import streamlit as st
import os
from pathlib import Path

from src.rag_engine import RAGEngine
from src.evaluator import RAGEvaluator

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="InteliKB-Lite",
    page_icon="ğŸ¤–",
    layout="wide"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #666;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
if 'engine' not in st.session_state:
    st.session_state.engine = None
if 'documents_loaded' not in st.session_state:
    st.session_state.documents_loaded = False

def main():
    """ä¸»å‡½æ•°"""

    # æ ‡é¢˜
    st.markdown('<p class="main-header">ğŸ¤– InteliKB-Lite</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ</p>', unsafe_allow_html=True)

    st.sidebar.title("ç³»ç»Ÿè®¾ç½®")

    # ä¾§è¾¹æ ï¼šAPIå¯†é’¥
    api_key = st.sidebar.text_input("OpenAI API Key", type="password")
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key

    # ä¾§è¾¹æ ï¼šåŠŸèƒ½é€‰æ‹©
    page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["æ–‡æ¡£ç®¡ç†", "æ™ºèƒ½é—®ç­”", "ç³»ç»Ÿè¯„ä¼°"])

    # é¡µé¢è·¯ç”±
    if page == "æ–‡æ¡£ç®¡ç†":
        document_management_page()
    elif page == "æ™ºèƒ½é—®ç­”":
        qa_page()
    elif page == "ç³»ç»Ÿè¯„ä¼°":
        evaluation_page()

def document_management_page():
    """æ–‡æ¡£ç®¡ç†é¡µé¢"""
    st.header("ğŸ“„ æ–‡æ¡£ç®¡ç†")

    # ä¸Šä¼ æ–‡æ¡£
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£",
        type=["txt", "md", "pdf"],
        accept_multiple_files=True,
        help="æ”¯æŒTXTã€Markdownã€PDFæ ¼å¼"
    )

    if uploaded_files:
        # ä¿å­˜æ–‡æ¡£
        data_dir = Path("data/processed")
        data_dir.mkdir(parents=True, exist_ok=True)

        for file in uploaded_files:
            file_path = data_dir / file.name
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())

        st.success(f"âœ“ æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶")

        # æ„å»ºç´¢å¼•
        if st.button("æ„å»ºç´¢å¼•", type="primary"):
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                engine = RAGEngine()
                documents = engine.load_documents("data/processed")
                engine.build_index(documents)
                engine.save_index()

                st.session_state.engine = engine
                st.session_state.documents_loaded = True

            st.success(f"âœ“ æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£")
            st.info(f"âœ“ ç´¢å¼•å·²ä¿å­˜åˆ° ./chroma_db")

def qa_page():
    """é—®ç­”é¡µé¢"""
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½æ–‡æ¡£
    if not st.session_state.documents_loaded:
        st.warning("âš ï¸ è¯·å…ˆåœ¨'æ–‡æ¡£ç®¡ç†'é¡µé¢ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
        return

    # åŠ è½½å¼•æ“
    if st.session_state.engine is None:
        try:
            st.session_state.engine = RAGEngine.load_index()
        except:
            st.error("âŒ æ— æ³•åŠ è½½ç´¢å¼•ï¼Œè¯·é‡æ–°æ„å»º")
            return

    engine = st.session_state.engine

    # é—®ç­”ç•Œé¢
    question = st.text_area(
        "è¾“å…¥ä½ çš„é—®é¢˜",
        placeholder="ä¾‹å¦‚ï¼šPythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        height=100
    )

    if st.button("æé—®", type="primary"):
        if not question:
            st.warning("è¯·è¾“å…¥é—®é¢˜")
            return

        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            try:
                answer, sources = engine.query(question)

                # æ˜¾ç¤ºç­”æ¡ˆ
                st.subheader("ğŸ“ ç­”æ¡ˆ")
                st.write(answer)

                # æ˜¾ç¤ºæ¥æº
                st.subheader("ğŸ“š å‚è€ƒæ¥æº")
                for i, source in enumerate(sources, 1):
                    with st.expander(f"æ¥æº {i}"):
                        st.write(source["content"])
                        st.caption(f"ç›¸å…³åº¦: {source['score']:.3f}")

            except Exception as e:
                st.error(f"âŒ å‡ºé”™: {str(e)}")

def evaluation_page():
    """è¯„ä¼°é¡µé¢"""
    st.header("ğŸ“Š ç³»ç»Ÿè¯„ä¼°")

    # ä¸Šä¼ é»„é‡‘æ•°æ®é›†
    golden_file = st.file_uploader(
        "ä¸Šä¼ é»„é‡‘æ•°æ®é›†",
        type=["json"],
        help="JSONæ ¼å¼çš„é—®ç­”å¯¹"
    )

    if golden_file:
        # ä¿å­˜æ•°æ®é›†
        data_dir = Path("data/eval")
        data_dir.mkdir(parents=True, exist_ok=True)

        dataset_path = data_dir / "golden_dataset.json"
        with open(dataset_path, "wb") as f:
            f.write(golden_file.getbuffer())

        st.success("âœ“ æ•°æ®é›†å·²ä¸Šä¼ ")

        # è¿è¡Œè¯„ä¼°
        if st.button("å¼€å§‹è¯„ä¼°", type="primary"):
            if not st.session_state.documents_loaded:
                st.warning("âš ï¸ è¯·å…ˆå¤„ç†æ–‡æ¡£")
                return

            with st.spinner("æ­£åœ¨è¯„ä¼°..."):
                try:
                    evaluator = RAGEvaluator()
                    results = evaluator.run_evaluation(
                        st.session_state.engine,
                        str(dataset_path)
                    )

                    # æ˜¾ç¤ºç»“æœ
                    st.subheader("è¯„ä¼°ç»“æœ")

                    col1, col2 = st.columns(2)

                    with col1:
                        st.metric(
                            "å¿ å®åº¦",
                            f"{results['faithfulness']:.2%}",
                            help="ç­”æ¡ˆåŸºäºæ–‡æ¡£çš„ç¨‹åº¦"
                        )

                    with col2:
                        st.metric(
                            "ç›¸å…³æ€§",
                            f"{results['relevancy']:.2%}",
                            help="ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦"
                        )

                    st.info(f"âœ“ è¯„ä¼°äº† {results['num_evaluated']} ä¸ªé—®ç­”å¯¹")

                    # è¯„çº§
                    avg_score = (results['faithfulness'] + results['relevancy']) / 2

                    if avg_score > 0.8:
                        st.success("ğŸ† ä¼˜ç§€ï¼ç³»ç»Ÿè¡¨ç°è‰¯å¥½")
                    elif avg_score > 0.6:
                        st.info("ğŸ‘ è‰¯å¥½ï¼Œä»æœ‰ä¼˜åŒ–ç©ºé—´")
                    else:
                        st.warning("âš ï¸ éœ€è¦æ”¹è¿›")

                except Exception as e:
                    st.error(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()
```

#### æ­¥éª¤5ï¼šä¾èµ–æ–‡ä»¶

```python
# æ–‡ä»¶åï¼šrequirements.txt
# InteliKB-Liteä¾èµ–

# æ ¸å¿ƒæ¡†æ¶
llama-index-core>=0.10.0
llama-index-llms-openai>=0.1.0
llama-index-embeddings-openai>=0.1.0
llama-index-vector-stores-chroma>=0.1.0

# å‘é‡æ•°æ®åº“
chromadb>=0.4.0

# æ–‡æ¡£å¤„ç†
pypdf>=3.0.0
python-dotenv>=1.0.0

# Webç•Œé¢
streamlit>=1.28.0

# è¯„ä¼°
openai>=1.0.0
```

### é¡¹ç›®è¿è¡ŒæŒ‡å—

#### 1. å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### 2. å‡†å¤‡æ•°æ®

```bash
# å°†æ–‡æ¡£æ”¾å…¥data/processedç›®å½•
# æ”¯æŒæ ¼å¼ï¼š.txt, .md, .pdf
```

#### 3. é…ç½®APIå¯†é’¥

```bash
# åˆ›å»º.envæ–‡ä»¶
echo "OPENAI_API_KEY=your_api_key_here" > .env
```

#### 4. å¯åŠ¨åº”ç”¨

```bash
# å¯åŠ¨Streamlit
streamlit run app.py
```

#### 5. ä½¿ç”¨æŒ‡å—

**æ­¥éª¤1ï¼šæ–‡æ¡£ç®¡ç†**
- ä¸Šä¼ ä½ çš„æ–‡æ¡£ï¼ˆTXTã€MDã€PDFï¼‰
- ç‚¹å‡»"æ„å»ºç´¢å¼•"
- ç­‰å¾…å¤„ç†å®Œæˆ

**æ­¥éª¤2ï¼šæ™ºèƒ½é—®ç­”**
- è¾“å…¥é—®é¢˜
- æŸ¥çœ‹ç­”æ¡ˆå’Œå‚è€ƒæ¥æº
- è¯„ä¼°ç­”æ¡ˆè´¨é‡

**æ­¥éª¤3ï¼šç³»ç»Ÿè¯„ä¼°**
- ä¸Šä¼ é»„é‡‘æ•°æ®é›†ï¼ˆJSONæ ¼å¼ï¼‰
- è¿è¡Œè¯„ä¼°
- æŸ¥çœ‹è¯„ä¼°ç»“æœ

---

## é¡¹ç›®äº¤ä»˜æ¸…å•

### å¿…é¡»å®Œæˆ

- [ ] å®Œæ•´çš„ä»£ç å®ç°
- [ ] èƒ½å¤Ÿå¤„ç†PDFå’ŒTXTæ–‡æ¡£
- [ ] åŸºç¡€é—®ç­”åŠŸèƒ½æ­£å¸¸
- [ ] æ˜¾ç¤ºå‚è€ƒæ¥æº
- [ ] åŸºç¡€è¯„ä¼°æŒ‡æ ‡

### åŠ åˆ†é¡¹

- [ ] ç”¨æˆ·åé¦ˆæ”¶é›†åŠŸèƒ½
- [ ] å¤šè½®å¯¹è¯æ”¯æŒ
- [ ] ç­”æ¡ˆç¼“å­˜æœºåˆ¶
- [ ] æŸ¥è¯¢å†å²è®°å½•
- [ ] å¯è§†åŒ–è¯„ä¼°ç»“æœ

### æ–‡æ¡£è¦æ±‚

- [ ] README.mdï¼ˆé¡¹ç›®è¯´æ˜ï¼‰
- [ ] requirements.txtï¼ˆä¾èµ–æ¸…å•ï¼‰
- [ ] ä½¿ç”¨è¯´æ˜ï¼ˆç”¨æˆ·æŒ‡å—ï¼‰
- [ ] è¯„ä¼°æŠ¥å‘Š

---

## è¯„åˆ†æ ‡å‡†

| é¡¹ç›® | å æ¯” | è¯„åˆ†æ ‡å‡† |
|------|------|---------|
| **åŠŸèƒ½å®Œæ•´æ€§** | 40% | å®ç°æ‰€æœ‰å¿…é¡»åŠŸèƒ½ |
| **ä»£ç è´¨é‡** | 30% | ä»£ç è§„èŒƒã€æ³¨é‡Šå®Œæ•´ã€ç»“æ„æ¸…æ™° |
| **ç”¨æˆ·ä½“éªŒ** | 20% | ç•Œé¢å‹å¥½ã€äº¤äº’æµç•… |
| **æ–‡æ¡£è¯´æ˜** | 10% | READMEå®Œæ•´ã€ä½¿ç”¨è¯´æ˜æ¸…æ™° |

---

## æ€»ç»“

### æ¨¡å—1å­¦ä¹ æˆæœ

å®Œæˆæ¨¡å—1åï¼Œä½ å·²ç»ï¼š

1. **æŒæ¡äº†RAGåŸºç¡€**
   - ç†è§£RAGçš„æ ¸å¿ƒæ¦‚å¿µå’Œä»·å€¼
   - äº†è§£5å¤§æ ¸å¿ƒç»„ä»¶çš„ä½œç”¨
   - çŸ¥é“å¦‚ä½•é€‰æ‹©åˆé€‚çš„æŠ€æœ¯æ ˆ

2. **å®ç°äº†åŸºç¡€RAGç³»ç»Ÿ**
   - èƒ½å¤ŸåŠ è½½å’Œå¤„ç†æ–‡æ¡£
   - å®ç°äº†å‘é‡æ£€ç´¢
   - ç”Ÿæˆäº†åŸºäºæ–‡æ¡£çš„ç­”æ¡ˆ

3. **å»ºç«‹äº†è¯„ä¼°ä½“ç³»**
   - çŸ¥é“å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿ
   - æŒæ¡äº†å…³é”®è¯„ä¼°æŒ‡æ ‡
   - èƒ½å¤ŸåŸºäºè¯„ä¼°ä¼˜åŒ–ç³»ç»Ÿ

4. **å®Œæˆäº†å®æˆ˜é¡¹ç›®**
   - æ„å»ºäº†å®Œæ•´çš„æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
   - ç»¼åˆè¿ç”¨äº†æ‰€å­¦çŸ¥è¯†
   - è·å¾—äº†å®é™…é¡¹ç›®ç»éªŒ

### ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘

**æ¨¡å—2é¢„å‘Š**ï¼šæ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

- âœ¨ è¯­ä¹‰åˆ†å—
- âœ¨ æŸ¥è¯¢å¢å¼ºï¼ˆHyDEï¼‰
- âœ¨ æ··åˆæ£€ç´¢
- âœ¨ é‡æ’åº
- âœ¨ æ€§èƒ½ä¼˜åŒ–

**é¢„è®¡æå‡**ï¼š
- Hit Rate: 0.6 â†’ 0.75 (+25%)
- MRR: 0.5 â†’ 0.65 (+30%)
- ç”¨æˆ·æ»¡æ„åº¦: 70% â†’ 85% (+21%)

---

## æ‰©å±•ç»ƒä¹ 

### è¿›é˜¶æŒ‘æˆ˜

1. **ä¼˜åŒ–æ£€ç´¢è´¨é‡**
   - å®ç°æŸ¥è¯¢é‡å†™
   - å°è¯•ä¸åŒçš„åˆ†å—ç­–ç•¥
   - ä½¿ç”¨æ›´å¼ºçš„åµŒå…¥æ¨¡å‹

2. **æ”¹è¿›ç”¨æˆ·ä½“éªŒ**
   - æ·»åŠ æŸ¥è¯¢å†å²
   - å®ç°ç›¸ä¼¼é—®é¢˜æ¨è
   - æ”¯æŒæ–‡æ¡£é«˜äº®æ˜¾ç¤º

3. **æ€§èƒ½ä¼˜åŒ–**
   - å®ç°ç­”æ¡ˆç¼“å­˜
   - ä¼˜åŒ–ç´¢å¼•ç»“æ„
   - å‡å°‘APIè°ƒç”¨

### åˆ›æ–°é¡¹ç›®

1. **å¤šè¯­è¨€RAG**
   - æ”¯æŒä¸­è‹±æ–‡æ··åˆ
   - è·¨è¯­è¨€æ£€ç´¢

2. **å®æ—¶æ›´æ–°**
   - æ–‡æ¡£å¢é‡ç´¢å¼•
   - çƒ­æ›´æ–°æœºåˆ¶

3. **å¤šæ¨¡æ€æ”¯æŒ**
   - å›¾åƒé—®ç­”
   - è¡¨æ ¼ç†è§£

---

## å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»

1. **LlamaIndexæ–‡æ¡£**
   - é“¾æ¥ï¼š[https://docs.llamaindex.ai/](https://docs.llamaindex.ai/)
   - é‡ç‚¹ï¼šæŸ¥è¯¢å¼•æ“ã€ç´¢å¼•ç±»å‹

2. **Chromaæ–‡æ¡£**
   - é“¾æ¥ï¼š[https://docs.trychroma.com/](https://docs.trychroma.com/)
   - é‡ç‚¹ï¼šé›†åˆæ“ä½œã€æŸ¥è¯¢ä¼˜åŒ–

3. **RAGè¯„ä¼°è®ºæ–‡**
   - é“¾æ¥ï¼š[https://arxiv.org/abs/2309.15217](https://arxiv.org/abs/2309.15217)
   - é‡ç‚¹ï¼šRAGASè¯„ä¼°æ¡†æ¶

### ç›¸å…³é¡¹ç›®

- **LlamaIndexç¤ºä¾‹**: [https://github.com/run-llama/llama_index/tree/main/examples](https://github.com/run-llama/llama_index/tree/main/examples)
- **RAGAS**: [https://github.com/explodinggradients/ragas](https://github.com/explodinggradients/ragas)

---

**æ­å–œå®Œæˆæ¨¡å—1ï¼** ğŸ‰

ä½ å·²ç»æŒæ¡äº†RAGçš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶æ„å»ºäº†ç¬¬ä¸€ä¸ªå®Œæ•´çš„åº”ç”¨ã€‚å‡†å¤‡å¥½è¿æ¥æ›´é«˜çº§çš„æŒ‘æˆ˜äº†å—ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥æ¨¡å—2ï¼Œå­¦ä¹ å¦‚ä½•ä¼˜åŒ–RAGç³»ç»Ÿï¼

---

**æ¨¡å—1 å®Œ**

**è¿”å›ç›®å½•** | **ä¸Šä¸€ç« ï¼šRAGè¯„ä¼°åŸºç¡€** | **ä¸‹ä¸€æ¨¡å—ï¼šæ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯**

---

**ä½œè€…ç¬”è®°**

> æ¨¡å—1æ˜¯æ•´ä¸ªæ•™ç¨‹çš„åŸºç¡€ã€‚å¦‚æœä½ è§‰å¾—å†…å®¹å¾ˆå¤šæˆ–æœ‰ç‚¹å›°éš¾ï¼Œä¸è¦æ‹…å¿ƒã€‚è¿™æ˜¯æ­£å¸¸çš„ï¼å»ºè®®ï¼š
>
> 1. **å…ˆç†è§£æ ¸å¿ƒæ¦‚å¿µ**ï¼Œä¸è¦çº ç»“äºæ¯ä¸ªç»†èŠ‚
> 2. **åŠ¨æ‰‹è¿è¡Œä»£ç **ï¼Œåœ¨å®è·µä¸­ç†è§£
> 3. **å®Œæˆå®æˆ˜é¡¹ç›®**ï¼Œå·©å›ºæ‰€å­¦çŸ¥è¯†
> 4. **è®°å½•é—®é¢˜**ï¼Œåœ¨åç»­å­¦ä¹ ä¸­å¯»æ‰¾ç­”æ¡ˆ
>
> è®°ä½ï¼Œå­¦ä¹ æ˜¯ä¸€ä¸ªæ¸è¿›çš„è¿‡ç¨‹ã€‚æŒæ¡åŸºç¡€åï¼Œæ¨¡å—2ä¼šæ›´å®¹æ˜“ç†è§£ã€‚
