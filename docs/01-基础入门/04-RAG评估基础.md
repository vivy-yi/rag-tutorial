# ç¬¬4ç« ï¼šRAGè¯„ä¼°åŸºç¡€

> å¦‚ä½•çŸ¥é“ä½ çš„RAGç³»ç»Ÿå¥½ä¸å¥½ï¼Ÿæœ¬ç« å°†å¸¦ä½ å»ºç«‹å®Œæ•´çš„è¯„ä¼°ä½“ç³»ï¼Œç”¨ç§‘å­¦çš„æ–¹æ³•è¡¡é‡å’Œæ”¹è¿›RAGç³»ç»Ÿã€‚

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£RAGè¯„ä¼°çš„é‡è¦æ€§å’Œè¯„ä¼°ç»´åº¦
- [ ] å®ç°æ£€ç´¢è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼ˆHit Rateã€MRRï¼‰
- [ ] å®ç°ç”Ÿæˆè´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼ˆFaithfulnessã€Relevancyï¼‰
- [ ] ä½¿ç”¨RAGASè¯„ä¼°æ¡†æ¶
- [ ] å»ºç«‹è‡ªåŠ¨åŒ–çš„è¯„ä¼°æµç¨‹
- [ ] åŸºäºè¯„ä¼°ç»“æœä¼˜åŒ–RAGç³»ç»Ÿ

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š1.5å°æ—¶
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â˜†â˜†â˜†

---

## å‰ç½®çŸ¥è¯†

åœ¨å¼€å§‹æœ¬ç« å­¦ä¹ å‰ï¼Œä½ éœ€è¦ï¼š

- [ ] å®Œæˆç¬¬3ç« ï¼Œæœ‰ä¸€ä¸ªå¯è¿è¡Œçš„RAGç³»ç»Ÿ
- [ ] ç†è§£å‘é‡æ£€ç´¢çš„åŸºæœ¬åŸç†
- [ ] äº†è§£åŸºç¡€çš„ç»Ÿè®¡æŒ‡æ ‡æ¦‚å¿µ

**ç¯å¢ƒè¦æ±‚**ï¼š
- å·²å®‰è£…RAGç³»ç»Ÿ
- éœ€è¦é¢å¤–çš„è¯„ä¼°åº“ï¼šragas

---

## 4.1 ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°ï¼Ÿ

### è¯„ä¼°çš„é‡è¦æ€§

#### é—®é¢˜ï¼šæ„Ÿè§‰å¥½ â‰  å®é™…å¥½

```
å¼€å‘è€…è§†è§’ï¼š
  "æˆ‘çš„RAGç³»ç»Ÿçœ‹èµ·æ¥å·¥ä½œæ­£å¸¸ï¼"
  â†’ å¶å°”æµ‹è¯•å‡ æ¬¡ï¼Œè§‰å¾—è¿˜è¡Œ

ç”¨æˆ·è§†è§’ï¼š
  "ä¸ºä»€ä¹ˆæ€»æ˜¯æ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Ÿ"
  â†’ å®é™…ä½¿ç”¨æ—¶é¢‘ç¹é‡åˆ°é—®é¢˜
```

#### éœ€è¦ç§‘å­¦çš„è¯„ä¼°æ–¹æ³•

**è¯„ä¼°çš„ç›®çš„**ï¼š

1. **é‡åŒ–æ€§èƒ½**ï¼šç”¨æ•°å­—è¯´è¯ï¼Œä¸æ˜¯é æ„Ÿè§‰
2. **å‘ç°ç“¶é¢ˆ**ï¼šæ‰¾å‡ºç³»ç»Ÿçš„è–„å¼±ç¯èŠ‚
3. **æŒ‡å¯¼ä¼˜åŒ–**ï¼šçŸ¥é“è¯¥æ”¹è¿›ä»€ä¹ˆ
4. **å¯¹æ¯”æ–¹æ¡ˆ**ï¼šå®¢è§‚æ¯”è¾ƒä¸åŒæŠ€æœ¯
5. **æŒç»­ç›‘æ§**ï¼šç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ

### RAGè¯„ä¼°çš„ä¸‰ä¸ªç»´åº¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RAGè¯„ä¼°ç»´åº¦                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  1. æ£€ç´¢è´¨é‡ (Retrieval Quality)       â”‚
â”‚     - æ‰¾åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³ï¼Ÿ              â”‚
â”‚     - ç›¸å…³æ–‡æ¡£æ˜¯å¦æ’åœ¨å‰é¢ï¼Ÿ            â”‚
â”‚     æŒ‡æ ‡ï¼šHit Rate, MRR, Precision@K    â”‚
â”‚                                         â”‚
â”‚  2. ç”Ÿæˆè´¨é‡ (Generation Quality)      â”‚
â”‚     - ç­”æ¡ˆæ˜¯å¦å‡†ç¡®ï¼Ÿ                    â”‚
â”‚     - æ˜¯å¦åŸºäºæ£€ç´¢æ–‡æ¡£ï¼Ÿ                â”‚
â”‚     æŒ‡æ ‡ï¼šFaithfulness, Relevancy       â”‚
â”‚                                         â”‚
â”‚  3. ç³»ç»Ÿæ€§èƒ½ (System Performance)      â”‚
â”‚     - å“åº”é€Ÿåº¦å¦‚ä½•ï¼Ÿ                    â”‚
â”‚     - èƒ½å¤„ç†å¤šå°‘è¯·æ±‚ï¼Ÿ                  â”‚
â”‚     æŒ‡æ ‡ï¼šå»¶è¿Ÿ, QPS, æˆæœ¬               â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.2 æ£€ç´¢è´¨é‡è¯„ä¼°

### Hit Rateï¼ˆå‘½ä¸­ç‡ï¼‰

#### å®šä¹‰

**Hit Rate**ï¼šè‡³å°‘æ£€ç´¢åˆ°ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢å æ¯”

```
Hit Rate = æœ‰ç›¸å…³ç»“æœçš„æŸ¥è¯¢æ•° / æ€»æŸ¥è¯¢æ•°
```

#### è®¡ç®—

```python
# æ–‡ä»¶åï¼š04_01_hit_rate.py
"""
Hit Rateè®¡ç®—ç¤ºä¾‹
"""

def calculate_hit_rate(queries, retrieved_docs, relevant_docs):
    """
    è®¡ç®—Hit Rate

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆæ¯ä¸ªæŸ¥è¯¢çš„top-kç»“æœï¼‰
        relevant_docs: çœŸå®ç›¸å…³æ–‡æ¡£åˆ—è¡¨

    Returns:
        hit_rate: å‘½ä¸­ç‡
    """
    hits = 0

    for query_id, retrieved in enumerate(retrieved_docs):
        # è·å–è¯¥æŸ¥è¯¢çš„çœŸå®ç›¸å…³æ–‡æ¡£
        relevant = set(relevant_docs[query_id])

        # æ£€æŸ¥æ£€ç´¢ç»“æœä¸­æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªç›¸å…³æ–‡æ¡£
        if any(doc_id in relevant for doc_id in retrieved):
            hits += 1

    hit_rate = hits / len(queries)
    return hit_rate

# ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    queries = ["Q1", "Q2", "Q3", "Q4", "Q5"]

    # æ£€ç´¢ç»“æœï¼ˆæ¯ä¸ªæŸ¥è¯¢çš„top-3æ–‡æ¡£IDï¼‰
    retrieved_docs = [
        [1, 5, 8],    # Q1çš„ç»“æœ
        [2, 6, 9],    # Q2çš„ç»“æœ
        [3, 7, 10],   # Q3çš„ç»“æœ
        [4, 8, 12],   # Q4çš„ç»“æœ
        [5, 9, 13]    # Q5çš„ç»“æœ
    ]

    # çœŸå®ç›¸å…³æ–‡æ¡£
    relevant_docs = [
        {1, 8},      # Q1çš„ç›¸å…³æ–‡æ¡£
        {6},         # Q2çš„ç›¸å…³æ–‡æ¡£
        {11, 12},    # Q3çš„ç›¸å…³æ–‡æ¡£
        {4, 15},     # Q4çš„ç›¸å…³æ–‡æ¡£
        {9, 13}      # Q5çš„ç›¸å…³æ–‡æ¡£
    ]

    # è®¡ç®—Hit Rate
    hit_rate = calculate_hit_rate(queries, retrieved_docs, relevant_docs)

    print("Hit Rateè®¡ç®—ç¤ºä¾‹")
    print("="*50)
    for i, (retrieved, relevant) in enumerate(zip(retrieved_docs, relevant_docs), 1):
        hit = any(doc in relevant for doc in retrieved)
        status = "âœ“" if hit else "âœ—"
        print(f"Q{i}: {status} æ£€ç´¢={retrieved}, ç›¸å…³={relevant}")

    print(f"\nHit Rate: {hit_rate:.2%}")
    print(f"è§£é‡Š: {hit_rate:.0%}çš„æŸ¥è¯¢è‡³å°‘æ£€ç´¢åˆ°ä¸€ä¸ªç›¸å…³æ–‡æ¡£")
```

#### åŸºå‡†å€¼

| Hit Rate | è¯„çº§ | è¯´æ˜ |
|----------|------|------|
| < 0.5 | å·® | éœ€è¦å¤§å¹…æ”¹è¿› |
| 0.5 - 0.7 | ä¸­ | åŸºæœ¬å¯ç”¨ |
| 0.7 - 0.85 | è‰¯å¥½ | è¡¨ç°ä¸é”™ |
| > 0.85 | ä¼˜ç§€ | è¾¾åˆ°ç”Ÿäº§æ ‡å‡† |

### MRRï¼ˆMean Reciprocal Rankï¼‰

#### å®šä¹‰

**MRR**ï¼šå¹³å‡å€’æ•°æ’åï¼Œè¡¡é‡ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„å¹³å‡æ’å

```
MRR = (1 / ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æ’å) çš„å¹³å‡å€¼
```

#### è®¡ç®—

```python
def calculate_mrr(retrieved_docs, relevant_docs):
    """
    è®¡ç®—MRR

    Args:
        retrieved_docs: æ£€ç´¢ç»“æœ
        relevant_docs: çœŸå®ç›¸å…³æ–‡æ¡£

    Returns:
        mrr: å¹³å‡å€’æ•°æ’å
    """
    reciprocal_ranks = []

    for retrieved, relevant in zip(retrieved_docs, relevant_docs):
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®
        for rank, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                reciprocal_ranks.append(1 / rank)
                break
        else:
            # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
            reciprocal_ranks.append(0)

    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)
    return mrr

# ç¤ºä¾‹
if __name__ == "__main__":
    retrieved_docs = [
        [1, 5, 8],    # Q1ï¼šç›¸å…³æ–‡æ¡£æ˜¯1ï¼Œæ’ç¬¬1
        [2, 6, 9],    # Q2ï¼šç›¸å…³æ–‡æ¡£æ˜¯6ï¼Œæ’ç¬¬2
        [3, 7, 10],   # Q3ï¼šç›¸å…³æ–‡æ¡£æ˜¯11ï¼Œä¸åœ¨ç»“æœä¸­
        [4, 8, 12],   # Q4ï¼šç›¸å…³æ–‡æ¡£æ˜¯4ï¼Œæ’ç¬¬1
        [5, 9, 13]    # Q5ï¼šç›¸å…³æ–‡æ¡£æ˜¯13ï¼Œæ’ç¬¬3
    ]

    relevant_docs = [
        {1},         # Q1
        {6},         # Q2
        {11},        # Q3
        {4},         # Q4
        {13}         # Q5
    ]

    mrr = calculate_mrr(retrieved_docs, relevant_docs)

    print("\nMRRè®¡ç®—ç¤ºä¾‹")
    print("="*50)
    print(f"Q1: ç›¸å…³æ–‡æ¡£æ’ç¬¬1 â†’ 1/1 = 1.000")
    print(f"Q2: ç›¸å…³æ–‡æ¡£æ’ç¬¬2 â†’ 1/2 = 0.500")
    print(f"Q3: ç›¸å…³æ–‡æ¡£æœªæ£€ç´¢åˆ° â†’ 0.000")
    print(f"Q4: ç›¸å…³æ–‡æ¡£æ’ç¬¬1 â†’ 1/1 = 1.000")
    print(f"Q5: ç›¸å…³æ–‡æ¡£æ’ç¬¬3 â†’ 1/3 = 0.333")
    print(f"\nMRR: (1.0 + 0.5 + 0.0 + 1.0 + 0.333) / 5 = {mrr:.3f}")
```

#### åŸºå‡†å€¼

| MRR | è¯„çº§ | è¯´æ˜ |
|-----|------|------|
| < 0.3 | å·® | ç›¸å…³æ–‡æ¡£æ’åé å |
| 0.3 - 0.5 | ä¸­ | ç›¸å…³æ–‡æ¡£åœ¨ä¸­æ¸¸ |
| 0.5 - 0.7 | è‰¯å¥½ | ç›¸å…³æ–‡æ¡£é å‰ |
| > 0.7 | ä¼˜ç§€ | ç›¸å…³æ–‡æ¡£é€šå¸¸åœ¨å‰3 |

### Precision@K

#### å®šä¹‰

**Precision@K**ï¼šå‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹

```python
def calculate_precision_at_k(retrieved_docs, relevant_docs, k=5):
    """
    è®¡ç®—Precision@K

    Args:
        retrieved_docs: æ£€ç´¢ç»“æœ
        relevant_docs: çœŸå®ç›¸å…³æ–‡æ¡£
        k: å‰Kä¸ªç»“æœ

    Returns:
        precision_at_k: ç²¾ç¡®ç‡
    """
    precisions = []

    for retrieved, relevant in zip(retrieved_docs, relevant_docs):
        # å–å‰Kä¸ªç»“æœ
        top_k = retrieved[:k]

        # è®¡ç®—ç›¸å…³æ–‡æ¡£æ•°é‡
        relevant_count = sum(1 for doc in top_k if doc in relevant)

        # ç²¾ç¡®ç‡
        precision = relevant_count / k
        precisions.append(precision)

    avg_precision = sum(precisions) / len(precisions)
    return avg_precision
```

### å®Œæ•´æ£€ç´¢è¯„ä¼°æ¡†æ¶

```python
# æ–‡ä»¶åï¼š04_02_retrieval_eval.py
"""
å®Œæ•´çš„æ£€ç´¢è¯„ä¼°æ¡†æ¶
"""

class RetrievalEvaluator:
    """æ£€ç´¢è´¨é‡è¯„ä¼°å™¨"""

    def __init__(self):
        self.metrics = {}

    def evaluate(self, queries, retrieved_docs, relevant_docs):
        """
        å…¨é¢è¯„ä¼°æ£€ç´¢è´¨é‡

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            retrieved_docs: æ£€ç´¢ç»“æœ
            relevant_docs: çœŸå®ç›¸å…³æ–‡æ¡£

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        results = {
            "hit_rate": self.calculate_hit_rate(retrieved_docs, relevant_docs),
            "mrr": self.calculate_mrr(retrieved_docs, relevant_docs),
            "precision_at_1": self.calculate_precision_at_k(retrieved_docs, relevant_docs, k=1),
            "precision_at_3": self.calculate_precision_at_k(retrieved_docs, relevant_docs, k=3),
            "precision_at_5": self.calculate_precision_at_k(retrieved_docs, relevant_docs, k=5),
        }

        self.metrics = results
        return results

    def calculate_hit_rate(self, retrieved_docs, relevant_docs):
        """è®¡ç®—Hit Rate"""
        hits = sum(
            1 for retrieved, relevant in zip(retrieved_docs, relevant_docs)
            if any(doc in relevant for doc in retrieved)
        )
        return hits / len(retrieved_docs)

    def calculate_mrr(self, retrieved_docs, relevant_docs):
        """è®¡ç®—MRR"""
        reciprocal_ranks = []
        for retrieved, relevant in zip(retrieved_docs, relevant_docs):
            for rank, doc in enumerate(retrieved, 1):
                if doc in relevant:
                    reciprocal_ranks.append(1 / rank)
                    break
            else:
                reciprocal_ranks.append(0)
        return sum(reciprocal_ranks) / len(reciprocal_ranks)

    def calculate_precision_at_k(self, retrieved_docs, relevant_docs, k):
        """è®¡ç®—Precision@K"""
        precisions = []
        for retrieved, relevant in zip(retrieved_docs, relevant_docs):
            top_k = retrieved[:k]
            relevant_count = sum(1 for doc in top_k if doc in relevant)
            precisions.append(relevant_count / k)
        return sum(precisions) / len(precisions)

    def print_report(self):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("æ£€ç´¢è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        print("="*60)

        for metric, value in self.metrics.items():
            # æ ¼å¼åŒ–æŒ‡æ ‡åç§°
            metric_name = metric.replace("_", " ").title()
            # æ ¼å¼åŒ–å€¼
            if isinstance(value, float):
                print(f"{metric_name:20s}: {value:.3f}")
            else:
                print(f"{metric_name:20s}: {value}")

        print("="*60)

        # è¯„çº§
        hit_rate = self.metrics.get("hit_rate", 0)
        mrr = self.metrics.get("mrr", 0)

        if hit_rate > 0.85 and mrr > 0.7:
            rating = "ä¼˜ç§€ â­â­â­â­â­"
        elif hit_rate > 0.7 and mrr > 0.5:
            rating = "è‰¯å¥½ â­â­â­â­"
        elif hit_rate > 0.5 and mrr > 0.3:
            rating = "ä¸­ç­‰ â­â­â­"
        else:
            rating = "éœ€è¦æ”¹è¿› â­â­"

        print(f"ç»¼åˆè¯„çº§: {rating}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    queries = [f"Q{i}" for i in range(1, 11)]

    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœï¼ˆæ–‡æ¡£IDï¼‰
    retrieved_docs = [
        [1, 5, 8, 12, 15],
        [2, 6, 9, 13, 16],
        [3, 7, 10, 14, 17],
        [4, 8, 11, 15, 18],
        [5, 9, 12, 16, 19],
        [1, 6, 11, 16, 20],
        [2, 7, 12, 17, 21],
        [3, 8, 13, 18, 22],
        [4, 9, 14, 19, 23],
        [5, 10, 15, 20, 24]
    ]

    # çœŸå®ç›¸å…³æ–‡æ¡£
    relevant_docs = [
        {1, 8},
        {6, 13},
        {3},
        {15},
        {9},
        {1, 11},
        {2, 17},
        {8, 13},
        {14},
        {20}
    ]

    # è¯„ä¼°
    evaluator = RetrievalEvaluator()
    metrics = evaluator.evaluate(queries, retrieved_docs, relevant_docs)

    # æ‰“å°æŠ¥å‘Š
    evaluator.print_report()
```

---

## 4.3 ç”Ÿæˆè´¨é‡è¯„ä¼°

### Faithfulnessï¼ˆå¿ å®åº¦ï¼‰

#### å®šä¹‰

**ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£**ï¼Œè€Œä¸æ˜¯LLMç¼–é€ çš„ã€‚

#### è¯„ä¼°æ–¹æ³•

```python
# æ–‡ä»¶åï¼š04_03_faithfulness.py
"""
Faithfulnessè¯„ä¼°ç¤ºä¾‹
"""

from openai import OpenAI

def evaluate_faithfulness(answer, context_documents):
    """
    è¯„ä¼°ç­”æ¡ˆçš„å¿ å®åº¦

    Args:
        answer: RAGç”Ÿæˆçš„ç­”æ¡ˆ
        context_documents: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£

    Returns:
        faithfulness_score: å¿ å®åº¦åˆ†æ•° (0-1)
    """
    client = OpenAI()

    # æ„å»ºè¯„ä¼°æç¤ºè¯
    context = "\n\n".join([
        f"æ–‡æ¡£{i+1}: {doc}"
        for i, doc in enumerate(context_documents)
    ])

    prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

ç­”æ¡ˆï¼š
{answer}

è¯·è¯„ä¼°ï¼š
1. ç­”æ¡ˆä¸­çš„æ‰€æœ‰å£°æ˜æ˜¯å¦éƒ½èƒ½åœ¨å‚è€ƒæ–‡æ¡£ä¸­æ‰¾åˆ°æ”¯æŒï¼Ÿ
2. ç­”æ¡ˆæ˜¯å¦æ²¡æœ‰æ·»åŠ æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼Ÿ

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: å®Œå…¨åŸºäºæ–‡æ¡£ï¼Œæ— ç¼–é€ 
- 0.7-0.9: å¤§éƒ¨åˆ†åŸºäºæ–‡æ¡£ï¼Œæœ‰å°‘é‡åˆç†æ¨æ–­
- 0.4-0.6: éƒ¨åˆ†åŸºäºæ–‡æ¡£ï¼Œæœ‰æ˜æ˜¾æ·»åŠ ä¿¡æ¯
- 0.1-0.3: å¤§é‡ç¼–é€ ä¿¡æ¯
- 0.0: å®Œå…¨ä¸åŸºäºæ–‡æ¡£

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„åˆ†æ•°ï¼Œä¿ç•™ä¸¤ä½å°æ•°ã€‚
"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„ä¼°åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )

    # æå–åˆ†æ•°
    score_text = response.choices[0].message.content.strip()
    try:
        score = float(score_text)
        return max(0, min(1, score))  # ç¡®ä¿åœ¨0-1ä¹‹é—´
    except ValueError:
        # å¦‚æœè¿”å›çš„ä¸æ˜¯çº¯æ•°å­—ï¼Œå°è¯•æå–
        import re
        numbers = re.findall(r'0\.\d+', score_text)
        if numbers:
            return float(numbers[0])
        return 0.5  # é»˜è®¤åˆ†æ•°

# ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹æ–‡æ¡£
    context = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚",
        "Pythonçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨ï¼Œé€‚åˆåˆå­¦è€…ã€‚"
    ]

    # æµ‹è¯•ä¸åŒè´¨é‡çš„ç­”æ¡ˆ
    test_cases = [
        {
            "name": "å®Œå…¨å¿ å®",
            "answer": "Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»ºï¼Œå®ƒçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨ã€‚"
        },
        {
            "name": "å¤§éƒ¨åˆ†å¿ å®",
            "answer": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guidoåˆ›å»ºï¼Œå®ƒéå¸¸é€‚åˆæ•°æ®ç§‘å­¦å’ŒWebå¼€å‘ã€‚"
        },
        {
            "name": "éƒ¨åˆ†ç¼–é€ ",
            "answer": "Pythonæ˜¯Googleå¼€å‘çš„ç¼–ç¨‹è¯­è¨€ï¼Œå‘å¸ƒäº2000å¹´ï¼Œä¸»è¦ç”¨äºäººå·¥æ™ºèƒ½é¢†åŸŸã€‚"
        },
        {
            "name": "å®Œå…¨ç¼–é€ ",
            "answer": "Javaæ˜¯ä¸€ç§è„šæœ¬è¯­è¨€ï¼Œä¸»è¦ç”¨äºå‰ç«¯å¼€å‘ï¼Œè¯­æ³•éå¸¸å¤æ‚éš¾å­¦ã€‚"
        }
    ]

    print("Faithfulnessè¯„ä¼°ç¤ºä¾‹")
    print("="*60 + "\n")

    for test in test_cases:
        score = evaluate_faithfulness(test["answer"], context)
        print(f"{test['name']:15s}: {score:.2f}")
        print(f"  ç­”æ¡ˆ: {test['answer']}")
        print()
```

### Relevancyï¼ˆç›¸å…³æ€§ï¼‰

#### å®šä¹‰

**ç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜**ã€‚

#### è¯„ä¼°æ–¹æ³•

```python
def evaluate_relevancy(question, answer):
    """
    è¯„ä¼°ç­”æ¡ˆçš„ç›¸å…³æ€§

    Args:
        question: ç”¨æˆ·é—®é¢˜
        answer: RAGç”Ÿæˆçš„ç­”æ¡ˆ

    Returns:
        relevancy_score: ç›¸å…³æ€§åˆ†æ•° (0-1)
    """
    client = OpenAI()

    prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{question}

ç­”æ¡ˆï¼š
{answer}

è¯„ä¼°æ ‡å‡†ï¼š
- 1.0: å®Œå…¨å›ç­”äº†é—®é¢˜ï¼Œä¿¡æ¯å……åˆ†å‡†ç¡®
- 0.7-0.9: å¾ˆå¥½åœ°å›ç­”äº†é—®é¢˜ï¼Œç•¥æœ‰ä¸è¶³
- 0.4-0.6: éƒ¨åˆ†å›ç­”äº†é—®é¢˜ï¼Œä¿¡æ¯ä¸å®Œæ•´
- 0.1-0.3: åŸºæœ¬æ²¡æœ‰å›ç­”é—®é¢˜
- 0.0: å®Œå…¨ä¸ç›¸å…³

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„åˆ†æ•°ï¼Œä¿ç•™ä¸¤ä½å°æ•°ã€‚
"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„ä¼°åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )

    score_text = response.choices[0].message.content.strip()
    try:
        score = float(score_text)
        return max(0, min(1, score))
    except ValueError:
        import re
        numbers = re.findall(r'0\.\d+', score_text)
        if numbers:
            return float(numbers[0])
        return 0.5
```

### ä½¿ç”¨RAGASè¯„ä¼°æ¡†æ¶

RAGASæ˜¯ä¸€ä¸ªä¸“é—¨çš„RAGè¯„ä¼°åº“ï¼Œæä¾›æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ã€‚

#### å®‰è£…RAGAS

```bash
pip install ragas
```

#### ä½¿ç”¨RAGASè¯„ä¼°

```python
# æ–‡ä»¶åï¼š04_04_ragas_eval.py
"""
ä½¿ç”¨RAGASæ¡†æ¶è¯„ä¼°
"""

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision
)
from datasets import Dataset

def evaluate_with_ragas(questions, answers, contexts, ground_truths=None):
    """
    ä½¿ç”¨RAGASè¯„ä¼°

    Args:
        questions: é—®é¢˜åˆ—è¡¨
        answers: RAGç”Ÿæˆçš„ç­”æ¡ˆåˆ—è¡¨
        contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
        ground_truths: çœŸå®ç­”æ¡ˆåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        è¯„ä¼°ç»“æœ
    """
    # å‡†å¤‡æ•°æ®
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
    }

    if ground_truths:
        data["ground_truth"] = ground_truths

    dataset = Dataset.from_dict(data)

    # é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision
    ]

    # è¿è¡Œè¯„ä¼°
    results = evaluate(
        dataset=dataset,
        metrics=metrics
    )

    return results

# ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    questions = [
        "Pythonæ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ",
        "Pythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    ]

    answers = [
        "Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚",
        "Pythonçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨ã€‚"
    ]

    contexts = [
        ["Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚"],
        ["Pythonçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨ï¼Œé€‚åˆåˆå­¦è€…ã€‚"]
    ]

    # è¯„ä¼°
    print("ä½¿ç”¨RAGASè¯„ä¼°")
    print("="*60 + "\n")

    results = evaluate_with_ragas(questions, answers, contexts)

    # æ‰“å°ç»“æœ
    print(results.to_pandas())
```

---

## 4.4 å»ºç«‹è¯„ä¼°åŸºçº¿

### é»„é‡‘æ•°æ®é›†åˆ›å»º

#### ä»€ä¹ˆæ˜¯é»„é‡‘æ•°æ®é›†ï¼Ÿ

äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡é—®ç­”å¯¹ï¼Œä½œä¸ºè¯„ä¼°çš„"æ ‡å‡†ç­”æ¡ˆ"ã€‚

#### åˆ›å»ºæ­¥éª¤

```python
# æ–‡ä»¶åï¼š04_05_golden_dataset.py
"""
åˆ›å»ºé»„é‡‘æ•°æ®é›†
"""

import json
from typing import List, Dict

class GoldenDatasetBuilder:
    """é»„é‡‘æ•°æ®é›†æ„å»ºå™¨"""

    def __init__(self, output_path="data/eval/golden_dataset.json"):
        self.output_path = output_path
        self.dataset = []

    def add_example(self, question: str, answer: str, context: str, metadata: dict = None):
        """
        æ·»åŠ ä¸€ä¸ªç¤ºä¾‹

        Args:
            question: é—®é¢˜
            answer: æ ‡å‡†ç­”æ¡ˆ
            context: ç›¸å…³ä¸Šä¸‹æ–‡
            metadata: å…ƒæ•°æ®
        """
        example = {
            "question": question,
            "answer": answer,
            "context": context,
            "metadata": metadata or {}
        }
        self.dataset.append(example)

    def save(self):
        """ä¿å­˜æ•°æ®é›†"""
        import os
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)

        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(self.dataset, f, ensure_ascii=False, indent=2)

        print(f"âœ“ ä¿å­˜äº† {len(self.dataset)} ä¸ªç¤ºä¾‹åˆ° {self.output_path}")

    def load(self):
        """åŠ è½½æ•°æ®é›†"""
        with open(self.output_path, 'r', encoding='utf-8') as f:
            self.dataset = json.load(f)
        return self.dataset

# åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
if __name__ == "__main__":
    builder = GoldenDatasetBuilder()

    # æ·»åŠ ç¤ºä¾‹ï¼ˆåŸºäºæˆ‘ä»¬çš„æµ‹è¯•æ–‡æ¡£ï¼‰
    examples = [
        {
            "question": "Pythonæ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ",
            "answer": "Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚",
            "context": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚",
            "metadata": {"topic": "Pythonå†å²", "difficulty": "easy"}
        },
        {
            "question": "Pythonæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ",
            "answer": "Pythonçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨ï¼Œé€‚åˆåˆå­¦è€…ã€‚",
            "context": "Pythonçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨ï¼Œé€‚åˆåˆå­¦è€…ã€‚",
            "metadata": {"topic": "Pythonç‰¹ç‚¹", "difficulty": "easy"}
        },
        {
            "question": "JavaScriptä¸»è¦ç”¨äºä»€ä¹ˆï¼Ÿ",
            "answer": "JavaScriptä¸»è¦ç”¨äºWebå‰ç«¯å¼€å‘ã€‚",
            "context": "JavaScriptæ˜¯ä¸€ç§è„šæœ¬è¯­è¨€ï¼Œä¸»è¦ç”¨äºWebå‰ç«¯å¼€å‘ã€‚",
            "metadata": {"topic": "JavaScript", "difficulty": "easy"}
        },
        {
            "question": "Rustå¦‚ä½•ä¿è¯å†…å­˜å®‰å…¨ï¼Ÿ",
            "answer": "Rusté€šè¿‡æ‰€æœ‰æƒç³»ç»Ÿåœ¨ç¼–è¯‘æ—¶ä¿è¯å†…å­˜å®‰å…¨ï¼Œä¸éœ€è¦åƒåœ¾å›æ”¶ã€‚",
            "context": "Rustæ˜¯ä¸€ç§ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œæ³¨é‡å†…å­˜å®‰å…¨ã€å¹¶å‘å’Œæ€§èƒ½ã€‚å®ƒæ²¡æœ‰åƒåœ¾å›æ”¶ï¼Œè€Œæ˜¯é€šè¿‡æ‰€æœ‰æƒç³»ç»Ÿåœ¨ç¼–è¯‘æ—¶ä¿è¯å†…å­˜å®‰å…¨ã€‚",
            "metadata": {"topic": "Rust", "difficulty": "medium"}
        }
    ]

    for example in examples:
        builder.add_example(**example)

    # ä¿å­˜
    builder.save()

    print("\né»„é‡‘æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print(f"åŒ…å« {len(builder.dataset)} ä¸ªé—®ç­”å¯¹")
```

### è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

```python
# æ–‡ä»¶åï¼š04_06_auto_eval.py
"""
è‡ªåŠ¨åŒ–RAGè¯„ä¼°æµç¨‹
"""

from golden_dataset import GoldenDatasetBuilder
from retrieval_eval import RetrievalEvaluator
from ragas_eval import evaluate_with_ragas
from rag_system import SimpleRAG

class AutoEvaluator:
    """è‡ªåŠ¨åŒ–è¯„ä¼°å™¨"""

    def __init__(self, rag_system, golden_dataset_path):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            rag_system: RAGç³»ç»Ÿå®ä¾‹
            golden_dataset_path: é»„é‡‘æ•°æ®é›†è·¯å¾„
        """
        self.rag = rag_system
        self.dataset = self.load_dataset(golden_dataset_path)

    def load_dataset(self, path):
        """åŠ è½½é»„é‡‘æ•°æ®é›†"""
        builder = GoldenDatasetBuilder(path)
        return builder.load()

    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("="*70)
        print("è‡ªåŠ¨åŒ–RAGè¯„ä¼°")
        print("="*70 + "\n")

        # 1. å‡†å¤‡æ•°æ®
        questions = [item["question"] for item in self.dataset]
        ground_truths = [item["answer"] for item in self.dataset]

        # 2. è¿è¡ŒRAGç³»ç»Ÿ
        print("æ­¥éª¤1: è¿è¡ŒRAGç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ\n")
        rag_answers = []
        retrieved_contexts = []

        for question in questions:
            response = self.rag.query(question)
            rag_answers.append(str(response))
            # å‡è®¾responseåŒ…å«æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            retrieved_contexts.append([response.source_nodes[0].text])

        # 3. è¯„ä¼°ç”Ÿæˆè´¨é‡
        print("æ­¥éª¤2: è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼ˆä½¿ç”¨RAGASï¼‰\n")
        generation_scores = evaluate_with_ragas(
            questions,
            rag_answers,
            retrieved_contexts,
            ground_truths
        )

        # 4. è¯„ä¼°æ£€ç´¢è´¨é‡
        print("æ­¥éª¤3: è¯„ä¼°æ£€ç´¢è´¨é‡\n")
        # è¿™é‡Œéœ€è¦é¢å¤–çš„çœŸå®ç›¸å…³æ–‡æ¡£æ ‡æ³¨
        # ç®€åŒ–ç¤ºä¾‹ï¼šå‡è®¾ç¬¬ä¸€ä¸ªæ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯ç›¸å…³çš„
        retrieval_evaluator = RetrievalEvaluator()
        retrieval_scores = retrieval_evaluator.evaluate(
            questions,
            [[0] for _ in questions],  # ç®€åŒ–
            [set() for _ in questions]  # ç®€åŒ–
        )

        # 5. ç»¼åˆæŠ¥å‘Š
        self.print_report(generation_scores, retrieval_scores)

    def print_report(self, generation_scores, retrieval_scores):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print("\n" + "="*70)
        print("è¯„ä¼°æŠ¥å‘Š")
        print("="*70 + "\n")

        print("ç”Ÿæˆè´¨é‡:")
        for metric, value in generation_scores.items():
            print(f"  {metric}: {value:.3f}")

        print("\næ£€ç´¢è´¨é‡:")
        for metric, value in retrieval_scores.items():
            print(f"  {metric}: {value:.3f}")

        print("\n" + "="*70)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = SimpleRAG()
    documents = rag.load_documents()
    rag.build_index(documents)

    # è¿è¡Œè¯„ä¼°
    evaluator = AutoEvaluator(rag, "data/eval/golden_dataset.json")
    evaluator.run_evaluation()
```

---

## æ€»ç»“

### æœ¬ç« è¦ç‚¹å›é¡¾

1. **è¯„ä¼°çš„é‡è¦æ€§**ï¼šç”¨ç§‘å­¦æ–¹æ³•è¡¡é‡RAGç³»ç»Ÿ
2. **æ£€ç´¢è´¨é‡è¯„ä¼°**ï¼šHit Rateã€MRRã€Precision@K
3. **ç”Ÿæˆè´¨é‡è¯„ä¼°**ï¼šFaithfulnessã€Relevancy
4. **RAGASæ¡†æ¶**ï¼šä½¿ç”¨æ ‡å‡†åŒ–å·¥å…·è¯„ä¼°
5. **å»ºç«‹è¯„ä¼°åŸºçº¿**ï¼šåˆ›å»ºé»„é‡‘æ•°æ®é›†å’Œè‡ªåŠ¨åŒ–æµç¨‹

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£RAGè¯„ä¼°çš„ä¸‰ä¸ªç»´åº¦
- [ ] èƒ½å¤Ÿè®¡ç®—Hit Rateå’ŒMRR
- [ ] æŒæ¡Faithfulnessè¯„ä¼°æ–¹æ³•
- [ ] ä¼šä½¿ç”¨RAGASæ¡†æ¶
- [ ] åˆ›å»ºäº†é»„é‡‘æ•°æ®é›†
- [ ] å»ºç«‹äº†è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- **ä¸‹ä¸€ç« **ï¼š[ç¬¬5ç« ï¼šæ¨¡å—1æ€»ç»“ä¸é¡¹ç›®](./05-æ¨¡å—1æ€»ç»“ä¸é¡¹ç›®.md)
  - ç»¼åˆé¡¹ç›®å®æˆ˜
  - å®Œæ•´çš„ç«¯åˆ°ç«¯å®ç°

### æ‰©å±•ç»ƒä¹ 

1. **åŸºç¡€ç»ƒä¹ **ï¼š
   - ä¸ºä½ çš„RAGç³»ç»Ÿåˆ›å»ºé»„é‡‘æ•°æ®é›†
   - è®¡ç®—Hit Rateå’ŒMRR
   - ä½¿ç”¨RAGASè¯„ä¼°

2. **è¿›é˜¶ç»ƒä¹ **ï¼š
   - å®ç°æ›´å¤šçš„è¯„ä¼°æŒ‡æ ‡
   - å¯è§†åŒ–è¯„ä¼°ç»“æœ
   - å»ºç«‹æŒç»­è¯„ä¼°æµç¨‹

3. **æŒ‘æˆ˜é¡¹ç›®**ï¼š
   - å¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥çš„è¯„ä¼°ç»“æœ
   - ä¼˜åŒ–ä½ çš„RAGç³»ç»Ÿç›´åˆ°è¾¾åˆ°ä¼˜ç§€æ ‡å‡†

---

## æœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | å®šä¹‰ |
|------|------|------|
| **Hit Rate** | å‘½ä¸­ç‡ | è‡³å°‘æ£€ç´¢åˆ°ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢å æ¯” |
| **MRR** | Mean Reciprocal Rank | å¹³å‡å€’æ•°æ’å |
| **Precision@K** | ç²¾ç¡®ç‡@K | å‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹ |
| **Faithfulness** | å¿ å®åº¦ | ç­”æ¡ˆåŸºäºæ–‡æ¡£çš„ç¨‹åº¦ |
| **Relevancy** | ç›¸å…³æ€§ | ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦ |
| **é»„é‡‘æ•°æ®é›†** | Golden Dataset | äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡é—®ç­”å¯¹ |

---

**è¿”å›ç›®å½•** | **ä¸Šä¸€ç« ï¼šåŸºç¡€RAGå®ç°** | **ä¸‹ä¸€ç« ï¼šæ¨¡å—1æ€»ç»“ä¸é¡¹ç›®**

---

**æœ¬ç« ç»“æŸ**

> è¯„ä¼°æ˜¯æ”¹è¿›çš„åŸºç¡€ã€‚æ²¡æœ‰è¯„ä¼°ï¼Œæˆ‘ä»¬å°±åƒåœ¨é»‘æš—ä¸­æ‘¸ç´¢ã€‚ç°åœ¨ä½ æœ‰äº†ç§‘å­¦çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥è‡ªä¿¡åœ°è¡¡é‡å’Œä¼˜åŒ–ä½ çš„RAGç³»ç»Ÿäº†ï¼
