# ç¬¬21ç« ï¼šæ€§èƒ½ä¼˜åŒ–

> å…¨é¢æå‡RAGç³»ç»Ÿçš„å“åº”é€Ÿåº¦å’Œååé‡

---

## ğŸ“š ç« èŠ‚æ¦‚è¿°

æœ¬ç« å°†å­¦ä¹ å¦‚ä½•å¯¹RAGç³»ç»Ÿè¿›è¡Œå…¨é¢æ€§èƒ½ä¼˜åŒ–ï¼Œæå‡å“åº”é€Ÿåº¦å’Œé™ä½æˆæœ¬ã€‚

### å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- âœ… å®æ–½å¤šå±‚ç¼“å­˜ç­–ç•¥
- âœ… ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
- âœ… ä¼˜åŒ–å‘é‡æ£€ç´¢æ€§èƒ½
- âœ… å®æ–½æ‰¹å¤„ç†å’Œå¹¶å‘
- âœ… é™ä½APIè°ƒç”¨æˆæœ¬

### é¢„è®¡æ—¶é—´

- ç†è®ºå­¦ä¹ ï¼š60åˆ†é’Ÿ
- å®è·µæ“ä½œï¼š90-120åˆ†é’Ÿ
- æ€»è®¡ï¼šçº¦3-4å°æ—¶

---

## 1. æ€§èƒ½åˆ†æ

### 1.1 æ€§èƒ½æŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**ï¼š
```
å»¶è¿Ÿï¼ˆLatencyï¼‰ï¼š
  - P50: 50%è¯·æ±‚çš„å“åº”æ—¶é—´
  - P95: 95%è¯·æ±‚çš„å“åº”æ—¶é—´
  - P99: 99%è¯·æ±‚çš„å“åº”æ—¶é—´

ååé‡ï¼ˆThroughputï¼‰ï¼š
  - QPS: æ¯ç§’æŸ¥è¯¢æ•°
  - RPS: æ¯ç§’è¯·æ±‚æ•°

èµ„æºä½¿ç”¨ï¼š
  - CPUä½¿ç”¨ç‡
  - å†…å­˜ä½¿ç”¨ç‡
  - ç½‘ç»œ I/O
  - ç£ç›˜ I/O
```

### 1.2 æ€§èƒ½åˆ†æå·¥å…·

**Python Profiling**ï¼š
```python
import cProfile
import pstats
from io import StringIO

def profile_query(func):
    """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        result = func(*args, **kwargs)
        pr.disable()

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        s = StringIO()
        ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
        ps.print_stats(20)  # æ‰“å°å‰20ä¸ª
        print(s.getvalue())
        return result
    return wrapper

# ä½¿ç”¨
@profile_query
async def rag_query(text: str):
    # RAGé€»è¾‘
    pass
```

**å†…å­˜åˆ†æ**ï¼š
```python
import tracemalloc

def trace_memory(func):
    """å†…å­˜åˆ†æè£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        tracemalloc.start()
        result = func(*args, **kwargs)

        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')
        print("[Top 10 memory usage]")
        for stat in top_stats[:10]:
            print(stat)

        tracemalloc.stop()
        return result
    return wrapper
```

### 1.3 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

**å¸¸è§ç“¶é¢ˆ**ï¼š

1. **LLM APIè°ƒç”¨**ï¼š
   - ç½‘ç»œå»¶è¿Ÿï¼ˆ100-500msï¼‰
   - æ¨¡å‹æ¨ç†æ—¶é—´ï¼ˆ1-5sï¼‰
   - Tokené™åˆ¶å¤„ç†

2. **å‘é‡æ£€ç´¢**ï¼š
   - å¤§è§„æ¨¡å‘é‡æœç´¢
   - å‘é‡ç»´åº¦è¿‡é«˜
   - ç´¢å¼•æ•ˆç‡ä½

3. **æ•°æ®åº“æŸ¥è¯¢**ï¼š
   - N+1æŸ¥è¯¢é—®é¢˜
   - ç¼ºå°‘ç´¢å¼•
   - è¿æ¥æ± ä¸è¶³

4. **I/Oæ“ä½œ**ï¼š
   - åŒæ­¥I/Oé˜»å¡
   - å¤§æ–‡ä»¶è¯»å–
   - ç½‘ç»œä¼ è¾“

---

## 2. ç¼“å­˜ç­–ç•¥

### 2.1 å¤šå±‚ç¼“å­˜æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Client (Browser)            â”‚  L1: æµè§ˆå™¨ç¼“å­˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CDN / Load Balancer         â”‚  L2: CDNç¼“å­˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Application (Redis)         â”‚  L3: åº”ç”¨ç¼“å­˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Database / Vector Store     â”‚  L4: æ•°æ®åº“
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Redisç¼“å­˜å®ç°

```python
import redis
import pickle
from typing import Optional, Any
from functools import wraps
import hashlib

# Rediså®¢æˆ·ç«¯
redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=False
)

def cache_result(ttl: int = 3600, key_prefix: str = ""):
    """
    ç¼“å­˜è£…é¥°å™¨

    Args:
        ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        key_prefix: ç¼“å­˜é”®å‰ç¼€
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            key_data = f"{key_prefix}:{func.__name__}:{args}:{kwargs}"
            cache_key = hashlib.md5(key_data.encode()).hexdigest()

            # å°è¯•ä»ç¼“å­˜è·å–
            cached = redis_client.get(cache_key)
            if cached:
                return pickle.loads(cached)

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)

            # å­˜å…¥ç¼“å­˜
            redis_client.setex(
                cache_key,
                ttl,
                pickle.dumps(result)
            )

            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cache_result(ttl=1800, key_prefix="rag_query")
async def query_with_cache(text: str):
    # RAGæŸ¥è¯¢é€»è¾‘
    return await rag_query(text)
```

### 2.3 æŸ¥è¯¢ç»“æœç¼“å­˜

```python
class QueryCache:
    """æŸ¥è¯¢ç»“æœç¼“å­˜"""

    def __init__(self, redis_client, max_size: int = 1000):
        self.redis = redis_client
        self.max_size = max_size
        self.cache = {}

    async def get(self, query: str) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        cache_key = f"query:{hashlib.md5(query.encode()).hexdigest()}"

        # L1: å†…å­˜ç¼“å­˜
        if cache_key in self.cache:
            return self.cache[cache_key]

        # L2: Redisç¼“å­˜
        cached = self.redis.get(cache_key)
        if cached:
            result = pickle.loads(cached)
            self.cache[cache_key] = result
            return result

        return None

    async def set(self, query: str, result: dict, ttl: int = 1800):
        """è®¾ç½®ç¼“å­˜"""
        cache_key = f"query:{hashlib.md5(query.encode()).hexdigest()}"

        # å†…å­˜ç¼“å­˜ï¼ˆLRUï¼‰
        if len(self.cache) >= self.max_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        self.cache[cache_key] = result

        # Redisç¼“å­˜
        self.redis.setex(cache_key, ttl, pickle.dumps(result))

    async def invalidate(self, query: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        cache_key = f"query:{hashlib.md5(query.encode()).hexdigest()}"
        if cache_key in self.cache:
            del self.cache[cache_key]
        self.redis.delete(cache_key)
```

### 2.4 å‘é‡ç¼“å­˜

```python
class EmbeddingCache:
    """å‘é‡åµŒå…¥ç¼“å­˜"""

    def __init__(self, redis_client):
        self.redis = redis_client

    async def get_embeddings(self, texts: list[str]) -> dict[str, np.ndarray]:
        """æ‰¹é‡è·å–åµŒå…¥å‘é‡"""
        cache_keys = [f"emb:{hashlib.md5(t.encode()).hexdigest()}" for t in texts]

        # æ‰¹é‡è·å–
        cached_values = self.redis.mget(cache_keys)

        results = {}
        missing_indices = []

        for i, (text, cached) in enumerate(zip(texts, cached_values)):
            if cached:
                results[text] = pickle.loads(cached)
            else:
                missing_indices.append(i)

        return results, missing_indices

    async def set_embeddings(self, texts: list[str], embeddings: list[np.ndarray]):
        """æ‰¹é‡è®¾ç½®åµŒå…¥å‘é‡"""
        pipe = self.redis.pipeline()
        for text, emb in zip(texts, embeddings):
            cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
            pipe.setex(cache_key, 86400, pickle.dumps(emb))  # 24å°æ—¶
        pipe.execute()
```

---

## 3. æ•°æ®åº“ä¼˜åŒ–

### 3.1 ç´¢å¼•ä¼˜åŒ–

```sql
-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_documents_created_at ON documents(created_at);
CREATE INDEX idx_documents_metadata ON documents USING GIN(metadata);
CREATE INDEX idx_queries_user_id ON queries(user_id);
CREATE INDEX idx_queries_created_at ON queries(created_at DESC);

-- å¤åˆç´¢å¼•
CREATE INDEX idx_documents_user_created ON documents(user_id, created_at DESC);

-- éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æ´»è·ƒæ•°æ®ï¼‰
CREATE INDEX idx_active_documents ON documents(id)
WHERE status = 'active';
```

### 3.2 æŸ¥è¯¢ä¼˜åŒ–

```python
# ä¼˜åŒ–å‰ï¼ˆN+1é—®é¢˜ï¼‰
async def get_documents_with_tags_optimized(document_ids: list[int]):
    """æ‰¹é‡è·å–æ–‡æ¡£å’Œæ ‡ç­¾"""
    # ä½¿ç”¨INæŸ¥è¯¢ä»£æ›¿å¤šæ¬¡æŸ¥è¯¢
    documents = await db.execute(
        select(Document)
        .where(Document.id.in_(document_ids))
        .options(selectinload(Document.tags))  # é¢„åŠ è½½å…³è”
    )

    return {doc.id: doc for doc in documents}

# ä½¿ç”¨æ‰¹é‡æ“ä½œ
async def bulk_insert_embeddings(documents: list[dict]):
    """æ‰¹é‡æ’å…¥åµŒå…¥å‘é‡"""
    embeddings = [Embedding(**doc) for doc in documents]

    # æ‰¹é‡æ’å…¥ï¼ˆæ¯”é€ä¸ªæ’å…¥å¿«10-100å€ï¼‰
    async with database.session() as session:
        session.add_all(embeddings)
        await session.commit()
```

### 3.3 è¿æ¥æ± é…ç½®

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.pool import QueuePool

# åˆ›å»ºè¿æ¥æ± 
engine = create_async_engine(
    "postgresql+asyncpg://user:pass@localhost/ragdb",
    poolclass=QueuePool,
    pool_size=20,          # è¿æ¥æ± å¤§å°
    max_overflow=40,       # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
    pool_timeout=30,       # è·å–è¿æ¥è¶…æ—¶
    pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´
    pool_pre_ping=True,    # è¿æ¥å‰pingæ£€æŸ¥
    echo=False
)
```

---

## 4. å‘é‡æ£€ç´¢ä¼˜åŒ–

### 4.1 ç´¢å¼•é€‰æ‹©

```python
import chromadb
from chromadb.config import Settings

# é…ç½®ChromaDB
client = chromadb.HttpClient(
    host="localhost",
    port=8000,
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# åˆ›å»ºå¸¦ç´¢å¼•çš„é›†åˆ
collection = client.create_collection(
    name="documents",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 200,  # æ„å»ºæ—¶çš„å‚æ•°
        "hnsw:M": 16                  # å›¾çš„è¿æ¥æ•°
    }
)
```

### 4.2 æ‰¹é‡æ£€ç´¢

```python
async def batch_search(
    queries: list[str],
    collection,
    n_results: int = 10
) -> list[list[dict]]:
    """æ‰¹é‡æ£€ç´¢"""
    # æ‰¹é‡ç”ŸæˆåµŒå…¥
    embeddings = await embed_texts(queries)

    # æ‰¹é‡æ£€ç´¢
    results = collection.query(
        query_embeddings=embeddings,
        n_results=n_results
    )

    return results
```

### 4.3 é¢„è¿‡æ»¤

```python
async def filtered_search(
    query: str,
    filters: dict,
    collection
) -> list[dict]:
    """å¸¦é¢„è¿‡æ»¤çš„æ£€ç´¢"""

    # å…ˆç”¨whereå­å¥è¿‡æ»¤
    results = collection.query(
        query_embeddings=[await embed_text(query)],
        where=filters,  # {"category": "tech", "date": {"$gt": "2024-01-01"}}
        n_results=10
    )

    return results
```

---

## 5. å¹¶å‘å’Œå¼‚æ­¥

### 5.1 å¼‚æ­¥I/O

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def parallel_retrieval(query: str):
    """å¹¶è¡Œæ£€ç´¢å¤šä¸ªæ•°æ®æº"""

    tasks = [
        vector_search(query),
        keyword_search(query),
        hybrid_search(query)
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    return {
        "vector": results[0],
        "keyword": results[1],
        "hybrid": results[2]
    }
```

### 5.2 æ‰¹å¤„ç†

```python
class BatchProcessor:
    """æ‰¹å¤„ç†å¤„ç†å™¨"""

    def __init__(self, batch_size: int = 10, timeout: float = 1.0):
        self.batch_size = batch_size
        self.timeout = timeout
        self.queue = asyncio.Queue()
        self.task = None

    async def start(self):
        """å¯åŠ¨æ‰¹å¤„ç†ä»»åŠ¡"""
        self.task = asyncio.create_task(self._process_batch())

    async def _process_batch(self):
        """å¤„ç†æ‰¹æ¬¡"""
        while True:
            batch = []
            deadline = asyncio.time() + self.timeout

            # æ”¶é›†æ‰¹æ¬¡
            while len(batch) < self.batch_size:
                try:
                    item = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=deadline - asyncio.time()
                    )
                    batch.append(item)
                except asyncio.TimeoutError:
                    break

            if batch:
                await self._process(batch)

    async def _process(self, batch: list):
        """å¤„ç†æ‰¹æ¬¡æ•°æ®"""
        # æ‰¹é‡å¤„ç†é€»è¾‘
        results = await batch_embed([item['text'] for item in batch])

        for item, result in zip(batch, results):
            if 'future' in item:
                item['future'].set_result(result)

    async def submit(self, text: str) -> Any:
        """æäº¤å¾…å¤„ç†é¡¹ç›®"""
        future = asyncio.Future()
        await self.queue.put({'text': text, 'future': future})
        return await future
```

---

## 6. LLMè°ƒç”¨ä¼˜åŒ–

### 6.1 Tokenä¼˜åŒ–

```python
async def optimize_context(
    query: str,
    documents: list[dict],
    max_tokens: int = 3000
) -> str:
    """ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦"""

    # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„tokenæ•°
    for doc in documents:
        doc['token_count'] = count_tokens(doc['text'])

    # æŒ‰ç›¸å…³æ€§æ’åº
    documents.sort(key=lambda x: x['score'], reverse=True)

    # è´ªå¿ƒé€‰æ‹©æ–‡æ¡£ï¼ˆå°½å¯èƒ½å¤šçš„é«˜ç›¸å…³æ–‡æ¡£ï¼‰
    selected_docs = []
    total_tokens = 0

    for doc in documents:
        if total_tokens + doc['token_count'] <= max_tokens:
            selected_docs.append(doc)
            total_tokens += doc['token_count']
        else:
            break

    return format_context(selected_docs)
```

### 6.2 æ‰¹é‡ç”Ÿæˆ

```python
async def batch_generate(
    queries: list[str],
    batch_size: int = 5
) -> list[str]:
    """æ‰¹é‡ç”Ÿæˆå›ç­”"""

    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]

        # å¹¶å‘è°ƒç”¨LLM
        tasks = [generate_answer(q) for q in batch]
        batch_results = await asyncio.gather(*tasks)

        results.extend(batch_results)

        # é¿å…é€Ÿç‡é™åˆ¶
        if i + batch_size < len(queries):
            await asyncio.sleep(1)

    return results
```

---

## 7. æ€§èƒ½æµ‹è¯•

### 7.1 è´Ÿè½½æµ‹è¯•

```python
# ä½¿ç”¨Locustè¿›è¡Œè´Ÿè½½æµ‹è¯•
from locust import HttpUser, task, between

class RAGUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def query(self):
        query_text = "What is RAG?"
        self.client.post(
            "/query",
            json={"text": query_text}
        )

    @task(3)
    def health_check(self):
        self.client.get("/health")
```

**è¿è¡Œè´Ÿè½½æµ‹è¯•**ï¼š
```bash
locust -f locustfile.py --host=http://localhost:8000
```

### 7.2 åŸºå‡†æµ‹è¯•

```python
import time
from statistics import mean, median

def benchmark(func, iterations: int = 100):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    times = []
    results = []

    for _ in range(iterations):
        start = time.perf_counter()
        result = func()
        end = time.perf_counter()

        times.append(end - start)
        results.append(result)

    return {
        "mean": mean(times),
        "median": median(times),
        "min": min(times),
        "max": max(times),
        "p95": sorted(times)[int(iterations * 0.95)],
        "p99": sorted(times)[int(iterations * 0.99)],
        "throughput": iterations / sum(times)
    }

# ä½¿ç”¨
result = benchmark(lambda: query("test query"))
print(f"P95å»¶è¿Ÿ: {result['p95']*1000:.2f}ms")
print(f"ååé‡: {result['throughput']:.2f} QPS")
```

---

## 8. å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå®æ–½ç¼“å­˜ç­–ç•¥

**ä»»åŠ¡**ï¼š
1. å®ç°Redisç¼“å­˜
2. ç¼“å­˜æŸ¥è¯¢ç»“æœå’ŒåµŒå…¥å‘é‡
3. æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡
4. ä¼˜åŒ–ç¼“å­˜å¤±æ•ˆç­–ç•¥

**éªŒè¯**ï¼š
```python
# æµ‹è¯•ç¼“å­˜æ•ˆæœ
# æ— ç¼“å­˜ï¼š1000ms
# æœ‰ç¼“å­˜ï¼š10ms
# æå‡ï¼š100å€
```

---

### ç»ƒä¹ 2ï¼šä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢

**ä»»åŠ¡**ï¼š
1. æ·»åŠ é€‚å½“ç´¢å¼•
2. ä¼˜åŒ–æ…¢æŸ¥è¯¢
3. å®ç°æ‰¹é‡æ“ä½œ
4. é…ç½®è¿æ¥æ± 

**éªŒè¯**ï¼š
```sql
-- æŸ¥çœ‹æ…¢æŸ¥è¯¢
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

---

### ç»ƒä¹ 3ï¼šå¹¶å‘ä¼˜åŒ–

**ä»»åŠ¡**ï¼š
1. å®ç°å¼‚æ­¥å¤„ç†
2. é…ç½®æ‰¹å¤„ç†
3. æµ‹è¯•å¹¶å‘æ€§èƒ½
4. æ‰¾åˆ°æœ€ä¼˜å¹¶å‘æ•°

**éªŒè¯**ï¼š
```bash
# è´Ÿè½½æµ‹è¯•
# å•çº¿ç¨‹ï¼š10 QPS
# å¹¶å‘20ï¼š150 QPS
# æå‡ï¼š15å€
```

---

## 9. æœ€ä½³å®è·µ

### 9.1 ä¼˜åŒ–åŸåˆ™

- **Measure First**: å…ˆæµ‹é‡å†ä¼˜åŒ–
- **Optimize Hot Path**: ä¼˜åŒ–çƒ­ç‚¹è·¯å¾„
- **Avoid Premature Optimization**: é¿å…è¿‡æ—©ä¼˜åŒ–
- **Trade-offs**: åœ¨é€Ÿåº¦ã€æˆæœ¬ã€è´¨é‡é—´å¹³è¡¡

### 9.2 ç›‘æ§æŒ‡æ ‡

- **ç¼“å­˜å‘½ä¸­ç‡**: >80%
- **P95å»¶è¿Ÿ**: <2s
- **é”™è¯¯ç‡**: <1%
- **èµ„æºä½¿ç”¨**: CPU <70%, å†…å­˜ <80%

---

## 10. æ€»ç»“

### å…³é”®è¦ç‚¹

1. **æ€§èƒ½åˆ†æ**
   - è¯†åˆ«ç“¶é¢ˆ
   - ä½¿ç”¨profilingå·¥å…·
   - ç›‘æ§å…³é”®æŒ‡æ ‡

2. **ç¼“å­˜ç­–ç•¥**
   - å¤šå±‚ç¼“å­˜
   - LRUå¤±æ•ˆ
   - æ‰¹é‡æ“ä½œ

3. **ä¼˜åŒ–æŠ€æœ¯**
   - å¼‚æ­¥å¤„ç†
   - æ‰¹å¤„ç†
   - ç´¢å¼•ä¼˜åŒ–

### ä¸‹ä¸€æ­¥

- å­¦ä¹ å®‰å…¨å®è·µï¼ˆç¬¬22ç« ï¼‰
- æœ€ä½³å®è·µï¼ˆç¬¬23ç« ï¼‰

---

**æ­å–œå®Œæˆç¬¬21ç« ï¼** ğŸ‰

ä½ å·²ç»æŒæ¡RAGç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€èƒ½ï¼

**ä¸‹ä¸€æ­¥**ï¼šç¬¬22ç«  - å®‰å…¨å®è·µ
