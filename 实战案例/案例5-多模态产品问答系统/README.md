# æ¡ˆä¾‹5ï¼šå¤šæ¨¡æ€äº§å“é—®ç­”ç³»ç»Ÿ

> æ„å»ºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒ+æ–‡æœ¬çš„ç”µå•†äº§å“é—®ç­”ç³»ç»Ÿ

---

## ğŸ“‹ æ¡ˆä¾‹æ¦‚è¿°

### ä¸šåŠ¡åœºæ™¯

ç”µå•†å¹³å°äº§å“å’¨è¯¢æŒ‘æˆ˜ï¼š
- âœ— ç”¨æˆ·åªèƒ½ç”¨æ–‡å­—æè¿°äº§å“
- âœ— éš¾ä»¥æ‰¾åˆ°ç›¸ä¼¼äº§å“å›¾ç‰‡
- âœ— è§†è§‰ç‰¹æ€§æ— æ³•æ–‡å­—è¡¨è¾¾
- âœ— äº§å“å¯¹æ¯”è€—æ—¶

### å¤šæ¨¡æ€RAGè§£å†³æ–¹æ¡ˆ

æ„å»ºå›¾æ–‡ç»“åˆçš„äº§å“é—®ç­”ç³»ç»Ÿï¼š
- âœ… å›¾åƒä¸Šä¼ è¯†åˆ«
- âœ… å›¾æ–‡æ··åˆæ£€ç´¢
- âœ… è§†è§‰é—®ç­”
- âœ… æ™ºèƒ½æ¨è

---

## ğŸ¯ åŠŸèƒ½éœ€æ±‚

### æ ¸å¿ƒåŠŸèƒ½

1. **å›¾åƒç†è§£**
   - ä¸Šä¼ äº§å“å›¾ç‰‡
   - è‡ªåŠ¨è¯†åˆ«äº§å“ç±»å‹
   - æå–è§†è§‰ç‰¹å¾
   - ç”Ÿæˆæè¿°

2. **å›¾æ–‡æ£€ç´¢**
   - ä»¥å›¾æœå›¾
   - ä»¥æ–‡æœå›¾
   - å›¾æ–‡æ··åˆæœç´¢
   - ç›¸ä¼¼äº§å“æ¨è

3. **è§†è§‰é—®ç­”**
   - å›ç­”äº§å“ç›¸å…³é—®é¢˜
   - è§£é‡Šäº§å“ç‰¹æ€§
   - å¯¹æ¯”å¤šä¸ªäº§å“
   - æä¾›è´­ä¹°å»ºè®®

4. **æ™ºèƒ½æ¨è**
   - åŸºäºè§†è§‰ç›¸ä¼¼æ¨è
   - åŸºäºæ–‡æœ¬æè¿°æ¨è
   - ä¸ªæ€§åŒ–æ¨è
   - è·¨ç±»åˆ«æ¨è

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Streamlit Webç•Œé¢                â”‚
â”‚  - å›¾åƒä¸Šä¼                               â”‚
â”‚  - é—®ç­”äº¤äº’                               â”‚
â”‚  - ç»“æœå±•ç¤º                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FastAPIåç«¯                      â”‚
â”‚  - å¤šæ¨¡æ€API                              â”‚
â”‚  - ç»“æœèåˆ                               â”‚
â”‚  - ä¸šåŠ¡é€»è¾‘                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚CLIPæ¨¡å‹â”‚      â”‚  GPT-4V    â”‚
    â”‚(åµŒå…¥)  â”‚      â”‚  (ç†è§£)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ å‘é‡æ•°æ®åº“    â”‚
        â”‚ (ChromaDB)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆ

**å¤šæ¨¡æ€æ¨¡å‹**ï¼š
- CLIP (å›¾åƒ-æ–‡æœ¬åµŒå…¥)
- GPT-4V (è§†è§‰ç†è§£)
- BLIP (å›¾åƒæè¿°ç”Ÿæˆ)

**å‘é‡æ•°æ®åº“**ï¼š
- ChromaDB
- Pinecone (å¯é€‰)

**åç«¯**ï¼š
- FastAPI
- OpenAI API

**å‰ç«¯**ï¼š
- Streamlit
- Pillow (å›¾åƒå¤„ç†)

---

## ğŸ’» æ ¸å¿ƒå®ç°

### 1. å¤šæ¨¡æ€åµŒå…¥

```python
# multimodal_embedding.py
from sentence_transformers import SentenceTransformer, util
from PIL import Image
import torch
from typing import List, Union, Tuple

class MultiModalEmbedding:
    """
    å¤šæ¨¡æ€åµŒå…¥æœåŠ¡

    ä½¿ç”¨CLIPå®ç°å›¾åƒ-æ–‡æœ¬è”åˆåµŒå…¥
    """

    def __init__(self, model_name: str = "clip-ViT-B-32"):
        """
        åˆå§‹åŒ–CLIPæ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°
        """
        print(f"åŠ è½½CLIPæ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)

    def embed_image(self, image_path: str) -> torch.Tensor:
        """
        åµŒå…¥å›¾åƒ

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            å›¾åƒåµŒå…¥å‘é‡
        """
        image = Image.open(image_path)
        embedding = self.model.encode([image])
        return embedding[0]

    def embed_text(self, text: str) -> torch.Tensor:
        """
        åµŒå…¥æ–‡æœ¬

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            æ–‡æœ¬åµŒå…¥å‘é‡
        """
        embedding = self.model.encode([text])
        return embedding[0]

    def embed_batch_images(self,
                          image_paths: List[str]) -> torch.Tensor:
        """
        æ‰¹é‡åµŒå…¥å›¾åƒ

        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨

        Returns:
            å›¾åƒåµŒå…¥çŸ©é˜µ
        """
        images = [Image.open(path) for path in image_paths]
        embeddings = self.model.encode(images)
        return embeddings

    def compute_similarity(self,
                         image_emb: torch.Tensor,
                         text_emb: torch.Tensor) -> float:
        """
        è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦

        Args:
            image_emb: å›¾åƒåµŒå…¥
            text_emb: æ–‡æœ¬åµŒå…¥

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        similarity = util.cos_sim(
            image_emb.reshape(1, -1),
            text_emb.reshape(1, -1)
        )[0][0]

        return float(similarity)

    def find_similar_images(self,
                           query_image: str,
                           image_paths: List[str],
                           top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼å›¾åƒ

        Args:
            query_image: æŸ¥è¯¢å›¾åƒ
            image_paths: å€™é€‰å›¾åƒåˆ—è¡¨
            top_k: è¿”å›å‰Kä¸ªç»“æœ

        Returns:
            [(å›¾åƒè·¯å¾„, ç›¸ä¼¼åº¦)]
        """
        # åµŒå…¥æŸ¥è¯¢å›¾åƒ
        query_emb = self.embed_image(query_image)

        # æ‰¹é‡åµŒå…¥å€™é€‰å›¾åƒ
        candidate_embs = self.embed_batch_images(image_paths)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = util.cos_sim(
            query_emb.reshape(1, -1),
            candidate_embs
        )[0]

        # Top-K
        top_k_indices = similarities.argsort(descending=True)[:top_k]

        results = [
            (image_paths[i], float(similarities[i]))
            for i in top_k_indices
        ]

        return results
```

### 2. äº§å“ç´¢å¼•

```python
# product_indexer.py
from typing import List, Dict
import chromadb
from chromadb.config import Settings
from multimodal_embedding import MultiModalEmbedding

class ProductIndexer:
    """
    äº§å“ç´¢å¼•å™¨

    ç´¢å¼•äº§å“çš„å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯
    """

    def __init__(self,
                 persist_directory: str = "./data/chroma"):
        """
        åˆå§‹åŒ–

        Args:
            persist_directory: ChromaDBæŒä¹…åŒ–ç›®å½•
        """
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # åˆ›å»ºæˆ–è·å–collection
        self.collection = self.client.get_or_create_collection(
            name="products",
            metadata={"hnsw:space": "cosine"}
        )

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedder = MultiModalEmbedding()

    def index_products(self, products: List[Dict]):
        """
        ç´¢å¼•äº§å“

        Args:
            products: äº§å“åˆ—è¡¨
            [{
                "id": "P001",
                "name": "æ— çº¿è€³æœº",
                "description": "è“ç‰™5.0ï¼Œé™å™ªåŠŸèƒ½",
                "image_path": "products/P001.jpg",
                "category": "ç”µå­äº§å“",
                "price": 299
            }]
        """
        for product in products:
            try:
                # åµŒå…¥å›¾åƒ
                image_emb = self.embedder.embed_image(product["image_path"])

                # åµŒå…¥æ–‡æœ¬ï¼ˆåç§°+æè¿°ï¼‰
                text = f"{product['name']} {product['description']}"
                text_emb = self.embedder.embed_text(text)

                # åˆå¹¶åµŒå…¥ï¼ˆç®€å•å¹³å‡ï¼‰
                combined_emb = (image_emb + text_emb) / 2

                # æ·»åŠ åˆ°å‘é‡åº“
                self.collection.add(
                    embeddings=[combined_emb.tolist()],
                    documents=[text],
                    metadatas=[{
                        "id": product["id"],
                        "name": product["name"],
                        "category": product["category"],
                        "price": product["price"],
                        "image_path": product["image_path"]
                    }],
                    ids=[product["id"]]
                )

                print(f"âœ“ ç´¢å¼•äº§å“: {product['name']}")

            except Exception as e:
                print(f"âœ— ç´¢å¼•å¤±è´¥ {product['id']}: {e}")

    def search_by_image(self,
                       query_image: str,
                       top_k: int = 5) -> List[Dict]:
        """
        ä»¥å›¾æœå›¾

        Args:
            query_image: æŸ¥è¯¢å›¾åƒ
            top_k: è¿”å›ç»“æœæ•°

        Returns:
            ç›¸ä¼¼äº§å“åˆ—è¡¨
        """
        # åµŒå…¥æŸ¥è¯¢å›¾åƒ
        query_emb = self.embedder.embed_image(query_image)

        # æ£€ç´¢
        results = self.collection.query(
            query_embeddings=[query_emb.tolist()],
            n_results=top_k
        )

        # æ ¼å¼åŒ–ç»“æœ
        products = []
        for i, (doc, metadata) in enumerate(
            zip(results["documents"][0],
            results["metadatas"][0])
        ):
            products.append({
                "rank": i + 1,
                "id": metadata["id"],
                "name": metadata["name"],
                "category": metadata["category"],
                "price": metadata["price"],
                "image_path": metadata["image_path"],
                "description": doc
            })

        return products

    def search_by_text(self,
                       query_text: str,
                       top_k: int = 5) -> List[Dict]:
        """
        æ–‡æœ¬æœç´¢äº§å“

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°

        Returns:
            ç›¸å…³äº§å“åˆ—è¡¨
        """
        # åµŒå…¥æŸ¥è¯¢æ–‡æœ¬
        query_emb = self.embedder.embed_text(query_text)

        # æ£€ç´¢
        results = self.collection.query(
            query_embeddings=[query_emb.tolist()],
            n_results=top_k
        )

        # æ ¼å¼åŒ–ç»“æœ
        products = []
        for i, (doc, metadata) in enumerate(
            zip(results["documents"][0],
            results["metadatas"][0])
        ):
            products.append({
                "rank": i + 1,
                "id": metadata["id"],
                "name": metadata["name"],
                "category": metadata["category"],
                "price": metadata["price"],
                "image_path": metadata["image_path"],
                "description": doc
            })

        return products

    def multimodal_search(self,
                         query_text: str = None,
                         query_image: str = None,
                         top_k: int = 5) -> List[Dict]:
        """
        å¤šæ¨¡æ€æœç´¢

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
            query_image: æŸ¥è¯¢å›¾åƒï¼ˆå¯é€‰ï¼‰
            top_k: è¿”å›ç»“æœæ•°

        Returns:
            èåˆç»“æœ
        """
        if query_image and query_text:
            # å›¾æ–‡èåˆ
            image_emb = self.embedder.embed_image(query_image)
            text_emb = self.embedder.embed_text(query_text)

            # åŠ æƒèåˆ
            combined_emb = (image_emb + text_emb) / 2

            results = self.collection.query(
                query_embeddings=[combined_emb.tolist()],
                n_results=top_k
            )

        elif query_image:
            # çº¯å›¾åƒ
            return self.search_by_image(query_image, top_k)

        elif query_text:
            # çº¯æ–‡æœ¬
            return self.search_by_text(query_text, top_k)

        else:
            return []

        # æ ¼å¼åŒ–ç»“æœ
        products = []
        for i, (doc, metadata) in enumerate(
            zip(results["documents"][0],
            results["metadatas"][0])
        ):
            products.append({
                "rank": i + 1,
                "id": metadata["id"],
                "name": metadata["name"],
                "category": metadata["category"],
                "price": metadata["price"],
                "image_path": metadata["image_path"],
                "description": doc
            })

        return products
```

### 3. è§†è§‰é—®ç­”

```python
# visual_qa.py
from openai import OpenAI
from PIL import Image
import base64
from typing import Dict

class VisualQuestionAnswering:
    """
    è§†è§‰é—®ç­”æœåŠ¡

    ä½¿ç”¨GPT-4Vç†è§£å›¾åƒå¹¶å›ç­”é—®é¢˜
    """

    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–

        Args:
            api_key: OpenAI APIå¯†é’¥
        """
        self.client = OpenAI(api_key=api_key)

    def encode_image(self, image_path: str) -> str:
        """
        ç¼–ç å›¾åƒä¸ºbase64

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            base64ç¼–ç çš„å›¾åƒ
        """
        with open(image_path, "rb") as image_file:
            return base64.b64encode(
                image_file.read()
            ).decode('utf-8')

    def answer_question(self,
                       image_path: str,
                       question: str) -> Dict:
        """
        å›ç­”å…³äºå›¾åƒçš„é—®é¢˜

        Args:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜æ–‡æœ¬

        Returns:
            ç­”æ¡ˆ
        """
        # ç¼–ç å›¾åƒ
        base64_image = self.encode_image(image_path)

        # è°ƒç”¨GPT-4V
        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": question
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=500
        )

        answer = response.choices[0].message.content

        return {
            "question": question,
            "answer": answer,
            "image_path": image_path
        }

    def compare_products(self,
                        image1: str,
                        image2: str) -> Dict:
        """
        å¯¹æ¯”ä¸¤ä¸ªäº§å“

        Args:
            image1: äº§å“1å›¾åƒ
            image2: äº§å“2å›¾åƒ

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        base64_1 = self.encode_image(image1)
        base64_2 = self.encode_image(image2)

        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "å¯¹æ¯”è¿™ä¸¤ä¸ªäº§å“ï¼Œåˆ†æå®ƒä»¬çš„å¼‚åŒç‚¹ã€ä¼˜ç¼ºç‚¹ã€‚"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_1}"
                            }
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_2}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=800
        )

        comparison = response.choices[0].message.content

        return {
            "image1": image1,
            "image2": image2,
            "comparison": comparison
        }

    def generate_description(self, image_path: str) -> str:
        """
        ç”Ÿæˆäº§å“æè¿°

        Args:
            image_path: äº§å“å›¾åƒ

        Returns:
            äº§å“æè¿°
        """
        base64_image = self.encode_image(image_path)

        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """è¯·è¯¦ç»†æè¿°è¿™ä¸ªäº§å“ï¼ŒåŒ…æ‹¬ï¼š
1. äº§å“ç±»å‹
2. ä¸»è¦ç‰¹å¾
3. è®¾è®¡é£æ ¼
4. ç›®æ ‡ç”¨æˆ·
5. å–ç‚¹"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=500
        )

        return response.choices[0].message.content
```

### 4. Streamlitç•Œé¢

```python
# app.py
import streamlit as st
from PIL import Image
import os

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¤šæ¨¡æ€äº§å“é—®ç­”",
    page_icon="ğŸ–¼ï¸",
    layout="wide"
)

st.title("ğŸ–¼ï¸ğŸ“ å¤šæ¨¡æ€äº§å“é—®ç­”ç³»ç»Ÿ")
st.markdown("ä¸Šä¼ äº§å“å›¾ç‰‡ï¼Œæ™ºèƒ½é—®ç­”å’Œæ¨è")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("åŠŸèƒ½é€‰æ‹©")

    mode = st.radio(
        "é€‰æ‹©æ¨¡å¼",
        ["ğŸ“¸ å›¾åƒæ£€ç´¢", "ğŸ’¬ è§†è§‰é—®ç­”", "ğŸ”„ äº§å“å¯¹æ¯”", "âœ¨ æ™ºèƒ½æ¨è"]
    )

    st.divider()

    st.subheader("è®¾ç½®")
    top_k = st.slider("æ¨èæ•°é‡", 1, 10, 5)
    api_key = st.text_input("OpenAI API Key", type="password")

# ä¸»ç•Œé¢
if mode == "ğŸ“¸ å›¾åƒæ£€ç´¢":
    st.header("å›¾åƒæ£€ç´¢")

    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ä¸Šä¼ æŸ¥è¯¢å›¾åƒ")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©äº§å“å›¾ç‰‡",
            type=['jpg', 'jpeg', 'png']
        )

        if uploaded_file:
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = f"temp_{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            image = Image.open(temp_path)
            st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)

            query_text = st.text_input(
                "å¯é€‰ï¼šæ·»åŠ æ–‡å­—æè¿°"
            )

            if st.button("ğŸ” æœç´¢ç›¸ä¼¼äº§å“", type="primary"):
                # æ‰§è¡Œæ£€ç´¢ï¼ˆéœ€è¦åç«¯ï¼‰
                with st.spinner("æ­£åœ¨æœç´¢..."):
                    # è¿™é‡Œè°ƒç”¨åç«¯API
                    # results = indexer.multimodal_search(
                    #     query_image=temp_path,
                    #     query_text=query_text or None,
                    #     top_k=top_k
                    # )

                    # æ¨¡æ‹Ÿç»“æœ
                    st.session_state.results = [
                        {
                            "name": f"äº§å“{i}",
                            "price": f"{100 + i*50}å…ƒ",
                            "similarity": 0.95 - i*0.05
                        }
                        for i in range(top_k)
                    ]

    with col2:
        if 'results' in st.session_state:
            st.subheader(f"æ‰¾åˆ° {len(st.session_state.results)} ä¸ªç›¸ä¼¼äº§å“")

            for i, product in enumerate(st.session_state.results, 1):
                with st.expander(
                    f"{i}. {product['name']} - {product['price']}",
                    expanded=(i == 1)
                ):
                    st.write(f"**ç›¸ä¼¼åº¦**: {product['similarity']:.2%}")
                    # æ˜¾ç¤ºäº§å“å›¾ç‰‡
                    # st.image(product['image_path'])

elif mode == "ğŸ’¬ è§†è§‰é—®ç­”":
    st.header("è§†è§‰é—®ç­”")

    uploaded_file = st.file_uploader(
        "ä¸Šä¼ äº§å“å›¾ç‰‡",
        type=['jpg', 'jpeg', 'png']
    )

    if uploaded_file:
        col1, col2 = st.columns([1, 1])

        with col1:
            image = Image.open(uploaded_file)
            st.image(image, caption="äº§å“å›¾ç‰‡", use_column_width=True)

        with col2:
            question = st.text_area(
                "è¾“å…¥é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªäº§å“æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
                height=100
            )

            if st.button("â“ æé—®", type="primary"):
                if not api_key:
                    st.error("è¯·è¾“å…¥OpenAI API Key")
                elif not question:
                    st.warning("è¯·è¾“å…¥é—®é¢˜")
                else:
                    with st.spinner("æ­£åœ¨åˆ†æ..."):
                        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                        temp_path = f"temp_{uploaded_file.name}"
                        with open(temp_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())

                        # è°ƒç”¨è§†è§‰é—®ç­”
                        # vqa = VisualQuestionAnswering(api_key)
                        # result = vqa.answer_question(temp_path, question)
                        # st.write(result['answer'])

                        # æ¨¡æ‹Ÿ
                        st.write("""
                        è¿™ä¸ªäº§å“æ˜¯ä¸€æ¬¾**æ— çº¿è€³æœº**ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

                        **ä¸»è¦ç‰¹å¾**ï¼š
                        - è“ç‰™5.0è¿æ¥ï¼Œä½å»¶è¿Ÿ
                        - ä¸»åŠ¨é™å™ªåŠŸèƒ½
                        - 30å°æ—¶ç»­èˆª
                        - IPX4é˜²æ°´

                        **è®¾è®¡é£æ ¼**ï¼šç®€çº¦ç°ä»£ï¼Œäººä½“å·¥å­¦

                        **ç›®æ ‡ç”¨æˆ·**ï¼šéŸ³ä¹çˆ±å¥½è€…ã€å•†åŠ¡äººå£«

                        **å–ç‚¹**ï¼šé«˜æ€§ä»·æ¯”ã€é•¿ç»­èˆªã€èˆ’é€‚ä½©æˆ´
                        """)

elif mode == "ğŸ”„ äº§å“å¯¹æ¯”":
    st.header("äº§å“å¯¹æ¯”")

    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("äº§å“1")
        file1 = st.file_uploader("ä¸Šä¼ äº§å“1", type=['jpg', 'png'])

    with col2:
        st.subheader("äº§å“2")
        file2 = st.file_uploader("ä¸Šä¼ äº§å“2", type=['jpg', 'png'])

    if file1 and file2:
        if st.button("ğŸ”„ å¯¹æ¯”äº§å“", type="primary"):
            with st.spinner("æ­£åœ¨åˆ†æ..."):
                # è°ƒç”¨å¯¹æ¯”API
                st.session_state.comparison = "å¯¹æ¯”ç»“æœ..."

    if 'comparison' in st.session_state:
        st.subheader("å¯¹æ¯”ç»“æœ")
        st.write(st.session_state.comparison)

elif mode == "âœ¨ æ™ºèƒ½æ¨è":
    st.header("æ™ºèƒ½æ¨è")

    st.markdown("""
    åŸºäºæ‚¨çš„ä¸Šä¼ å›¾ç‰‡ï¼Œæ¨èä»¥ä¸‹äº§å“ï¼š

    1. **ç›¸ä¼¼äº§å“æ¨è**
       - åŸºäºè§†è§‰ç‰¹å¾ç›¸ä¼¼åº¦
       - ç›¸åŒç±»åˆ«äº§å“
       - åŒå“ç‰Œäº§å“

    2. **äº’è¡¥äº§å“æ¨è**
       - é…å¥—äº§å“
       - ç›¸å…³é…ä»¶
       - ä½¿ç”¨åœºæ™¯å»ºè®®

    3. **å‡çº§äº§å“æ¨è**
       - æ›´é«˜ç«¯å‹å·
       - æ–°ç‰ˆæœ¬
       - æ›¿ä»£äº§å“
    """)
```

---

## ğŸ“Š æ•°æ®å‡†å¤‡

### äº§å“æ•°æ®æ ¼å¼

```json
[
  {
    "id": "P001",
    "name": "Sony WH-1000XM4 æ— çº¿è€³æœº",
    "category": "ç”µå­äº§å“",
    "price": 2299,
    "description": "è¡Œä¸šé¢†å…ˆçš„é™å™ªè€³æœºï¼Œ30å°æ—¶ç»­èˆª",
    "image_path": "products/headphones_sony.jpg",
    "brand": "Sony",
    "features": ["é™å™ª", "æ— çº¿", "é•¿ç»­èˆª"],
    "tags": ["è€³æœº", "éŸ³é¢‘", "è“ç‰™"]
  },
  {
    "id": "P002",
    "name": "AirPods Pro 2",
    "category": "ç”µå­äº§å“",
    "price": 1899,
    "description": "Appleä¸»åŠ¨é™å™ªè€³æœºï¼Œç©ºé—´éŸ³é¢‘",
    "image_path": "products/headphones_apple.jpg",
    "brand": "Apple",
    "features": ["é™å™ª", "ç©ºé—´éŸ³é¢‘", "é€šé€æ¨¡å¼"],
    "tags": ["è€³æœº", "éŸ³é¢‘", "iOS"]
  }
]
```

---

## ğŸ§ª æµ‹è¯•åœºæ™¯

### åœºæ™¯1ï¼šä»¥å›¾æœå›¾

**è¾“å…¥**ï¼šä¸Šä¼ ä¸€å¼ è€³æœºå›¾ç‰‡

**è¾“å‡º**ï¼š
1. Sony WH-1000XM4 (ç›¸ä¼¼åº¦: 95%)
2. Bose QuietComfort 45 (ç›¸ä¼¼åº¦: 89%)
3. AirPods Pro 2 (ç›¸ä¼¼åº¦: 85%)

### åœºæ™¯2ï¼šè§†è§‰é—®ç­”

**è¾“å…¥**ï¼š
- å›¾ç‰‡ï¼šäº§å“å›¾
- é—®é¢˜ï¼š"è¿™ä¸ªäº§å“çš„ç»­èˆªæ—¶é—´æ˜¯å¤šå°‘ï¼Ÿ"

**è¾“å‡º**ï¼š
"æ ¹æ®äº§å“ä¿¡æ¯ï¼Œè¿™æ¬¾è€³æœºæä¾›**30å°æ—¶**çš„æ€»ç»­èˆªæ—¶é—´ï¼ˆå¼€å¯é™å™ªæ¨¡å¼ä¸‹ä¸º20å°æ—¶ï¼‰ï¼Œæ”¯æŒå¿«å……ï¼Œå……ç”µ10åˆ†é’Ÿå¯ä½¿ç”¨5å°æ—¶ã€‚"

### åœºæ™¯3ï¼šå›¾æ–‡æ··åˆæ£€ç´¢

**è¾“å…¥**ï¼š
- å›¾ç‰‡ï¼šäº§å“å›¾
- æ–‡æœ¬ï¼š"ä»·æ ¼åœ¨2000å…ƒä»¥ä¸‹"

**è¾“å‡º**ï¼šç¬¦åˆè§†è§‰ç‰¹å¾å’Œä»·æ ¼é™åˆ¶çš„äº§å“

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡åµŒå…¥

```python
def batch_index_products(products: List[Dict],
                         batch_size: int = 32):
    """æ‰¹é‡ç´¢å¼•äº§å“"""
    for i in range(0, len(products), batch_size):
        batch = products[i:i+batch_size]
        # æ‰¹é‡åµŒå…¥
        embeddings = embedder.encode(
            [p['image_path'] for p in batch],
            batch_size=batch_size,
            show_progress_bar=False
        )
        # æ‰¹é‡æ·»åŠ 
        collection.add(embeddings=embeddings, ...)
```

### 2. å›¾åƒé¢„å¤„ç†

```python
def preprocess_image(image_path: str,
                     size: int = 224) -> Image.Image:
    """é¢„å¤„ç†å›¾åƒ"""
    img = Image.open(image_path)

    # è°ƒæ•´å¤§å°
    img = img.resize((size, size))

    # å½’ä¸€åŒ–
    # img = normalize(img)

    return img
```

### 3. ç¼“å­˜

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_image_embedding(image_path: str):
    """ç¼“å­˜å›¾åƒåµŒå…¥"""
    return embedder.embed_image(image_path)
```

---

## ğŸ¨ UIä¼˜åŒ–

### 1. å›¾åƒé¢„è§ˆ

```python
def display_product_grid(products: List[Dict],
                         columns: int = 4):
    """å±•ç¤ºäº§å“ç½‘æ ¼"""
    for i in range(0, len(products), columns):
        cols = st.columns(columns)
        for j, col in enumerate(cols):
            idx = i + j
            if idx < len(products):
                with col:
                    st.image(products[idx]['image_path'])
                    st.write(products[idx]['name'])
                    st.caption(f"Â¥{products[idx]['price']}")
```

### 2. ç›¸ä¼¼åº¦å¯è§†åŒ–

```python
import plotly.graph_objects as go

def plot_similarity_bar(products: List[Dict]):
    """ç»˜åˆ¶ç›¸ä¼¼åº¦æ¡å½¢å›¾"""
    fig = go.Figure(
        data=[
            go.Bar(
                x=[p['similarity'] for p in products],
                y=[p['name'] for p in products],
                orientation='h',
                marker_color='skyblue'
            )
        ]
    )

    fig.update_layout(
        title="äº§å“ç›¸ä¼¼åº¦",
        xaxis_title="ç›¸ä¼¼åº¦",
        yaxis_title="äº§å“",
        height=400
    )

    st.plotly_chart(fig)
```

---

## ğŸ“ å­¦ä¹ è¦ç‚¹

å®Œæˆæœ¬æ¡ˆä¾‹åï¼Œä½ å°†æŒæ¡ï¼š

### âœ… å¤šæ¨¡æ€æŠ€æœ¯
- CLIPæ¨¡å‹ä½¿ç”¨
- å›¾åƒ-æ–‡æœ¬åµŒå…¥
- è·¨æ¨¡æ€æ£€ç´¢
- èåˆç­–ç•¥

### âœ… è§†è§‰é—®ç­”
- GPT-4Vé›†æˆ
- å›¾åƒç†è§£
- è§†è§‰æ¨ç†
- ç­”æ¡ˆç”Ÿæˆ

### âœ… ç”µå•†åº”ç”¨
- äº§å“ç´¢å¼•
- ç›¸ä¼¼æ¨è
- è§†è§‰æœç´¢
- æ™ºèƒ½å¯¹æ¯”

---

## ğŸš€ è¿›é˜¶æ–¹å‘

1. **é«˜çº§åŠŸèƒ½**
   - è§†é¢‘ç†è§£
   - 3Däº§å“å±•ç¤º
   - ARè¯•ç”¨
   - ä¸ªæ€§åŒ–æ¨è

2. **æ€§èƒ½æå‡**
   - åˆ†å¸ƒå¼æ£€ç´¢
   - å›¾åƒå‹ç¼©
   - å¢é‡ç´¢å¼•
   - å®æ—¶æ›´æ–°

3. **å•†ä¸šä»·å€¼**
   - ç”¨æˆ·è¡Œä¸ºåˆ†æ
   - A/Bæµ‹è¯•
   - è½¬åŒ–ç‡ä¼˜åŒ–
   - ç²¾å‡†è¥é”€

---

## ğŸ“š å‚è€ƒèµ„æº

- [OpenAI Vision API](https://platform.openai.com/docs/guides/vision)
- [CLIPè®ºæ–‡](https://openai.com/research/clip/)
- [Sentence-Transformers](https://www.sbert.net/examples/applications/multilingual-semantic.html)

---

**å¼€å§‹æ„å»ºä½ çš„å¤šæ¨¡æ€äº§å“é—®ç­”ç³»ç»Ÿå§ï¼** ğŸš€
