# ç¬¬16ç« ï¼šå¤šæ¨¡æ€RAG

> çªç ´æ–‡æœ¬é™åˆ¶ï¼šèåˆå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæ„å»ºçœŸæ­£çš„å…¨æ„ŸçŸ¥RAGç³»ç»Ÿï¼

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹
- [ ] æŒæ¡CLIPç­‰è·¨æ¨¡æ€æ¨¡å‹
- [ ] å®ç°å›¾æ–‡æ£€ç´¢RAG
- [ ] æ„å»ºå¤šæ¨¡æ€é—®ç­”ç³»ç»Ÿ
- [ ] äº†è§£å¤šæ¨¡æ€Agentå®ç°

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š6å°æ—¶
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â­â­

---

## å‰ç½®çŸ¥è¯†

- [ ] å®Œæˆæ¨¡å—1å’Œæ¨¡å—2
- [ ] äº†è§£å¤šæ¨¡æ€å­¦ä¹ åŸºç¡€
- [ ] ç†Ÿæ‚‰OpenAI CLIPæˆ–ç±»ä¼¼æ¨¡å‹
- [ ] ç†è§£å‘é‡æ•°æ®åº“

---

## 16.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€RAGï¼Ÿ

### 16.1.1 å•æ¨¡æ€çš„å±€é™

```
ä¼ ç»ŸRAGï¼ˆä»…æ–‡æœ¬ï¼‰ï¼š

ç”¨æˆ·æŸ¥è¯¢ï¼š"è¿™å¼ å›¾ç‰‡ä¸­çš„å»ºç­‘æ˜¯ä»€ä¹ˆé£æ ¼ï¼Ÿ"
é—®é¢˜ï¼šæ— æ³•å¤„ç†å›¾ç‰‡

ç³»ç»Ÿï¼š
  âœ— æ— æ³•ç†è§£å›¾åƒå†…å®¹
  âœ— åªèƒ½æ£€ç´¢æ–‡æœ¬æè¿°
  âœ— ç­”æ¡ˆè´¨é‡å—é™

å¤šæ¨¡æ€RAGï¼š

ç”¨æˆ·æŸ¥è¯¢ï¼š[å›¾ç‰‡] + "è¿™æ˜¯ä»€ä¹ˆé£æ ¼ï¼Ÿ"

ç³»ç»Ÿï¼š
  âœ“ ç†è§£å›¾åƒå†…å®¹
  âœ“ æ£€ç´¢ç›¸å…³å›¾æ–‡ä¿¡æ¯
  âœ“ ç”Ÿæˆå¤šæ¨¡æ€ç­”æ¡ˆ
```

### 16.1.2 å¤šæ¨¡æ€åº”ç”¨åœºæ™¯

```
åœºæ™¯1ï¼šäº§å“æœç´¢
  è¾“å…¥ï¼š[äº§å“å›¾ç‰‡]
  æ£€ç´¢ï¼šç›¸ä¼¼äº§å“ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰
  è¾“å‡ºï¼šäº§å“æ¨è

åœºæ™¯2ï¼šåŒ»ç–—è¯Šæ–­
  è¾“å…¥ï¼š[Xå…‰ç‰‡] + "æœ‰ä»€ä¹ˆå¼‚å¸¸ï¼Ÿ"
  æ£€ç´¢ï¼šç›¸ä¼¼ç—…ä¾‹ + åŒ»å­¦çŸ¥è¯†
  è¾“å‡ºï¼šè¯Šæ–­å»ºè®®

åœºæ™¯3ï¼šæ•™è‚²é—®ç­”
  è¾“å…¥ï¼š[é¢˜ç›®æˆªå›¾] + "å¦‚ä½•è§£ç­”ï¼Ÿ"
  æ£€ç´¢ï¼šç±»ä¼¼é¢˜ç›® + è§£é¢˜æ–¹æ³•
  è¾“å‡ºï¼šè¯¦ç»†è§£ç­”

åœºæ™¯4ï¼šå†…å®¹å®¡æ ¸
  è¾“å…¥ï¼š[è§†é¢‘] + "æ˜¯å¦è¿è§„ï¼Ÿ"
  æ£€ç´¢ï¼šè¿è§„æ¡ˆä¾‹ + å®¡æ ¸è§„åˆ™
  è¾“å‡ºï¼šå®¡æ ¸æŠ¥å‘Š
```

---

## 16.2 å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹

### 16.2.1 CLIPæ¨¡å‹

**CLIP (Contrastive Language-Image Pre-training)**

```
åŸç†ï¼š
  å›¾åƒç¼–ç å™¨ + æ–‡æœ¬ç¼–ç å™¨
  é€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹é½

æ¶æ„ï¼š
  Image Encoder (Vision Transformer)
    â†“
  Image Embedding (512ç»´)

  Text Encoder (Transformer)
    â†“
  Text Embedding (512ç»´)

  å¯¹æ¯”å­¦ä¹ ï¼šå¯¹é½å›¾åƒå’Œæ–‡æœ¬åµŒå…¥
```

### 16.2.2 CLIPä½¿ç”¨

```python
# æ–‡ä»¶åï¼šmultimodal_embedding.py
"""
å¤šæ¨¡æ€åµŒå…¥ï¼šCLIPä½¿ç”¨
"""

import torch
from PIL import Image
from sentence_transformers import SentenceTransformer, util


# åŠ è½½CLIPæ¨¡å‹
model = SentenceTransformer('clip-ViT-B-32')
print("CLIPæ¨¡å‹å·²åŠ è½½")

# å›¾åƒåµŒå…¥
def embed_image(image_path: str) -> torch.Tensor:
    """åµŒå…¥å›¾åƒ"""
    img = Image.open(image_path)
    img_emb = model.encode([img])
    return img_emb

# æ–‡æœ¬åµŒå…¥
def embed_text(text: str) -> torch.Tensor:
    """åµŒå…¥æ–‡æœ¬"""
    text_emb = model.encode([text])
    return text_emb

# è®¡ç®—ç›¸ä¼¼åº¦
def compute_similarity(image_emb: torch.Tensor,
                        text_emb: torch.Tensor) -> float:
    """è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = util.cos_sim(image_emb, text_emb)[0][0]
    return float(similarity)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åµŒå…¥å›¾åƒå’Œæ–‡æœ¬
    image_emb = embed_image("./images/building.jpg")
    text_emb = embed_text("A modern skyscraper with glass facade")

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = compute_similarity(image_emb, text_emb)
    print(f"å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦: {similarity:.4f}")

    # æ‰¹é‡æ–‡æœ¬æ£€ç´¢
    texts = [
        "A historical building",
        "A modern office building",
        "A residential house"
    ]
    text_embeddings = model.encode(texts)

    similarities = util.cos_sim(image_emb, text_embeddings)[0]
    best_idx = similarities.argmax()

    print(f"\næœ€ä½³åŒ¹é…: {texts[best_idx]} (ç›¸ä¼¼åº¦: {similarities[best_idx]:.4f})")
```

---

## 16.3 å›¾æ–‡æ£€ç´¢RAG

### 16.3.1 ç³»ç»Ÿæ¶æ„

```
å›¾æ–‡æ£€ç´¢RAGæ¶æ„

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          å¤šæ¨¡æ€æŸ¥è¯¢                      â”‚
â”‚  [å›¾åƒ] + [æ–‡æœ¬é—®é¢˜]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚ å¤šæ¨¡æ€åµŒå…¥  â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚å›¾åƒæ£€ç´¢â”‚      â”‚æ–‡æœ¬æ£€ç´¢    â”‚
    â”‚(CLIP)  â”‚      â”‚(BM25/å‘é‡)â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ç»“æœèåˆ    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  LLMç”Ÿæˆç­”æ¡ˆ    â”‚
        â”‚  (å›¾æ–‡ç»“åˆ)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 16.3.2 å®Œæ•´å®ç°

```python
# æ–‡ä»¶åï¼šmultimodal_rag.py
"""
å¤šæ¨¡æ€RAGç³»ç»Ÿ
"""

from typing import List, Dict, Union, Any
import torch
from PIL import Image
from sentence_transformers import SentenceTransformer
import numpy as np


class MultiModalRAG:
    """
    å¤šæ¨¡æ€RAGç³»ç»Ÿ

    æ”¯æŒå›¾åƒ+æ–‡æœ¬æŸ¥è¯¢
    """

    def __init__(self,
                 image_db: List[str],
                 text_db: List[str],
                 llm_client):
        """
        åˆå§‹åŒ–

        Args:
            image_db: å›¾åƒæ•°æ®åº“è·¯å¾„åˆ—è¡¨
            text_db: æ–‡æœ¬æ•°æ®åº“
            llm_client: LLMå®¢æˆ·ç«¯
        """
        # åŠ è½½CLIPæ¨¡å‹
        self.clip_model = SentenceTransformer('clip-ViT-B-32')

        # é¢„åµŒå…¥å›¾åƒ
        print("é¢„åµŒå…¥å›¾åƒ...")
        self.image_paths = image_db
        self.image_embeddings = self.clip_model.encode(
            [Image.open(p) for p in image_db]
        )

        # é¢„åµŒå…¥æ–‡æœ¬
        print("é¢„åµŒå…¥æ–‡æœ¬...")
        self.text_db = text_db
        self.text_embeddings = self.clip_model.encode(text_db)

        self.llm = llm_client

    def retrieve_by_image(self, image_path: str,
                          top_k: int = 5) -> List[Dict]:
        """
        å›¾åƒæ£€ç´¢ï¼šç”¨å›¾åƒæŸ¥è¯¢

        Args:
            image_path: æŸ¥è¯¢å›¾åƒè·¯å¾„
            top_k: è¿”å›ç»“æœæ•°
        """
        # åµŒå…¥æŸ¥è¯¢å›¾åƒ
        query_img = Image.open(image_path)
        query_emb = self.clip_model.encode([query_img])

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = util.cos_sim(query_emb, self.image_embeddings)[0]

        # Top-K
        top_k_indices = similarities.argsort()[::-1][:top_k]

        results = [
            {
                'image_path': self.image_paths[idx],
                'similarity': float(similarities[idx]),
                'type': 'image'
            }
            for idx in top_k_indices
        ]

        return results

    def retrieve_by_text(self, query: str,
                         top_k: int = 5) -> List[Dict]:
        """
        æ–‡æœ¬æ£€ç´¢ï¼šç”¨æ–‡æœ¬æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°
        """
        # åµŒå…¥æŸ¥è¯¢æ–‡æœ¬
        query_emb = self.clip_model.encode([query])

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = util.cos_sim(query_emb, self.text_embeddings)[0]

        # Top-K
        top_k_indices = similarities.argsort()[::-1][:top_k]

        results = [
            {
                'text': self.text_db[idx],
                'similarity': float(similarities[idx]),
                'type': 'text'
            }
            for idx in top_k_indices
        ]

        return results

    def retrieve_multimodal(self,
                            query_text: str,
                            query_image: str = None,
                            top_k: int = 10) -> List[Dict]:
        """
        å¤šæ¨¡æ€æ£€ç´¢ï¼šå›¾åƒ+æ–‡æœ¬æŸ¥è¯¢

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_image: æŸ¥è¯¢å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            top_k: è¿”å›ç»“æœæ•°
        """
        results = []

        # æ–‡æœ¬æ£€ç´¢
        text_results = self.retrieve_by_text(query_text, top_k=top_k // 2)
        results.extend(text_results)

        # å›¾åƒæ£€ç´¢ï¼ˆå¦‚æœæä¾›ï¼‰
        if query_image:
            image_results = self.retrieve_by_image(query_image, top_k=top_k // 2)
            results.extend(image_results)

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)

        return results[:top_k]

    def generate_answer(self, query_text: str,
                       query_image: str = None,
                       context: List[Dict] = None) -> Dict:
        """
        ç”Ÿæˆå¤šæ¨¡æ€ç­”æ¡ˆ

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_image: æŸ¥è¯¢å›¾åƒï¼ˆå¯é€‰ï¼‰
            context: æ£€ç´¢ä¸Šä¸‹æ–‡
        """
        # æ„å»ºæç¤º
        if query_image:
            # åŒ…å«å›¾åƒçš„æŸ¥è¯¢
            prompt = f"""åŸºäºä»¥ä¸‹å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ã€‚

é—®é¢˜ï¼š{query_text}

ç›¸å…³ä¿¡æ¯ï¼š
{self._format_context(context)}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆã€‚
"""
        else:
            # çº¯æ–‡æœ¬æŸ¥è¯¢
            prompt = f"""é—®é¢˜ï¼š{query_text}

ç›¸å…³ä¿¡æ¯ï¼š
{self._format_context(context)}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆã€‚
"""

        # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        answer = self.llm.generate(prompt)

        return {
            'answer': answer,
            'query_type': 'multimodal' if query_image else 'text',
            'context_used': context
        }

    def _format_context(self, context: List[Dict]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        if not context:
            return "æ— ç›¸å…³ä¿¡æ¯"

        formatted = []
        for i, item in enumerate(context[:5], 1):
            if item['type'] == 'image':
                formatted.append(f"{i}. [å›¾åƒ] {item['image_path']} (ç›¸ä¼¼åº¦: {item['similarity']:.2f})")
            else:
                formatted.append(f"{i}. [æ–‡æœ¬] {item['text'][:100]}... (ç›¸ä¼¼åº¦: {item['similarity']:.2f})")

        return "\n".join(formatted)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡†å¤‡æ•°æ®
    image_db = [
        "./data/images/building1.jpg",
        "./data/images/building2.jpg",
        "./data/images/building3.jpg"
    ]

    text_db = [
        "ç°ä»£å»ºç­‘é£æ ¼åŒ…æ‹¬ç»ç’ƒå¹•å¢™ã€æµçº¿å‹è®¾è®¡ç­‰",
        "å“¥ç‰¹å¼å»ºç­‘ç‰¹ç‚¹æ˜¯å°–é¡¶ã€é£æ‰¶å£ã€å½©è‰²ç»ç’ƒ",
        "ä¸­å›½å¤ä»£å»ºç­‘ä»¥æœ¨ç»“æ„ä¸ºä¸»ï¼Œå¦‚æ–—æ‹±ã€æ¦«å¯"
    ]

    # åˆ›å»ºå¤šæ¨¡æ€RAGç³»ç»Ÿ
    system = MultiModalRAG(image_db, text_db, llm_client=None)

    # å›¾æ–‡æŸ¥è¯¢
    results = system.retrieve_multimodal(
        query_text="ç°ä»£å»ºç­‘é£æ ¼ç‰¹ç‚¹",
        query_image="./data/images/query.jpg",
        top_k=5
    )

    print("å¤šæ¨¡æ€æ£€ç´¢ç»“æœ:")
    for r in results:
        if r['type'] == 'image':
            print(f"å›¾åƒ: {r['image_path']} (ç›¸ä¼¼åº¦: {r['similarity']:.2f})")
        else:
            print(f"æ–‡æœ¬: {r['text'][:50]}... (ç›¸ä¼¼åº¦: {r['similarity']:.2f})")
```

---

## 16.4 å¤šæ¨¡æ€Agent

### 16.4.1 å¤šæ¨¡æ€å·¥å…·å®šä¹‰

```python
# æ–‡ä»¶åï¼šmultimodal_agent.py
"""
å¤šæ¨¡æ€Agent
"""

from langchain.agents import initialize_agent, Tool, AgentExecutor
from langchain.tools import StructuredTool


class MultiModalAgent:
    """
    å¤šæ¨¡æ€Agent

    æ”¯æŒå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€
    """

    def __init__(self, openai_api_key: str):
        from langchain_openai import ChatOpenAI
        from sentence_transformers import SentenceTransformer

        # LLM
        self.llm = ChatOpenAI(model="gpt-4-vision-preview", api_key=openai_api_key)

        # CLIPæ¨¡å‹
        self.clip_model = SentenceTransformer('clip-ViT-B-32')

        # å®šä¹‰å·¥å…·
        self.tools = self._create_tools()

        # åˆ›å»ºAgent
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent="zero-shot-react-description",
            verbose=True
        )

        self.executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            max_iterations=10
        )

    def _create_tools(self) -> List[Tool]:
        """åˆ›å»ºå¤šæ¨¡æ€å·¥å…·"""
        tools = [
            Tool(
                name="ImageSearch",
                func=self._image_search,
                description="æœç´¢ç›¸ä¼¼çš„å›¾åƒã€‚è¾“å…¥ï¼šå›¾åƒè·¯å¾„æˆ–æè¿°ã€‚"
            ),
            Tool(
                name="TextSearch",
                func=self._text_search,
                description="æœç´¢ç›¸å…³æ–‡æ¡£ã€‚è¾“å…¥ï¼šæœç´¢å…³é”®è¯ã€‚"
            ),
            Tool(
                name="ImageAnalysis",
                func=self._image_analysis,
                description="åˆ†æå›¾åƒå†…å®¹ã€‚è¾“å…¥ï¼šå›¾åƒè·¯å¾„ã€‚"
            ),
            Tool(
                name="MultimodalQA",
                func=self._multimodal_qa,
                description="åŸºäºå›¾åƒå’Œæ–‡æœ¬å›ç­”é—®é¢˜ã€‚è¾“å…¥ï¼šå›¾åƒè·¯å¾„å’Œé—®é¢˜ã€‚"
            )
        ]
        return tools

    def _image_search(self, query: str) -> str:
        """å›¾åƒæœç´¢"""
        # å®é™…å®ç°æœç´¢å›¾åƒæ•°æ®åº“
        return f"æ‰¾åˆ°ä¸'{query}'ç›¸ä¼¼çš„5å¼ å›¾åƒ..."

    def _text_search(self, query: str) -> str:
        """æ–‡æœ¬æœç´¢"""
        # å®é™…å®ç°æœç´¢æ–‡æ¡£æ•°æ®åº“
        return f"æ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„10ä¸ªæ–‡æ¡£..."

    def _image_analysis(self, image_path: str) -> str:
        """å›¾åƒåˆ†æ"""
        # ä½¿ç”¨GPT-4Våˆ†æå›¾åƒ
        from PIL import Image

        image = Image.open(image_path)

        # è°ƒç”¨GPT-4V
        response = self.llm.invoke([
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"},
                    {"type": "image_url", "image_url": image_path}
                ]
            }
        ])

        return response.content[0].text

    def _multimodal_qa(self, inputs: str) -> str:
        """å¤šæ¨¡æ€é—®ç­”"""
        # è§£æè¾“å…¥ï¼ˆæ ¼å¼ï¼š"image_path: é—®é¢˜"ï¼‰
        parts = inputs.split(":")
        image_path = parts[0].strip()
        question = parts[1].strip() if len(parts) > 1 else "è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆï¼Ÿ"

        # å›¾åƒ+æ–‡æœ¬å¤šæ¨¡æ€ç†è§£
        from PIL import Image

        image = Image.open(image_path)

        response = self.llm.invoke([
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{question}"},
                    {"type": "image_url", "image_url": image_path}
                ]
            }
        ])

        return response.content[0].text

    def query(self, user_input: Union[str, tuple]) -> Dict:
        """
        å¤šæ¨¡æ€æŸ¥è¯¢

        Args:
            user_input: æ–‡æœ¬æŸ¥è¯¢ æˆ– (image_path, query)

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        # è½¬æ¢è¾“å…¥æ ¼å¼
        if isinstance(user_input, tuple):
            # (å›¾åƒè·¯å¾„, é—®é¢˜)
            query_str = f"{user_input[0]}: {user_input[1]}"
        else:
            query_str = user_input

        # æ‰§è¡ŒAgent
        result = self.executor.invoke({"input": query_str})

        return {
            'answer': result['output'],
            'intermediate_steps': result.get('intermediate_steps', [])
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import os

    agent = MultiModalAgent(openai_api_key=os.getenv("OPENAI_API_KEY"))

    # å›¾åƒæŸ¥è¯¢
    result1 = agent.query(("./images/photo.jpg", "è¿™å¼ ç…§ç‰‡æ˜¯ä»€ä¹ˆé£æ ¼ï¼Ÿ"))
    print(f"ç­”æ¡ˆ: {result1['answer']}")

    # çº¯æ–‡æœ¬æŸ¥è¯¢
    result2 = agent.query("æ¯”è¾ƒPythonå’ŒJavaScriptçš„æ€§èƒ½")
    print(f"ç­”æ¡ˆ: {result2['answer']}")
```

---

## 16.5 å®Œæ•´é¡¹ç›®ï¼šå›¾æ–‡é—®ç­”ç³»ç»Ÿ

### 16.5.1 é¡¹ç›®è®¾è®¡

**é¡¹ç›®åç§°**ï¼šVisualQA System

**åŠŸèƒ½éœ€æ±‚**ï¼š
1. å›¾åƒä¸Šä¼ å’Œè¯†åˆ«
2. å›¾åƒæ£€ç´¢
3. å›¾æ–‡ç»“åˆçš„é—®ç­”
4. å¤šæ¨¡æ€ç­”æ¡ˆç”Ÿæˆ
5. Webç•Œé¢ï¼ˆStreamlitï¼‰

### 16.5.2 å®Œæ•´å®ç°

```python
# æ–‡ä»¶åï¼švisual_qa_app.py
"""
å›¾æ–‡é—®ç­”ç³»ç»ŸWebåº”ç”¨
"""

import streamlit as st
from PIL import Image
from multimodal_rag import MultiModalRAG
import os


def main():
    st.set_page_config(page_title="å¤šæ¨¡æ€RAGé—®ç­”ç³»ç»Ÿ", layout="wide")

    st.title("ğŸ–¼ï¸ğŸ“ å¤šæ¨¡æ€RAGé—®ç­”ç³»ç»Ÿ")
    st.markdown("æ”¯æŒå›¾åƒ+æ–‡æœ¬æŸ¥è¯¢ï¼Œæä¾›æ™ºèƒ½é—®ç­”æœåŠ¡")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("è®¾ç½®")
        api_key = st.text_input("OpenAI API Key", type="password")

        if api_key:
            st.session_state.api_key = api_key

    # ä¸»ç•Œé¢
    tab1, tab2, tab3 = st.tabs(["å›¾æ–‡é—®ç­”", "å›¾åƒæ£€ç´¢", "ç³»ç»Ÿè¯´æ˜"])

    with tab1:
        st.header("å›¾æ–‡é—®ç­”")

        # ä¸Šä¼ å›¾åƒ
        uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=['jpg', 'png', 'jpeg'])

        if uploaded_file:
            # æ˜¾ç¤ºä¸Šä¼ çš„å›¾ç‰‡
            image = Image.open(uploaded_file)
            st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)

            # è¾“å…¥é—®é¢˜
            question = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")

            col1, col2 = st.columns([1, 1])

            with col1:
                if st.button("ğŸš€ æäº¤", use_container_width=True):
                    if question and api_key:
                        with st.spinner("æ­£åœ¨åˆ†æ..."):
                            # è°ƒç”¨å¤šæ¨¡æ€RAG
                            system = MultiModalRag(api_key)
                            result = system.query((uploaded_file, question))

                            st.subheader("ç­”æ¡ˆ")
                            st.write(result['answer'])
                    else:
                        st.warning("è¯·è¾“å…¥é—®é¢˜")
```

---

## ç»ƒä¹ é¢˜

### ç»ƒä¹ 16.1ï¼šCLIPå›¾åƒæ£€ç´¢ï¼ˆåŸºç¡€ï¼‰

**é¢˜ç›®**ï¼šä½¿ç”¨CLIPå®ç°å›¾åƒæ£€ç´¢ç³»ç»Ÿ

**è¦æ±‚**ï¼š
1. å‡†å¤‡100å¼ å›¾åƒæ•°æ®é›†
2. ä½¿ç”¨CLIPåµŒå…¥å›¾åƒ
3. å®ç°ç›¸ä¼¼å›¾åƒæ£€ç´¢
4. æä¾›å¯è§†åŒ–ç»“æœ

---

### ç»ƒä¹ 16.2ï¼šå›¾æ–‡RAGå®ç°ï¼ˆè¿›é˜¶ï¼‰

**é¢˜ç›®**ï¼šæ„å»ºå›¾æ–‡ç»“åˆçš„RAGç³»ç»Ÿ

**è¦æ±‚**ï¼š
1. å›¾åƒæ£€ç´¢æ¨¡å—
2. æ–‡æœ¬æ£€ç´¢æ¨¡å—
3. ç»“æœèåˆç®—æ³•
4. å¤šæ¨¡æ€ç­”æ¡ˆç”Ÿæˆ
5. è¯„ä¼°ç³»ç»Ÿæ•ˆæœ

---

### ç»ƒä¹ 16.3ï¼šå¤šæ¨¡æ€Agentï¼ˆæŒ‘æˆ˜ï¼‰

**é¢˜ç›®**ï¼šå®ç°èƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬çš„Agent

**åŠŸèƒ½éœ€æ±‚**ï¼š
1. å›¾åƒç†è§£å’Œåˆ†æ
2. å›¾æ–‡ç»“åˆçš„æ¨ç†
3. å¤šå·¥å…·è°ƒç”¨
4. å¤æ‚ä»»åŠ¡å¤„ç†
5. Webç•Œé¢

---

## æ€»ç»“

### æœ¬ç« è¦ç‚¹

1. **å¤šæ¨¡æ€ä»·å€¼**
   - çªç ´å•æ¨¡æ€é™åˆ¶
   - ä¸°å¯Œçš„ä¿¡æ¯æ¥æº
   - æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

2. **CLIPæ¨¡å‹**
   - å›¾åƒ-æ–‡æœ¬å¯¹é½
   - è·¨æ¨¡æ€æ£€ç´¢
   - é›¶æ ·æœ¬èƒ½åŠ›

3. **å¤šæ¨¡æ€RAG**
   - å›¾åƒ+æ–‡æœ¬åµŒå…¥
   - è·¨æ¨¡æ€æ£€ç´¢
   - å¤šæ¨¡æ€ç­”æ¡ˆç”Ÿæˆ

4. **å¤šæ¨¡æ€Agent**
   - å¤šæ¨¡æ€å·¥å…·
   - GPT-4Vé›†æˆ
   - å¤æ‚ä»»åŠ¡å¤„ç†

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å¤šæ¨¡æ€åµŒå…¥
- [ ] æŒæ¡CLIPä½¿ç”¨
- [ ] å®ç°å›¾æ–‡RAG
- [ ] æ„å»ºå¤šæ¨¡æ€ç³»ç»Ÿ

### æ¨¡å—3å®Œæˆï¼

**æ­å–œå®Œæˆæ¨¡å—3çš„å­¦ä¹ ï¼** ğŸ‰

ä½ å·²ç»æŒæ¡äº†ï¼š
- âœ… Agentic RAGï¼ˆç¬¬13ç« ï¼‰
- âœ… é«˜çº§Agentæ¨¡å¼ï¼ˆç¬¬14ç« ï¼‰
- âœ… çŸ¥è¯†å›¾è°±RAGï¼ˆç¬¬15ç« ï¼‰
- âœ… å¤šæ¨¡æ€RAGï¼ˆç¬¬16ç« ï¼‰

**èƒ½åŠ›æå‡**ï¼š
- å¤æ‚é—®é¢˜è§£å†³èƒ½åŠ› +60%
- ç³»ç»Ÿè‡ªä¸»æ€§ +80%
- çŸ¥è¯†æ•´åˆèƒ½åŠ› +70%
- æ¶æ„è®¾è®¡èƒ½åŠ› +50%

### ä¸‹ä¸€æ­¥

**é€‰é¡¹1**ï¼šå¼€å§‹æ¨¡å—4 - ç”Ÿäº§éƒ¨ç½²å®æˆ˜
**é€‰é¡¹2**ï¼šåˆ›å»ºæ›´å¤šé…å¥—èµ„æº
**é€‰é¡¹3**ï¼šå¼€å§‹ç»¼åˆé¡¹ç›®

---

**æ¨¡å—3å®Œæˆï¼** ğŸŠğŸŠğŸŠ
