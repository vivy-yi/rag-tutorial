# ç¬¬13ç« ï¼šæ£€ç´¢å‹ç¼©ä¼˜åŒ–

> æ£€ç´¢ç»“æœå¤ªé•¿ï¼Ÿä¸Šä¸‹æ–‡çª—å£ä¸å¤Ÿç”¨ï¼Ÿæ£€ç´¢å‹ç¼©æŠ€æœ¯å¸®ä½ åœ¨ä¿ç•™å…³é”®ä¿¡æ¯çš„åŒæ—¶å¤§å¹…å‡å°‘tokenæ¶ˆè€—ï¼

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£æ£€ç´¢å‹ç¼©çš„åŸç†å’Œä»·å€¼
- [ ] æŒæ¡å¤šç§å‹ç¼©ç­–ç•¥
- [ ] å®ç°è¯­ä¹‰ä¿ç•™å‹ç¼©
- [ ] åº”ç”¨æ®‹è½çº§å‹ç¼©æŠ€æœ¯
- [ ] è¯„ä¼°å‹ç¼©æ•ˆæœ
- [ ] é€‰æ‹©åˆé€‚çš„å‹ç¼©æ–¹æ¡ˆ

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š4å°æ—¶
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â­â˜†

---

## å‰ç½®çŸ¥è¯†

- [ ] å®Œæˆç¬¬9ç« ï¼šæ··åˆæ£€ç´¢ä¸é‡æ’åº
- [ ] ç†è§£å‘é‡æ£€ç´¢åŸç†
- [ ] ç†Ÿæ‚‰LLMä¸Šä¸‹æ–‡é™åˆ¶
- [ ] äº†è§£RAGå·¥ä½œæµç¨‹

---

## 13.1 ä¸ºä»€ä¹ˆéœ€è¦æ£€ç´¢å‹ç¼©ï¼Ÿ

### 13.1.1 é—®é¢˜åˆ†æ

**åœºæ™¯1ï¼šé•¿æ–‡æ¡£æ£€ç´¢**

```
é—®é¢˜: "Pythonçš„å¼‚æ­¥ç¼–ç¨‹å¦‚ä½•ä½¿ç”¨ï¼Ÿ"

ä¼ ç»Ÿæ£€ç´¢:
â”œâ”€ æ£€ç´¢åˆ°3ä¸ªç›¸å…³æ–‡æ¡£
â”œâ”€ æ¯ä¸ªæ–‡æ¡£2000 tokens
â”œâ”€ æ€»å…±6000 tokens
â””â”€ ä¸Šä¸‹æ–‡çª—å£ç´§å¼ !

ç»“æœ:
âœ— å¯èƒ½è¶…å‡ºæ¨¡å‹é™åˆ¶
âœ— å¤§é‡æ— å…³ä¿¡æ¯
âœ— å“åº”é€Ÿåº¦æ…¢
âœ— APIæˆæœ¬é«˜
```

**åœºæ™¯2ï¼šå†—ä½™ä¿¡æ¯**

```
æ£€ç´¢ç»“æœ:
æ–‡æ¡£1: "Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨async/awaitè¯­æ³•..."
æ–‡æ¡£2: "Pythonçš„asyncioåº“æä¾›äº†å¼‚æ­¥ç¼–ç¨‹æ”¯æŒ..."
æ–‡æ¡£3: "å¼‚æ­¥ç¼–ç¨‹åœ¨Pythonä¸­é€šè¿‡async defå®ç°..."

é—®é¢˜:
- ä¸‰ä¸ªæ–‡æ¡£éƒ½åœ¨è¯´åŒä¸€ä»¶äº‹
- å¤§é‡é‡å¤ä¿¡æ¯
- å…³é”®ç‚¹è¢«ç¨€é‡Š
```

### 13.1.2 å‹ç¼©çš„ä»·å€¼

```
å‹ç¼©å‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–‡æ¡£1 (2000 tokens)            â”‚
â”‚ - å¼•è¨€: 300 tokens             â”‚
â”‚ - åŸºç¡€æ¦‚å¿µ: 500 tokens          â”‚
â”‚ - æ ¸å¿ƒå†…å®¹: 800 tokens          â”‚
â”‚ - ç¤ºä¾‹ä»£ç : 400 tokens          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–‡æ¡£2 (2000 tokens)            â”‚
â”‚ - ç±»ä¼¼ç»“æ„...                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ€»è®¡: 4000+ tokens

å‹ç¼©å:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‹ç¼©æ‘˜è¦ (500 tokens)           â”‚
â”‚ - åˆå¹¶æ ¸å¿ƒæ¦‚å¿µ                 â”‚
â”‚ - å»é™¤å†—ä½™ä¿¡æ¯                 â”‚
â”‚ - ä¿ç•™å…³é”®ç¤ºä¾‹                 â”‚
â”‚ - çªå‡ºé‡ç‚¹å†…å®¹                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ€»è®¡: 500 tokens (èŠ‚çœ87.5%)
```

**æ ¸å¿ƒä¼˜åŠ¿**:

| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| **èŠ‚çœæˆæœ¬** | å‡å°‘tokenæ¶ˆè€— â†’ é™ä½APIè°ƒç”¨æˆæœ¬ |
| **æå‡é€Ÿåº¦** | æ›´çŸ­çš„ä¸Šä¸‹æ–‡ â†’ æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ |
| **æé«˜è´¨é‡** | å»é™¤å™ªå£° â†’ èšç„¦å…³é”®ä¿¡æ¯ |
| **æ‰©å±•å®¹é‡** | ç›¸åŒçª—å£ â†’ æ£€ç´¢æ›´å¤šæ–‡æ¡£ |

---

## 13.2 å‹ç¼©ç­–ç•¥åˆ†ç±»

### 13.2.1 å››å¤§å‹ç¼©ç­–ç•¥

```
æ£€ç´¢å‹ç¼©ç­–ç•¥å±‚æ¬¡:

Level 1: ç®€å•æˆªæ–­
â”œâ”€ å›ºå®šé•¿åº¦æˆªæ–­
â”œâ”€ ä¿ç•™å¼€å¤´/ç»“å°¾
â””â”€ æˆæœ¬: ä½, æ•ˆæœ: å·®

Level 2: è§„åˆ™å‹ç¼©
â”œâ”€ å»é™¤åœç”¨è¯
â”œâ”€ å»é™¤æ ¼å¼æ ‡è®°
â”œâ”€ å»é™¤å†—ä½™å¥å­
â””â”€ æˆæœ¬: ä¸­, æ•ˆæœ: ä¸­

Level 3: è¯­ä¹‰å‹ç¼© â­
â”œâ”€ ä¿ç•™å…³é”®ä¿¡æ¯
â”œâ”€ é‡è¿°ç®€åŒ–è¡¨è¾¾
â”œâ”€ å¤šæ–‡æ¡£åˆå¹¶
â””â”€ æˆæœ¬: ä¸­, æ•ˆæœ: å¥½

Level 4: æ™ºèƒ½å‹ç¼© â­â­
â”œâ”€ LLMé‡å†™
â”œâ”€ æ®µè½çº§å‹ç¼©
â”œâ”€ æŸ¥è¯¢æ„ŸçŸ¥å‹ç¼©
â””â”€ æˆæœ¬: é«˜, æ•ˆæœ: æœ€å¥½
```

### 13.2.2 é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èç­–ç•¥ | ç†ç”± |
|------|---------|------|
| å®æ—¶é—®ç­” | Level 2 | é€Ÿåº¦å¿«ï¼Œè´¨é‡å¯æ¥å— |
| æŠ€æœ¯æ–‡æ¡£ | Level 3 | ä¿ç•™æŠ€æœ¯ç»†èŠ‚ |
| å¤æ‚æ¨ç† | Level 4 | éœ€è¦å®Œæ•´è¯­ä¹‰ |
| æˆæœ¬æ•æ„Ÿ | Level 2-3 | å¹³è¡¡æ•ˆæœå’Œæˆæœ¬ |

---

## 13.3 ç®€å•å‹ç¼©æŠ€æœ¯

### 13.3.1 å›ºå®šé•¿åº¦æˆªæ–­

```python
# æ–‡ä»¶åï¼šsimple_compression.py
"""
ç®€å•å‹ç¼©æ–¹æ³•
"""

def truncate_by_length(text: str, max_length: int) -> str:
    """
    æŒ‰å›ºå®šé•¿åº¦æˆªæ–­

    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æœ€å¤§é•¿åº¦(å­—ç¬¦æ•°)

    Returns:
        æˆªæ–­åçš„æ–‡æœ¬
    """
    if len(text) <= max_length:
        return text

    # ä¿ç•™å¼€å¤´80%ï¼Œç»“å°¾20%
    head_length = int(max_length * 0.8)
    tail_length = max_length - head_length

    head = text[:head_length]
    tail = text[-tail_length:] if tail_length > 0 else ""

    return f"{head}\n...\n{tail}"


# ç¤ºä¾‹
long_text = """
Pythonæ˜¯ä¸€é—¨é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚
Pythonä»¥å…¶ç®€æ´ã€æ˜“è¯»çš„è¯­æ³•è€Œé—»åï¼Œè¢«å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸã€‚

[æ­¤å¤„çœç•¥2000å­—]

Pythonæ‹¥æœ‰ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬NumPyã€Pandasã€Djangoç­‰æµè¡Œåº“ã€‚
è¿™ä½¿å¾—Pythonæˆä¸ºæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€Webå¼€å‘ç­‰é¢†åŸŸçš„é¦–é€‰è¯­è¨€ã€‚
"""

compressed = truncate_by_length(long_text, max_length=200)

print("å‹ç¼©å‰:", len(long_text), "å­—ç¬¦")
print("å‹ç¼©å:", len(compressed), "å­—ç¬¦")
print(f"å‹ç¼©ç‡: {len(compressed)/len(long_text)*100:.1f}%")
```

### 13.3.2 å»é™¤æ ¼å¼å’Œå†—ä½™

```python
import re

def clean_text(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ï¼šå»é™¤æ ¼å¼æ ‡è®°å’Œå†—ä½™å†…å®¹
    """
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)

    # å»é™¤Markdownæ ¼å¼
    text = re.sub(r'#{1,6}\s', '', text)  # æ ‡é¢˜
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # ç²—ä½“
    text = re.sub(r'\*(.*?)\*', r'\1', text)  # æ–œä½“
    text = re.sub(r'`(.*?)`', r'\1', text)  # ä»£ç 

    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)

    # å»é™¤ç‰¹æ®Šç¬¦å·
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()\-\"\']', '', text)

    return text.strip()


def remove_redundant_sentences(text: str, similarity_threshold: float = 0.9):
    """
    å»é™¤å†—ä½™å¥å­

    Args:
        text: è¾“å…¥æ–‡æœ¬
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
    """
    sentences = text.split('ã€‚')
    unique_sentences = []

    for sent in sentences:
        if not sent.strip():
            continue

        # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰å¥å­è¿‡äºç›¸ä¼¼
        is_redundant = False
        for existing in unique_sentences:
            # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå®é™…åº”è¯¥ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼‰
            if (sent.strip() in existing) or (existing in sent.strip()):
                is_redundant = True
                break

        if not is_redundant:
            unique_sentences.append(sent.strip())

    return 'ã€‚'.join(unique_sentences) + 'ã€‚'


# ç¤ºä¾‹
text_with_format = """
## Pythonç®€ä»‹

**Python**æ˜¯ä¸€é—¨*é«˜çº§*ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåˆ›å»ºã€‚

Pythonä»¥å…¶ç®€æ´ã€æ˜“è¯»çš„è¯­æ³•è€Œé—»åã€‚
Pythonçš„è¯­æ³•éå¸¸ç®€æ´ã€‚
Pythonçš„ä»£ç å¾ˆå®¹æ˜“é˜…è¯»ã€‚

Pythonè¢«å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸã€‚
"""

cleaned = clean_text(text_with_format)
deduped = remove_redundant_sentences(cleaned)

print("åŸå§‹æ–‡æœ¬:")
print(text_with_format)
print("\næ¸…ç†å»é‡å:")
print(deduped)
```

---

## 13.4 è¯­ä¹‰ä¿ç•™å‹ç¼© â­

### 13.4.1 åŸç†

**ç›®æ ‡**: åœ¨å¤§å¹…å‹ç¼©çš„åŒæ—¶ä¿ç•™å…³é”®è¯­ä¹‰

```
è¯­ä¹‰å‹ç¼©æµç¨‹:

åŸæ–‡æ¡£ (2000 tokens)
   â†“
[æå–å…³é”®ä¿¡æ¯]
   â”œâ”€ è¯†åˆ«æ ¸å¿ƒæ¦‚å¿µ
   â”œâ”€ æå–å…³é”®è®ºç‚¹
   â”œâ”€ ä¿ç•™é‡è¦ç¤ºä¾‹
   â””â”€ å»é™¤ä¿®é¥°å†…å®¹
   â†“
[é‡è¿°ç®€åŒ–]
   â”œâ”€ ä¿æŒåŸæ„
   â”œâ”€ ç®€åŒ–è¡¨è¾¾
   â””â”€ åˆå¹¶é‡å¤
   â†“
å‹ç¼©æ–‡æ¡£ (500 tokens)
   â””â”€ ä¿ç•™80%+çš„å…³é”®ä¿¡æ¯
```

### 13.4.2 å®Œæ•´å®ç°

```python
# æ–‡ä»¶åï¼šsemantic_compression.py
"""
è¯­ä¹‰ä¿ç•™å‹ç¼©å®ç°
"""

from typing import List, Dict
import json


class SemanticCompressor:
    """
    è¯­ä¹‰å‹ç¼©å™¨

    ç‰¹ç‚¹:
    1. ä¿ç•™å…³é”®ä¿¡æ¯
    2. ç®€åŒ–è¡¨è¾¾
    3. å»é™¤å†—ä½™
    4. ä¿æŒå¯è¯»æ€§
    """

    def __init__(self, llm):
        self.llm = llm

    def compress_single_document(self,
                                  document: str,
                                  compression_ratio: float = 0.25) -> str:
        """
        å‹ç¼©å•ä¸ªæ–‡æ¡£

        Args:
            document: åŸå§‹æ–‡æ¡£
            compression_ratio: å‹ç¼©æ¯”ä¾‹ (0-1), é»˜è®¤0.25è¡¨ç¤ºå‹ç¼©åˆ°25%

        Returns:
            å‹ç¼©åçš„æ–‡æ¡£
        """
        # è®¡ç®—ç›®æ ‡é•¿åº¦
        target_length = int(len(document) * compression_ratio)

        prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æ¡£å‹ç¼©åˆ°{target_length}å­—ç¬¦å·¦å³ã€‚

è¦æ±‚:
1. ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯
2. å»é™¤å†—ä½™å’Œä¿®é¥°å†…å®¹
3. ç®€åŒ–è¡¨è¾¾ä½†ä¿æŒåŸæ„
4. ä¿æŒé€»è¾‘ç»“æ„æ¸…æ™°
5. ä¿ç•™é‡è¦ç¤ºä¾‹

åŸæ–‡æ¡£:
{document}

è¯·è¾“å‡ºå‹ç¼©åçš„æ–‡æ¡£:"""

        compressed = self.llm.predict(prompt)
        return compressed

    def compress_multiple_documents(self,
                                     documents: List[str],
                                     query: str,
                                     max_total_length: int = 1000) -> str:
        """
        å‹ç¼©å¹¶åˆå¹¶å¤šä¸ªæ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            query: åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºç›¸å…³æ€§åˆ¤æ–­ï¼‰
            max_total_length: æœ€å¤§æ€»é•¿åº¦

        Returns:
            åˆå¹¶å‹ç¼©åçš„æ–‡æ¡£
        """
        print(f"å‹ç¼©{len(documents)}ä¸ªæ–‡æ¡£ï¼Œç›®æ ‡é•¿åº¦: {max_total_length}")

        # æ­¥éª¤1: é€ä¸ªå‹ç¼©æ–‡æ¡£
        compressed_docs = []
        for i, doc in enumerate(documents, 1):
            print(f"  å‹ç¼©æ–‡æ¡£ {i}/{len(documents)}...")

            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…é•¿åº¦é¢„ç®—
            budget = max_total_length // len(documents)

            compressed = self._compress_with_context(
                document=doc,
                query=query,
                max_length=budget
            )
            compressed_docs.append(compressed)

        # æ­¥éª¤2: åˆå¹¶å‹ç¼©åçš„æ–‡æ¡£
        merged = self._merge_compressed_documents(
            compressed_docs,
            query=query,
            max_length=max_total_length
        )

        return merged

    def _compress_with_context(self,
                                document: str,
                                query: str,
                                max_length: int) -> str:
        """
        åŸºäºæŸ¥è¯¢ä¸Šä¸‹æ–‡å‹ç¼©æ–‡æ¡£
        """
        prompt = f"""ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·å°†ä»¥ä¸‹æ–‡æ¡£å‹ç¼©åˆ°{max_length}å­—ç¬¦ä»¥å†…ï¼Œè¦æ±‚:
1. ä¼˜å…ˆä¿ç•™ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹
2. ä¿ç•™å…³é”®ç»†èŠ‚å’Œæ•°æ®
3. å»é™¤ä¸æŸ¥è¯¢æ— å…³çš„å†…å®¹
4. ç®€åŒ–è¡¨è¾¾

æ–‡æ¡£:
{document}

å‹ç¼©ç»“æœ:"""

        return self.llm.predict(prompt)

    def _merge_compressed_documents(self,
                                     compressed_docs: List[str],
                                     query: str,
                                     max_length: int) -> str:
        """
        åˆå¹¶å¤šä¸ªå‹ç¼©æ–‡æ¡£
        """
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡é•¿åº¦
        avg_length = max_length // len(compressed_docs)

        # å†æ¬¡å‹ç¼©ä»¥ç¡®ä¿æ€»é•¿åº¦ä¸è¶…é™
        final_docs = []
        for doc in compressed_docs:
            if len(doc) > avg_length:
                # è¿›ä¸€æ­¥å‹ç¼©
                prompt = f"""è¯·å°†ä»¥ä¸‹å†…å®¹è¿›ä¸€æ­¥å‹ç¼©åˆ°{avg_length}å­—ç¬¦ï¼š

{doc}

è¦æ±‚: ä¿ç•™æœ€æ ¸å¿ƒçš„ä¿¡æ¯"""
                doc = self.llm.predict(prompt)
            final_docs.append(doc)

        # åˆå¹¶
        merged = "\n\n".join(final_docs)

        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œæ•´ä½“å‹ç¼©
        if len(merged) > max_length:
            prompt = f"""è¯·å°†ä»¥ä¸‹å†…å®¹æ•´ä½“å‹ç¼©åˆ°{max_length}å­—ç¬¦ï¼š

{merged}

è¦æ±‚: å»é™¤å†—ä½™ï¼Œåˆå¹¶é‡å¤å†…å®¹"""
            merged = self.llm.predict(prompt)

        return merged


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡æ‹ŸLLM
    class MockLLM:
        def predict(self, prompt):
            if "å‹ç¼©åˆ°" in prompt:
                # ç®€å•æ¨¡æ‹Ÿå‹ç¼©
                return "è¿™æ˜¯å‹ç¼©åçš„å†…å®¹ã€‚åŒ…å«äº†åŸæ–‡æ¡£çš„å…³é”®ä¿¡æ¯ï¼Œå»é™¤äº†å†—ä½™å†…å®¹ã€‚"
            return "æ¨¡æ‹Ÿå“åº”"

    # åˆ›å»ºå‹ç¼©å™¨
    llm = MockLLM()
    compressor = SemanticCompressor(llm)

    # ç¤ºä¾‹æ–‡æ¡£
    docs = [
        "Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨async/awaitè¯­æ³•ã€‚asyncioæ˜¯Pythonçš„æ ‡å‡†åº“...",
        "å¼‚æ­¥ç¼–ç¨‹å¯ä»¥æé«˜ç¨‹åºçš„å¹¶å‘æ€§èƒ½ã€‚é€šè¿‡äº‹ä»¶å¾ªç¯æœºåˆ¶...",
        "Pythonçš„asyncioæ¨¡å—æä¾›äº†ä¸°å¯Œçš„äº‹ä»¶å¾ªç¯APIã€‚åŒ…æ‹¬Taskã€Future..."
    ]

    # å‹ç¼©å¹¶åˆå¹¶
    query = "Pythonå¼‚æ­¥ç¼–ç¨‹å¦‚ä½•ä½¿ç”¨ï¼Ÿ"
    result = compressor.compress_multiple_documents(
        documents=docs,
        query=query,
        max_total_length=300
    )

    print(f"å‹ç¼©ç»“æœ:\n{result}")
```

---

## 13.5 æ®µè½çº§å‹ç¼© â­â­

### 13.5.1 ä»€ä¹ˆæ˜¯æ®µè½çº§å‹ç¼©ï¼Ÿ

**ä¼ ç»Ÿå‹ç¼©** vs **æ®µè½çº§å‹ç¼©**:

```
ä¼ ç»Ÿå‹ç¼©:
æ–‡æ¡£1 (2000 tokens) â†’ LLM â†’ å‹ç¼©å (500 tokens)
æ–‡æ¡£2 (2000 tokens) â†’ LLM â†’ å‹ç¼©å (500 tokens)
æ–‡æ¡£3 (2000 tokens) â†’ LLM â†’ å‹ç¼©å (500 tokens)

é—®é¢˜:
- æ¯ä¸ªæ–‡æ¡£ç‹¬ç«‹å‹ç¼©ï¼Œå¯èƒ½ä¸¢å¤±å…³è”ä¿¡æ¯
- éœ€è¦å¤šæ¬¡LLMè°ƒç”¨ï¼Œæˆæœ¬é«˜

æ®µè½çº§å‹ç¼©:
æ–‡æ¡£1-3 (6000 tokens)
   â†“
[æ‹†åˆ†æ®µè½]
   â”œâ”€ æ®µè½1-1 (500 tokens)
   â”œâ”€ æ®µè½1-2 (500 tokens)
   â”œâ”€ æ®µè½2-1 (500 tokens)
   â””â”€ ...
   â†“
[é€æ®µå‹ç¼©]
   â”œâ”€ æ®µè½1-1' (200 tokens) â­ ä¿ç•™
   â”œâ”€ æ®µè½1-2' (200 tokens) â­ ä¿ç•™
   â”œâ”€ æ®µè½2-1' (200 tokens) âŠ— ä¸¢å¼ƒ(ç›¸å…³æ€§ä½)
   â””â”€ ...
   â†“
[æ™ºèƒ½åˆå¹¶]
æœ€ç»ˆæ–‡æ¡£ (1000 tokens)
```

### 13.5.2 å®Œæ•´å®ç°

```python
# æ–‡ä»¶åï¼šparagraph_compression.py
"""
æ®µè½çº§å‹ç¼©å®ç°
"""

from typing import List, Tuple
import re


class ParagraphCompressor:
    """
    æ®µè½çº§å‹ç¼©å™¨

    ç‰¹ç‚¹:
    1. æ®µè½çº§åˆ«çš„ç»†ç²’åº¦æ§åˆ¶
    2. ç›¸å…³æ€§ç­›é€‰
    3. æ™ºèƒ½åˆå¹¶
    4. æ›´å¥½çš„å¯è§£é‡Šæ€§
    """

    def __init__(self, llm):
        self.llm = llm

    def compress(self,
                 documents: List[str],
                 query: str,
                 target_length: int) -> Tuple[str, Dict]:
        """
        æ®µè½çº§å‹ç¼©ä¸»æ–¹æ³•

        Returns:
            (å‹ç¼©åçš„æ–‡æ¡£, ç»Ÿè®¡ä¿¡æ¯)
        """
        stats = {
            "original_length": sum(len(d) for d in documents),
            "num_paragraphs": 0,
            "num_kept": 0,
            "num_discarded": 0,
            "compression_ratio": 0.0
        }

        # æ­¥éª¤1: æ‹†åˆ†æ‰€æœ‰æ–‡æ¡£ä¸ºæ®µè½
        paragraphs = self._split_into_paragraphs(documents)
        stats["num_paragraphs"] = len(paragraphs)
        print(f"æ‹†åˆ†å¾—åˆ° {len(paragraphs)} ä¸ªæ®µè½")

        # æ­¥éª¤2: è¯„ä¼°æ®µè½ç›¸å…³æ€§
        scored_paragraphs = self._score_relevance(paragraphs, query)

        # æ­¥éª¤3: é€‰æ‹©é«˜ç›¸å…³æ€§æ®µè½
        selected = self._select_paragraphs(
            scored_paragraphs,
            target_length=target_length
        )
        stats["num_kept"] = len(selected)
        stats["num_discarded"] = len(paragraphs) - len(selected)

        # æ­¥éª¤4: å‹ç¼©é€‰ä¸­çš„æ®µè½
        compressed_paragraphs = []
        for para, score in selected:
            compressed = self._compress_paragraph(para, query)
            compressed_paragraphs.append(compressed)

        # æ­¥éª¤5: åˆå¹¶
        final_doc = "\n\n".join(compressed_paragraphs)

        stats["final_length"] = len(final_doc)
        stats["compression_ratio"] = stats["final_length"] / stats["original_length"]

        return final_doc, stats

    def _split_into_paragraphs(self, documents: List[str]) -> List[str]:
        """
        å°†æ–‡æ¡£æ‹†åˆ†ä¸ºæ®µè½
        """
        paragraphs = []

        for doc in documents:
            # æŒ‰åŒæ¢è¡Œç¬¦æ‹†åˆ†
            doc_paragraphs = doc.split('\n\n')
            paragraphs.extend([p.strip() for p in doc_paragraphs if p.strip()])

        return paragraphs

    def _score_relevance(self,
                         paragraphs: List[str],
                         query: str) -> List[Tuple[str, float]]:
        """
        è¯„ä¼°æ®µè½ç›¸å…³æ€§

        Returns:
            [(æ®µè½, ç›¸å…³æ€§åˆ†æ•°), ...]
        """
        scored = []

        for para in paragraphs:
            # ç®€å•çš„ç›¸å…³æ€§è¯„ä¼°ï¼ˆå®é™…åº”è¯¥ç”¨embeddingæˆ–æ›´å¤æ‚çš„æ–¹æ³•ï¼‰
            score = self._calculate_relevance(para, query)
            scored.append((para, score))

        # æŒ‰ç›¸å…³æ€§æ’åº
        scored.sort(key=lambda x: x[1], reverse=True)

        return scored

    def _calculate_relevance(self, paragraph: str, query: str) -> float:
        """
        è®¡ç®—æ®µè½ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        """
        # ç®€åŒ–ç‰ˆï¼šåŸºäºå…³é”®è¯é‡å 
        query_words = set(query.lower().split())
        para_words = set(paragraph.lower().split())

        overlap = len(query_words & para_words)
        return min(overlap / len(query_words), 1.0) if query_words else 0.0

    def _select_paragraphs(self,
                            scored_paragraphs: List[Tuple[str, float]],
                            target_length: int) -> List[Tuple[str, float]]:
        """
        é€‰æ‹©æ®µè½ï¼Œç¡®ä¿æ€»é•¿åº¦ä¸è¶…è¿‡ç›®æ ‡
        """
        selected = []
        current_length = 0

        for para, score in scored_paragraphs:
            # é¢„ä¼°å‹ç¼©åçš„é•¿åº¦ï¼ˆå‡è®¾å‹ç¼©åˆ°40%ï¼‰
            estimated_length = len(para) * 0.4

            if current_length + estimated_length <= target_length:
                selected.append((para, score))
                current_length += estimated_length
            else:
                # å°è¯•æˆªæ–­å½“å‰æ®µè½
                remaining = target_length - current_length
                if remaining > 100:  # è‡³å°‘ä¿ç•™100å­—ç¬¦
                    truncated = para[:int(remaining * 2.5)]  # åæ¨åŸå§‹é•¿åº¦
                    selected.append((truncated, score))
                break

        return selected

    def _compress_paragraph(self, paragraph: str, query: str) -> str:
        """
        å‹ç¼©å•ä¸ªæ®µè½
        """
        # ç›®æ ‡ï¼šå‹ç¼©åˆ°åŸå§‹é•¿åº¦çš„40%
        target_length = max(len(paragraph) * 0.4, 50)

        prompt = f"""ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·å°†ä»¥ä¸‹æ®µè½å‹ç¼©åˆ°{int(target_length)}å­—ç¬¦å·¦å³ã€‚

è¦æ±‚:
1. ä¿ç•™ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯
2. å»é™¤å†—ä½™å’Œä¿®é¥°
3. ä¿æŒè¯­ä¹‰å®Œæ•´

æ®µè½:
{paragraph}

å‹ç¼©ç»“æœ:"""

        return self.llm.predict(prompt)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    class MockLLM:
        def predict(self, prompt):
            return "è¿™æ˜¯å‹ç¼©åçš„æ®µè½ï¼Œä¿ç•™äº†å…³é”®ä¿¡æ¯ã€‚"

    llm = MockLLM()
    compressor = ParagraphCompressor(llm)

    docs = [
        """
        Pythonçš„å¼‚æ­¥ç¼–ç¨‹æ˜¯ä¸€ä¸ªé‡è¦çš„ç‰¹æ€§ã€‚å®ƒå…è®¸ç¨‹åºåœ¨ç­‰å¾…IOæ“ä½œæ—¶æ‰§è¡Œå…¶ä»–ä»»åŠ¡ã€‚

        async/awaitè¯­æ³•æ˜¯Python 3.5å¼•å…¥çš„ï¼Œç”¨äºç®€åŒ–å¼‚æ­¥ä»£ç çš„ç¼–å†™ã€‚

        asyncioæ˜¯Pythonçš„æ ‡å‡†åº“ï¼Œæä¾›äº†äº‹ä»¶å¾ªç¯ã€åç¨‹ã€Futureç­‰ç»„ä»¶ã€‚
        """,

        """
        å¼‚æ­¥ç¼–ç¨‹å¯ä»¥æ˜¾è‘—æé«˜IOå¯†é›†å‹åº”ç”¨çš„æ€§èƒ½ã€‚

        åœ¨Webå¼€å‘ä¸­ï¼Œå¼‚æ­¥æ¡†æ¶å¦‚FastAPIã€aiohttpå¯ä»¥åˆ©ç”¨å¼‚æ­¥ç‰¹æ€§å¤„ç†å¤§é‡å¹¶å‘è¯·æ±‚ã€‚

        å¼‚æ­¥ç¼–ç¨‹çš„æŒ‘æˆ˜åŒ…æ‹¬ï¼šè°ƒè¯•å›°éš¾ã€å›è°ƒåœ°ç‹±ã€å¹¶å‘æ§åˆ¶ç­‰ã€‚
        """,

        """
        Pythonçš„å¼‚æ­¥ç”Ÿæ€ç³»ç»ŸåŒ…æ‹¬å¾ˆå¤šç¬¬ä¸‰æ–¹åº“ã€‚

        aiohttpç”¨äºå¼‚æ­¥HTTPè¯·æ±‚ï¼Œasyncpgç”¨äºå¼‚æ­¥æ•°æ®åº“æ“ä½œã€‚

        é€‰æ‹©åˆé€‚çš„å¼‚æ­¥åº“å¯¹äºæ„å»ºé«˜æ€§èƒ½åº”ç”¨å¾ˆé‡è¦ã€‚
        """
    ]

    query = "Pythonå¼‚æ­¥ç¼–ç¨‹æœ‰å“ªäº›ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Ÿ"
    result, stats = compressor.compress(docs, query, target_length=500)

    print("å‹ç¼©ç»“æœ:")
    print(result)
    print("\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸå§‹é•¿åº¦: {stats['original_length']}")
    print(f"  æœ€ç»ˆé•¿åº¦: {stats['final_length']}")
    print(f"  æ®µè½æ€»æ•°: {stats['num_paragraphs']}")
    print(f"  ä¿ç•™: {stats['num_kept']}")
    print(f"  ä¸¢å¼ƒ: {stats['num_discarded']}")
    print(f"  å‹ç¼©ç‡: {stats['compression_ratio']:.1%}")
```

---

## 13.6 å‹ç¼©æ•ˆæœè¯„ä¼°

### 13.6.1 è¯„ä¼°æŒ‡æ ‡

```python
# æ–‡ä»¶åï¼šcompression_evaluation.py
"""
å‹ç¼©æ•ˆæœè¯„ä¼°
"""

from typing import Dict, List
import math


class CompressionEvaluator:
    """
    å‹ç¼©æ•ˆæœè¯„ä¼°å™¨
    """

    def evaluate(self,
                 original: str,
                 compressed: str,
                 query: str) -> Dict:
        """
        è¯„ä¼°å‹ç¼©æ•ˆæœ

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        return {
            "compression_ratio": self._compression_ratio(original, compressed),
            "token_savings": self._token_savings(original, compressed),
            "semantic_similarity": self._semantic_similarity(original, compressed),
            "query_relevance": self._query_relevance(compressed, query),
            "readability": self._readability(compressed)
        }

    def _compression_ratio(self, original: str, compressed: str) -> float:
        """å‹ç¼©æ¯”ä¾‹"""
        return len(compressed) / len(original)

    def _token_savings(self, original: str, compressed: str) -> Dict:
        """TokenèŠ‚çœ"""
        # ç®€åŒ–ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦=1tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦=1token
        original_tokens = len(original) / 2
        compressed_tokens = len(compressed) / 2

        return {
            "original_tokens": int(original_tokens),
            "compressed_tokens": int(compressed_tokens),
            "saved_tokens": int(original_tokens - compressed_tokens),
            "savings_ratio": (original_tokens - compressed_tokens) / original_tokens
        }

    def _semantic_similarity(self, original: str, compressed: str) -> float:
        """
        è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰

        å®é™…åº”è¯¥ä½¿ç”¨embeddingè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # ç®€åŒ–ç‰ˆï¼šåŸºäºå…³é”®è¯é‡å 
        orig_words = set(original.lower().split())
        comp_words = set(compressed.lower().split())

        overlap = len(orig_words & comp_words)
        union = len(orig_words | comp_words)

        return overlap / union if union > 0 else 0.0

    def _query_relevance(self, compressed: str, query: str) -> float:
        """æŸ¥è¯¢ç›¸å…³æ€§"""
        query_words = set(query.lower().split())
        comp_words = set(compressed.lower().split())

        overlap = len(query_words & comp_words)
        return overlap / len(query_words) if query_words else 0.0

    def _readability(self, text: str) -> Dict:
        """å¯è¯»æ€§åˆ†æ"""
        sentences = text.split('ã€‚')
        words = text.split()

        return {
            "avg_sentence_length": len(words) / len(sentences) if sentences else 0,
            "num_sentences": len(sentences),
            "num_words": len(words)
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    original = "Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨async/awaitè¯­æ³•ã€‚asyncioæ˜¯Pythonçš„æ ‡å‡†åº“ï¼Œæä¾›äº†äº‹ä»¶å¾ªç¯æœºåˆ¶ã€‚å¼‚æ­¥ç¼–ç¨‹å¯ä»¥æé«˜ç¨‹åºçš„å¹¶å‘æ€§èƒ½ï¼Œç‰¹åˆ«é€‚åˆIOå¯†é›†å‹ä»»åŠ¡ã€‚"
    compressed = "Pythonå¼‚æ­¥ç¼–ç¨‹ç”¨async/awaitè¯­æ³•ï¼Œé€šè¿‡asyncioçš„äº‹ä»¶å¾ªç¯æœºåˆ¶æå‡IOå¯†é›†å‹ä»»åŠ¡çš„å¹¶å‘æ€§èƒ½ã€‚"

    evaluator = CompressionEvaluator()
    results = evaluator.evaluate(original, compressed, "Pythonå¼‚æ­¥ç¼–ç¨‹")

    print("å‹ç¼©æ•ˆæœè¯„ä¼°:")
    for metric, value in results.items():
        if isinstance(value, dict):
            print(f"\n{metric}:")
            for k, v in value.items():
                print(f"  {k}: {v}")
        else:
            print(f"{metric}: {value}")
```

---

## 13.7 å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ™ºèƒ½å®¢æœRAGç³»ç»Ÿ

```python
# åœºæ™¯ï¼šç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°å¤šä¸ªé•¿æ–‡æ¡£
query = "å¦‚ä½•é€€æ¬¾ï¼Ÿ"
retrieved_docs = [
    "ç”¨æˆ·å¯ä»¥åœ¨è®¢å•è¯¦æƒ…é¡µç‚¹å‡»é€€æ¬¾æŒ‰é’®...ï¼ˆ1500å­—ï¼‰",
    "é€€æ¬¾æµç¨‹åŒ…æ‹¬æäº¤ç”³è¯·ã€å•†å®¶å®¡æ ¸ã€å¹³å°å¤„ç†...ï¼ˆ1200å­—ï¼‰",
    "é€€æ¬¾è§„åˆ™ï¼š7å¤©æ— ç†ç”±é€€æ¬¾ã€è´¨é‡é—®é¢˜é€€æ¬¾...ï¼ˆ1000å­—ï¼‰"
]

# ä½¿ç”¨æ£€ç´¢å‹ç¼©
compressor = SemanticCompressor(llm)
compressed = compressor.compress_multiple_documents(
    documents=retrieved_docs,
    query=query,
    max_total_length=500
)

# ç»“æœï¼šä»3700å­—å‹ç¼©åˆ°500å­—ï¼Œä¿ç•™æ‰€æœ‰é€€æ¬¾å…³é”®ä¿¡æ¯
```

### æ¡ˆä¾‹2ï¼šæŠ€æœ¯æ–‡æ¡£é—®ç­”

```python
# åœºæ™¯ï¼šå¼€å‘è€…æŸ¥è¯¢APIç”¨æ³•
query = "FastAPIçš„Dependency Injectionå¦‚ä½•ä½¿ç”¨ï¼Ÿ"
retrieved_docs = [
    # ä»FastAPIå®˜æ–¹æ–‡æ¡£æ£€ç´¢åˆ°çš„å¤šä¸ªé•¿ç« èŠ‚
    "Dependencies Intro ...",
    "Dependencies Advanced ...",
    "Dependencies Security ..."
]

# ä½¿ç”¨æ®µè½çº§å‹ç¼©ï¼Œä¿ç•™ä»£ç ç¤ºä¾‹
compressor = ParagraphCompressor(llm)
compressed, stats = compressor.compress(
    documents=retrieved_docs,
    query=query,
    target_length=800
)

# ç»“æœï¼šä¿ç•™æ‰€æœ‰ä»£ç ç¤ºä¾‹ï¼Œå‹ç¼©è§£é‡Šæ€§æ–‡å­—
```

---

## ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå®ç°åŸºç¡€å‹ç¼©å™¨

**é¢˜ç›®**ï¼šå®ç°ä¸€ä¸ªç®€å•çš„æ–‡æœ¬å‹ç¼©å™¨

**è¦æ±‚**ï¼š
1. æ”¯æŒå›ºå®šé•¿åº¦æˆªæ–­
2. å»é™¤æ ¼å¼æ ‡è®°
3. å»é™¤å†—ä½™å¥å­
4. è®¡ç®—å‹ç¼©æ¯”ä¾‹

---

### ç»ƒä¹ 2ï¼šè¯­ä¹‰å‹ç¼©å®ç°

**é¢˜ç›®**ï¼šå®ç°è¯­ä¹‰ä¿ç•™å‹ç¼©

**è¦æ±‚**ï¼š
1. é›†æˆLLMè¿›è¡Œæ™ºèƒ½å‹ç¼©
2. æ”¯æŒå•æ–‡æ¡£å‹ç¼©
3. æ”¯æŒå¤šæ–‡æ¡£åˆå¹¶å‹ç¼©
4. ä¿ç•™æŸ¥è¯¢ç›¸å…³å†…å®¹

---

### ç»ƒä¹ 3ï¼šå‹ç¼©æ•ˆæœè¯„ä¼°

**é¢˜ç›®**ï¼šæ„å»ºå®Œæ•´çš„è¯„ä¼°ç³»ç»Ÿ

**è¦æ±‚**ï¼š
1. å®ç°å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
2. å¯¹æ¯”ä¸åŒå‹ç¼©ç­–ç•¥
3. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
4. æä¾›ä¼˜åŒ–å»ºè®®

---

## æ€»ç»“

### æœ¬ç« è¦ç‚¹

1. **å‹ç¼©ä»·å€¼**
   - èŠ‚çœæˆæœ¬
   - æå‡é€Ÿåº¦
   - æé«˜è´¨é‡
   - æ‰©å±•å®¹é‡

2. **å‹ç¼©ç­–ç•¥**
   - ç®€å•æˆªæ–­
   - è§„åˆ™å‹ç¼©
   - è¯­ä¹‰å‹ç¼©
   - æ®µè½çº§å‹ç¼©

3. **å®ç°æ–¹æ³•**
   - SemanticCompressor
   - ParagraphCompressor
   - æ•ˆæœè¯„ä¼°

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£æ£€ç´¢å‹ç¼©çš„ä»·å€¼
- [ ] æŒæ¡å¤šç§å‹ç¼©ç­–ç•¥
- [ ] èƒ½å¤Ÿå®ç°è¯­ä¹‰å‹ç¼©
- [ ] èƒ½å¤Ÿå®ç°æ®µè½çº§å‹ç¼©
- [ ] èƒ½å¤Ÿè¯„ä¼°å‹ç¼©æ•ˆæœ
- [ ] èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„å‹ç¼©æ–¹æ¡ˆ

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- **ä¸‹ä¸€ç« **ï¼š[ç¬¬14ç« ï¼šç»¼åˆé¡¹ç›®ä¼˜åŒ–](./14-ç»¼åˆé¡¹ç›®ä¼˜åŒ–.md)
- **ç›¸å…³ç« èŠ‚**ï¼š
  - [ç¬¬9ç« ï¼šæ··åˆæ£€ç´¢ä¸é‡æ’åº](./09-æ··åˆæ£€ç´¢ä¸é‡æ’åº.md)
  - [ç¬¬11ç« ï¼šæ€§èƒ½ä¼˜åŒ–](./11-æ€§èƒ½ä¼˜åŒ–.md)

---

**æ­å–œå®Œæˆç¬¬13ç« ï¼** ğŸ‰

**æŒæ¡æ£€ç´¢å‹ç¼©ï¼Œè®©RAGç³»ç»Ÿæ›´é«˜æ•ˆï¼** â†’ [ç¬¬14ç« ](./14-ç»¼åˆé¡¹ç›®ä¼˜åŒ–.md)
