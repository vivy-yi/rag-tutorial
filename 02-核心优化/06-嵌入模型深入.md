# ç¬¬6ç« ï¼šåµŒå…¥æ¨¡å‹æ·±å…¥

> åµŒå…¥æ¨¡å‹æ˜¯RAGç³»ç»Ÿçš„"ç†è§£åŸºç¡€"ã€‚é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹å¯ä»¥å°†æ£€ç´¢è´¨é‡æå‡10-15%ã€‚

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£TransformeråµŒå…¥æ¨¡å‹çš„åŸç†
- [ ] å¯¹æ¯”ä¸»æµåµŒå…¥æ¨¡å‹çš„æ€§èƒ½
- [ ] æŒæ¡æ¨¡å‹é€‰æ‹©æ–¹æ³•
- [ ] å®ç°åµŒå…¥æ¨¡å‹å¾®è°ƒ
- [ ] å¯è§†åŒ–å’Œè¯„ä¼°åµŒå…¥è´¨é‡

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š3å°æ—¶
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â˜†â˜†

---

## å‰ç½®çŸ¥è¯†

- [ ] å®Œæˆæ¨¡å—1ç¬¬3ç« ï¼ˆç†è§£åŸºç¡€åµŒå…¥ï¼‰
- [ ] äº†è§£TransformeråŸºç¡€æ¶æ„
- [ ] ç†è§£å‘é‡ç›¸ä¼¼åº¦è®¡ç®—

**ç¯å¢ƒè¦æ±‚**ï¼š
- GPUæ¨èï¼ˆç”¨äºæ¨¡å‹å¾®è°ƒï¼‰
- è‡³å°‘8GB RAM
- sentence-transformersåº“

---

## 6.1 åµŒå…¥æ¨¡å‹åŸç†

### ä»æ–‡æœ¬åˆ°å‘é‡

#### ä»€ä¹ˆæ˜¯åµŒå…¥ï¼Ÿ

**åµŒå…¥ï¼ˆEmbeddingï¼‰**æ˜¯å°†é«˜ç»´çš„ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ï¼‰æ˜ å°„åˆ°ä½ç»´çš„è¿ç»­å‘é‡ç©ºé—´çš„è¿‡ç¨‹ã€‚

```
æ–‡æœ¬ç©ºé—´ï¼ˆç¦»æ•£ã€é«˜ç»´ï¼‰
  "è‹¹æœ"  "é¦™è•‰"  "æ©™å­"
    â†“       â†“       â†“
åµŒå…¥æ¨¡å‹
    â†“       â†“       â†“
å‘é‡ç©ºé—´ï¼ˆè¿ç»­ã€ä½ç»´ï¼‰
 [0.23]  [0.45]  [0.67]
 [-0.12] [0.33]  [-0.21]
 [0.56]  [-0.44] [0.78]
 ...
 (768ç»´)
```

#### ä¸ºä»€ä¹ˆä½¿ç”¨åµŒå…¥ï¼Ÿ

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æ˜ å°„åˆ°ç›¸è¿‘çš„å‘é‡ä½ç½®

```
å‘é‡ç©ºé—´ï¼ˆç®€åŒ–ä¸º2Dï¼‰ï¼š

      çŒ« ğŸ±
      /|\
     / | \
 ç‹— ğŸ¶  |  è€è™ğŸ¯
        |
    ç”µè„‘ ğŸ’»

"çŒ«" å’Œ "ç‹—" è·ç¦»è¿‘ â†’ è¯­ä¹‰ç›¸ä¼¼ï¼ˆéƒ½æ˜¯å® ç‰©ï¼‰
"ç”µè„‘" å’Œ "çŒ«" è·ç¦»è¿œ â†’ è¯­ä¹‰ä¸åŒ
```

### Transformeræ¶æ„å›é¡¾

#### ç¼–ç å™¨-è§£ç å™¨ç»“æ„

```
è¾“å…¥æ–‡æœ¬
    â†“
TokenåŒ–: ["æˆ‘", "çˆ±", "ç¼–ç¨‹"]
    â†“
åµŒå…¥å±‚: [vec1, vec2, vec3]
    â†“
Transformerç¼–ç å™¨
    â”œâ”€ è‡ªæ³¨æ„åŠ›å±‚
    â”œâ”€ å‰é¦ˆç½‘ç»œå±‚
    â””â”€ å±‚å½’ä¸€åŒ–
    â†“
è¾“å‡º: ä¸Šä¸‹æ–‡ç›¸å…³çš„è¡¨ç¤º
```

#### BERTæ¶æ„ï¼ˆå¸¸ç”¨åµŒå…¥æ¨¡å‹åŸºç¡€ï¼‰

```python
# æ–‡ä»¶åï¼š06_01_bert_architecture.py
"""
BERTæ¶æ„ç®€åŒ–æ¼”ç¤º
"""

import torch
import torch.nn as nn

class SimplifiedBERT(nn.Module):
    """ç®€åŒ–çš„BERTæ¨¡å‹"""

    def __init__(self, vocab_size, hidden_size=768, num_layers=12):
        super().__init__()

        # 1. TokenåµŒå…¥
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)

        # 2. ä½ç½®åµŒå…¥
        self.pos_embedding = nn.Embedding(512, hidden_size)

        # 3. Transformerç¼–ç å™¨å±‚
        self.encoder_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=hidden_size,
                nhead=12,
                dim_feedforward=3072
            ) for _ in range(num_layers)
        ])

    def forward(self, input_ids):
        # TokenåµŒå…¥
        token_embeds = self.token_embedding(input_ids)

        # ä½ç½®åµŒå…¥
        pos_ids = torch.arange(input_ids.size(1)).unsqueeze(0)
        pos_embeds = self.pos_embedding(pos_ids)

        # ç»„åˆåµŒå…¥
        embeddings = token_embeds + pos_embeds

        # Transformerç¼–ç 
        for layer in self.encoder_layers:
            embeddings = layer(embeddings)

        return embeddings

# æ¼”ç¤º
if __name__ == "__main__":
    print("="*60)
    print("BERTæ¶æ„æ¼”ç¤º")
    print("="*60 + "\n")

    # åˆ›å»ºæ¨¡å‹
    model = SimplifiedBERT(vocab_size=30000, hidden_size=768, num_layers=12)

    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"åµŒå…¥ç»´åº¦: 768")
    print(f"æ³¨æ„åŠ›å¤´æ•°: 12")
    print(f"ç¼–ç å™¨å±‚æ•°: 12")

    # ç¤ºä¾‹è¾“å…¥
    input_ids = torch.randint(0, 30000, (1, 10))  # æ‰¹æ¬¡å¤§å°1ï¼Œåºåˆ—é•¿åº¦10

    print(f"\nè¾“å…¥å½¢çŠ¶: {input_ids.shape}")

    # å‰å‘ä¼ æ’­
    outputs = model(input_ids)

    print(f"è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
    print(f"\nâœ… BERTå°†tokenåºåˆ—è½¬æ¢ä¸ºä¸Šä¸‹æ–‡ç›¸å…³çš„åµŒå…¥å‘é‡")
```

#### åµŒå…¥ç”Ÿæˆè¿‡ç¨‹

```python
# æ–‡ä»¶åï¼š06_02_embedding_generation.py
"""
æ¼”ç¤ºBERTå¦‚ä½•ç”ŸæˆåµŒå…¥
"""

from sentence_transformers import SentenceTransformer
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = SentenceTransformer('all-MiniLM-L6-v2')

# ç¤ºä¾‹æ–‡æœ¬
texts = [
    "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
    "Javaä¹Ÿæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
]

print("="*60)
print("åµŒå…¥ç”Ÿæˆæ¼”ç¤º")
print("="*60 + "\n")

# æ­¥éª¤1ï¼šTokenåŒ–
print("æ­¥éª¤1: Tokenizationï¼ˆåˆ†è¯ï¼‰")
tokens = model.tokenize(texts)
print(f"Tokenæ•°é‡: {len(tokens[0])}")
print(f"Tokens: {tokens[0][:10]}...")  # æ˜¾ç¤ºå‰10ä¸ªtoken
print()

# æ­¥éª¤2ï¼šç”ŸæˆåµŒå…¥
print("æ­¥éª¤2: ç”ŸæˆåµŒå…¥å‘é‡")
embeddings = model.encode(texts)

print(f"åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
print(f"åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
print()

# æ­¥éª¤3ï¼šæ˜¾ç¤ºéƒ¨åˆ†å‘é‡
print("æ­¥éª¤3: åµŒå…¥å‘é‡ï¼ˆå‰5ç»´ï¼‰")
for i, (text, emb) in enumerate(zip(texts, embeddings)):
    print(f"\næ–‡æœ¬{i+1}: {text[:20]}...")
    print(f"å‘é‡å‰5ç»´: {emb[:5]}")
    print(f"å‘é‡èŒƒæ•°: {torch.norm(torch.tensor(emb)):.3f}")
```

### åŒç¼–ç å™¨æ¶æ„

ç°ä»£åµŒå…¥æ¨¡å‹é€šå¸¸é‡‡ç”¨**åŒç¼–ç å™¨ï¼ˆSiameseï¼‰**æ¶æ„ï¼š

```python
# æ–‡ä»¶åï¼š06_03_siamese_network.py
"""
åŒç¼–ç å™¨ç½‘ç»œæ¼”ç¤º
"""

import torch
import torch.nn as nn

class SiameseNetwork(nn.Module):
    """åŒç¼–ç å™¨ç½‘ç»œ"""

    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder  # å…±äº«çš„ç¼–ç å™¨
        self.projection_head = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Linear(256, 256)
        )

    def forward(self, text1, text2):
        # ä½¿ç”¨å…±äº«ç¼–ç å™¨
        emb1 = self.encoder(text1)
        emb2 = self.encoder(text2)

        # æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´
        proj1 = self.projection_head(emb1)
        proj2 = self.projection_head(emb2)

        return proj1, proj2

# å¯¹æ¯”æŸå¤±
class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æŸå¤±"""

    def forward(self, proj1, proj2, temperature=0.07):
        # å½’ä¸€åŒ–
        proj1 = torch.nn.functional.normalize(proj1, dim=-1)
        proj2 = torch.nn.functional.normalize(proj2, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = torch.matmul(proj1, proj2.T) / temperature

        # å¯¹æ¯”æŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼‰
        loss = -torch.log(torch.exp(torch.diag(similarity)).sum())

        return loss

# æ¼”ç¤ºè®­ç»ƒè¿‡ç¨‹
if __name__ == "__main__":
    print("="*60)
    print("åŒç¼–ç å™¨ç½‘ç»œæ¼”ç¤º")
    print("="*60 + "\n")

    print("æ¶æ„è¯´æ˜:")
    print("1. å…±äº«ç¼–ç å™¨ï¼šä¸¤ä¸ªè¾“å…¥ä½¿ç”¨ç›¸åŒçš„BERT")
    print("2. æŠ•å½±å¤´ï¼šå°†BERTè¾“å‡ºæ˜ å°„åˆ°ç»Ÿä¸€ç©ºé—´")
    print("3. å¯¹æ¯”æŸå¤±ï¼šä½¿ç›¸ä¼¼æ–‡æœ¬é è¿‘ï¼Œä¸ç›¸ä¼¼æ–‡æœ¬è¿œç¦»")
    print()

    print("è®­ç»ƒè¿‡ç¨‹:")
    print("-" * 40)
    print("Epoch 1: Loss = 2.345")
    print("Epoch 10: Loss = 0.876")
    print("Epoch 50: Loss = 0.234")
    print("...")
    print("Epoch 100: Loss = 0.089")
    print()
    print("âœ… æ¨¡å‹å­¦ä¼šäº†è¯­ä¹‰ç›¸ä¼¼æ€§")
```

---

## 6.2 ä¸»æµåµŒå…¥æ¨¡å‹å¯¹æ¯”

### æ¨¡å‹åˆ†ç±»

```
åµŒå…¥æ¨¡å‹åˆ†ç±»æ ‘ï¼š

åµŒå…¥æ¨¡å‹
â”œâ”€ é—­æºAPIæ¨¡å‹
â”‚  â”œâ”€ OpenAI (text-embedding-3)
â”‚  â”œâ”€ Cohere (embed-v3)
â”‚  â””â”€ Google (text-embedding-gecko)
â”‚
â”œâ”€ å¼€æºé€šç”¨æ¨¡å‹
â”‚  â”œâ”€ Sentence-BERTç³»åˆ—
â”‚  â”œâ”€ BGEç³»åˆ— (æ™ºæº)
â”‚  â””â”€ E5ç³»åˆ— (Microsoft)
â”‚
â””â”€ å¼€æºé¢†åŸŸæ¨¡å‹
   â”œâ”€ CodeBERT (ä»£ç )
   â”œâ”€ SciBERT (ç§‘å­¦)
   â””â”€ MedCPT (åŒ»å­¦)
```

### è¯¦ç»†å¯¹æ¯”è¡¨æ ¼

| æ¨¡å‹ | ç»´åº¦ | MTEBå¾—åˆ†* | é€Ÿåº¦ | æˆæœ¬ | ç‰¹ç‚¹ |
|------|------|-----------|------|------|------|
| **OpenAI small** | 1536 | 72.3 | å¿« | ä»˜è´¹ | ç¨³å®šã€æ˜“ç”¨ |
| **OpenAI large** | 3072 | 78.1 | ä¸­ | ä»˜è´¹ | æœ€é«˜è´¨é‡ |
| **BGE-small-zh** | 512 | 68.5 | å¾ˆå¿« | å…è´¹ | ä¸­æ–‡ä¼˜åŒ– |
| **BGE-large-zh** | 1024 | 74.2 | ä¸­ | å…è´¹ | ä¸­æ–‡SOTA |
| **E5-large-v2** | 1024 | 75.1 | ä¸­ | å…è´¹ | å¤šè¯­è¨€ |
| **MiniLM-L6** | 384 | 65.8 | æå¿« | å…è´¹ | è½»é‡çº§ |

*MTEB: Massive Text Embedding Benchmark

### æ€§èƒ½å¯¹æ¯”å®éªŒ

```python
# æ–‡ä»¶åï¼š06_04_model_comparison.py
"""
åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”å®éªŒ
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import time
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict

class EmbeddingModel:
    """åµŒå…¥æ¨¡å‹åŒ…è£…å™¨"""

    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)
        self.name = model_name

    def encode(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        return self.model.encode(texts, show_progress_bar=False)

# æµ‹è¯•æ¨¡å‹åˆ—è¡¨
models_to_test = [
    "all-MiniLM-L6-v2",      # 384ç»´ï¼Œå¿«é€Ÿ
    "all-mpnet-base-v2",       # 768ç»´ï¼Œå¹³è¡¡
    "BAAI/bge-small-zh-v1.5",  # ä¸­æ–‡ä¼˜åŒ–
]

# æµ‹è¯•æ•°æ®
test_documents = [
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
    "Javaä¹Ÿæ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€",
    "JavaScriptä¸»è¦ç”¨äºWebå¼€å‘",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "æˆ‘å–œæ¬¢åƒè‹¹æœ"
]

test_queries = [
    "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
    "ä»€ä¹ˆè¯­è¨€ç”¨äºWebå¼€å‘ï¼Ÿ",
    "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
]

def evaluate_model(model: EmbeddingModel) -> Dict:
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\nè¯„ä¼°æ¨¡å‹: {model.name}")
    print("-" * 60)

    results = {}

    # 1. ç¼–ç é€Ÿåº¦
    print("æµ‹è¯•1: ç¼–ç é€Ÿåº¦")
    start_time = time.time()
    doc_embeddings = model.encode(test_documents)
    query_embeddings = model.encode(test_queries)
    encode_time = time.time() - start_time
    results['encode_time'] = encode_time
    print(f"  ç¼–ç æ—¶é—´: {encode_time:.3f}ç§’")

    # 2. åµŒå…¥ç»´åº¦
    results['embedding_dim'] = doc_embeddings.shape[1]
    print(f"  åµŒå…¥ç»´åº¦: {doc_embeddings.shape[1]}")

    # 3. æ£€ç´¢è´¨é‡
    print("\næµ‹è¯•2: æ£€ç´¢è´¨é‡")
    hit_count = 0

    for i, query_emb in enumerate(query_embeddings):
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(
            [query_emb],
            doc_embeddings
        )[0]

        # æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£
        top_idx = np.argmax(similarities)

        # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ï¼ˆå‡è®¾å‰3ä¸ªæŸ¥è¯¢å¯¹åº”å‰3ä¸ªæ–‡æ¡£ï¼‰
        if i < 3 and top_idx == i:
            hit_count += 1

    hit_rate = hit_count / 3  # å‰3ä¸ªæŸ¥è¯¢
    results['hit_rate'] = hit_rate
    print(f"  Hit Rate (å‰3æŸ¥è¯¢): {hit_rate:.2%}")

    # 4. å†…å­˜å ç”¨
    print("\næµ‹è¯•3: å†…å­˜å ç”¨")
    import sys
    model_size = sum(p.numel() * p.element_size() for p in model.model.parameters())
    model_size_mb = model_size / (1024 * 1024)
    results['model_size_mb'] = model_size_mb
    print(f"  æ¨¡å‹å¤§å°: {model_size_mb:.2f}MB")

    return results

# è¿è¡Œå¯¹æ¯”
print("="*70)
print("åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
print("="*70)

all_results = {}

for model_name in models_to_test:
    try:
        model = EmbeddingModel(model_name)
        results = evaluate_model(model)
        all_results[model_name] = results
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        continue

# æ€»ç»“å¯¹æ¯”
print("\n" + "="*70)
print("æ€§èƒ½å¯¹æ¯”æ€»ç»“")
print("="*70)

print(f"\n{'æ¨¡å‹':<30} {'ç»´åº¦':<8} {'é€Ÿåº¦':<10} {'Hit Rate':<12} {'å¤§å°':<10}")
print("-" * 70)

for model_name, results in all_results.items():
    short_name = model_name.split('/')[-1][:28]
    print(f"{short_name:<30} "
          f"{results['embedding_dim']:<8} "
          f"{results['encode_time']:.3f}s   {'':<2}"
          f"{results['hit_rate']:<12.0%} "
          f"{results['model_size_mb']:<10.1f}")
```

### é€‰æ‹©å»ºè®®

#### æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©

**åœºæ™¯1ï¼šå¿«é€ŸåŸå‹**
```
æ¨è: MiniLM-L6-v2
åŸå› :
  - æ¨¡å‹å°ï¼ˆ70MBï¼‰
  - é€Ÿåº¦å¿«
  - æ•ˆæœè¿˜å¯ä»¥
  - å…è´¹ä½¿ç”¨
```

**åœºæ™¯2ï¼šç”Ÿäº§ç¯å¢ƒï¼ˆè‹±æ–‡ï¼‰**
```
æ¨è: OpenAI text-embedding-3-small
åŸå› :
  - è´¨é‡ç¨³å®š
  - APIè°ƒç”¨æ–¹ä¾¿
  - ä¸éœ€è¦è‡ªå·±éƒ¨ç½²
  - æœ‰SLAä¿éšœ
```

**åœºæ™¯3ï¼šä¸­æ–‡åº”ç”¨**
```
æ¨è: BGE-large-zh-v1.5
åŸå› :
  - ä¸­æ–‡SOTAæ€§èƒ½
  - å¼€æºå…è´¹
  - å¯ç§æœ‰éƒ¨ç½²
  - ç¤¾åŒºæ´»è·ƒ
```

**åœºæ™¯4ï¼šå¤šè¯­è¨€**
```
æ¨è: E5-large-v2
åŸå› :
  - æ”¯æŒ100+è¯­è¨€
  - æ€§èƒ½ä¼˜ç§€
  - å¤šè¯­è¨€å¯¹é½å¥½
```

#### å†³ç­–æ ‘

```
å¼€å§‹
  â†“
éœ€è¦ä¸­æ–‡ä¼˜åŒ–ï¼Ÿ
  â”œâ”€ æ˜¯ â†’ BGE-large-zh
  â””â”€ å¦ â†“
    é¢„ç®—å……è¶³ï¼Ÿ
      â”œâ”€ æ˜¯ â†’ OpenAI large
      â””â”€ å¦ â†“
        éœ€è¦é€Ÿåº¦ï¼Ÿ
          â”œâ”€ æ˜¯ â†’ MiniLM-L6
          â””â”€ å¦ â†’ MPNet-base
```

---

## 6.3 åµŒå…¥æ¨¡å‹å¾®è°ƒ

### ä¸ºä»€ä¹ˆå¾®è°ƒï¼Ÿ

**é—®é¢˜**ï¼šé¢„è®­ç»ƒæ¨¡å‹å¯èƒ½ä¸é€‚åˆä½ çš„ç‰¹å®šé¢†åŸŸ

```
é€šç”¨æ¨¡å‹åœ¨ç»´åŸºç™¾ç§‘ä¸Šè®­ç»ƒ
  â†“
ä½ çš„é¢†åŸŸï¼šåŒ»å­¦ã€æ³•å¾‹ã€é‡‘è...
  â†“
é—®é¢˜ï¼š
- é¢†åŸŸæœ¯è¯­ç†è§£ä¸å‡†ç¡®
- ç‰¹å®šè¡¨è¾¾æ–¹å¼ä¸ç†Ÿæ‚‰
- ç›¸ä¼¼åº¦åˆ¤æ–­æœ‰åå·®
```

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨é¢†åŸŸæ•°æ®ä¸Šå¾®è°ƒ

```
é¢†åŸŸæ•°æ®å¾®è°ƒ
  â†“
æ¨¡å‹æ›´æ‡‚ä½ çš„é¢†åŸŸ
  â†“
æ£€ç´¢è´¨é‡æå‡5-10%
```

### å¾®è°ƒæ•°æ®å‡†å¤‡

#### æ•°æ®æ ¼å¼

```python
# æ–‡ä»¶åï¼š06_05_finetuning_data.py
"""
å‡†å¤‡å¾®è°ƒæ•°æ®
"""

# æ ‡å‡†å¾®è°ƒæ•°æ®æ ¼å¼
finetuning_data = [
    {
        "anchor": "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
        "positive": "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "negative": "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    },
    {
        "anchor": "å¦‚ä½•ä½¿ç”¨Pandasï¼Ÿ",
        "positive": "Pandasæ˜¯Pythonæ•°æ®åˆ†æåº“",
        "negative": "Javaä¹Ÿæœ‰ç±»ä¼¼çš„åŠŸèƒ½"
    }
]

# è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
from torch.utils.data import Dataset

class FinetuningDataset(Dataset):
    """å¾®è°ƒæ•°æ®é›†"""

    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            "anchor": item["anchor"],
            "positive": item["positive"],
            "negative": item["negative"]
        }

# åˆ›å»ºæ•°æ®é›†
dataset = FinetuningDataset(finetuning_data)
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
```

### ä½¿ç”¨Sentence-Transformerså¾®è°ƒ

```python
# æ–‡ä»¶åï¼š06_06_finetuning.py
"""
å¾®è°ƒåµŒå…¥æ¨¡å‹
"""

from sentence_transformers import SentenceTransformer, InputExample, losses, models
from torch.utils.data import DataLoader

def finetune_model(
    base_model_name: str,
    train_data: list,
    output_path: str,
    num_epochs: int = 3,
    batch_size: int = 16
):
    """
    å¾®è°ƒåµŒå…¥æ¨¡å‹

    Args:
        base_model_name: åŸºç¡€æ¨¡å‹åç§°
        train_data: è®­ç»ƒæ•°æ®åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
    """

    print("="*60)
    print("åµŒå…¥æ¨¡å‹å¾®è°ƒ")
    print("="*60 + "\n")

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    print("æ­¥éª¤1: åŠ è½½åŸºç¡€æ¨¡å‹")
    model = SentenceTransformer(base_model_name)
    print(f"âœ… åŠ è½½æ¨¡å‹: {base_model_name}")

    # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\næ­¥éª¤2: å‡†å¤‡è®­ç»ƒæ•°æ®")
    train_examples = [
        InputExample(texts=[item["anchor"], item["positive"]], label=1.0)
        for item in train_data
    ] + [
        InputExample(texts=[item["anchor"], item["negative"]], label=0.0)
        for item in train_data
    ]

    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(train_examples)}")

    # 3. é…ç½®æŸå¤±å‡½æ•°
    print("\næ­¥éª¤3: é…ç½®æŸå¤±å‡½æ•°")
    train_loss = losses.CosineSimilarityLoss(model=model)

    # 4. é…ç½®ä¼˜åŒ–å™¨
    print("æ­¥éª¤4: é…ç½®ä¼˜åŒ–å™¨")
    # ç¬¬ä¸€æ­¥ï¼šåªè®­ç»ƒæœ€åä¸€ä¸ªå˜æ¢å±‚
    warmup_steps = 100

    # 5. è®­ç»ƒ
    print("\næ­¥éª¤5: å¼€å§‹è®­ç»ƒ")
    print(f"Epochs: {num_epochs}")
    print(f"Batch size: {batch_size}")

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=num_epochs,
        warmup_steps=warmup_steps,
        output_path=output_path,
        show_progress_bar=True
    )

    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")

    # 6. è¯„ä¼°
    print("\næ­¥éª¤6: è¯„ä¼°å¾®è°ƒæ•ˆæœ")
    # è¿™é‡Œå¯ä»¥æ·»åŠ è¯„ä¼°ä»£ç 

    return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡†å¤‡ç¤ºä¾‹æ•°æ®
    train_data = [
        {
            "anchor": "Pythonæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ",
            "positive": "Pythonçš„ç‰¹ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€æ˜“å­¦æ˜“ç”¨",
            "negative": "JavaScriptæ˜¯Webå¼€å‘è¯­è¨€"
        },
        {
            "anchor": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
            "positive": "RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯",
            "negative": "ä»Šå¤©å¤©æ°”æ™´æœ—"
        }
        # ... æ›´å¤šæ•°æ®
    ]

    # å¾®è°ƒ
    finetune_model(
        base_model_name="BAAI/bge-small-zh-v1.5",
        train_data=train_data,
        output_path="./models/finetuned_bge",
        num_epochs=3
    )
```

---

## 6.4 åµŒå…¥å¯è§†åŒ–ä¸è¯„ä¼°

### åµŒå…¥ç©ºé—´å¯è§†åŒ–

ä½¿ç”¨t-SNEæˆ–UMAPé™ç»´å¯è§†åŒ–åµŒå…¥ç©ºé—´

```python
# æ–‡ä»¶åï¼š06_07_embedding_visualization.py
"""
åµŒå…¥ç©ºé—´å¯è§†åŒ–
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sentence_transformers import SentenceTransformer
import pandas as pd

def visualize_embeddings(model_name: str, texts: list, labels: list):
    """
    å¯è§†åŒ–åµŒå…¥ç©ºé—´

    Args:
        model_name: æ¨¡å‹åç§°
        texts: æ–‡æœ¬åˆ—è¡¨
        labels: æ ‡ç­¾åˆ—è¡¨
    """
    print("="*60)
    print("åµŒå…¥ç©ºé—´å¯è§†åŒ–")
    print("="*60 + "\n")

    # 1. ç”ŸæˆåµŒå…¥
    print("æ­¥éª¤1: ç”ŸæˆåµŒå…¥å‘é‡")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts)
    print(f"âœ… åµŒå…¥å½¢çŠ¶: {embeddings.shape}")

    # 2. é™ç»´ï¼ˆt-SNEï¼‰
    print("\næ­¥éª¤2: t-SNEé™ç»´")
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(texts)-1))
    embeddings_2d = tsne.fit_transform(embeddings)
    print(f"âœ… é™ç»´åå½¢çŠ¶: {embeddings_2d.shape}")

    # 3. ç»˜åˆ¶æ•£ç‚¹å›¾
    print("\næ­¥éª¤3: ç»˜åˆ¶å¯è§†åŒ–")
    plt.figure(figsize=(12, 8))

    # æŒ‰æ ‡ç­¾åˆ†ç»„ç€è‰²
    unique_labels = list(set(labels))
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))

    for i, label in enumerate(unique_labels):
        mask = np.array(labels) == label
        plt.scatter(
            embeddings_2d[mask, 0],
            embeddings_2d[mask, 1],
            c=[colors[i]],
            label=label,
            alpha=0.7,
            s=100
        )

    plt.xlabel("t-SNE ç»´åº¦ 1")
    plt.ylabel("t-SNE ç»´åº¦ 2")
    plt.title("åµŒå…¥ç©ºé—´å¯è§†åŒ–")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # ä¿å­˜
    output_path = "./outputs/embedding_visualization.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {output_path}")

    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡†å¤‡æ•°æ®
    texts = [
        # ç¼–ç¨‹è¯­è¨€
        "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "Javaä¹Ÿæ˜¯ç¼–ç¨‹è¯­è¨€",
        "JavaScriptæ˜¯Webè¯­è¨€",
        "C++é€‚åˆç³»ç»Ÿç¼–ç¨‹",

        # æ°´æœ
        "è‹¹æœæ˜¯æ°´æœ",
        "é¦™è•‰ä¹Ÿæ˜¯æ°´æœ",
        "æ©™å­å¯Œå«ç»´ç”Ÿç´ C",

        # åŠ¨ç‰©
        "çŒ«æ˜¯å® ç‰©",
        "ç‹—æ˜¯å® ç‰©",
        "è€è™æ˜¯é‡ç”ŸåŠ¨ç‰©"
    ]

    labels = ["ç¼–ç¨‹"] * 4 + ["æ°´æœ"] * 3 + ["åŠ¨ç‰©"] * 3

    # å¯è§†åŒ–
    visualize_embeddings(
        model_name="all-MiniLM-L6-v2",
        texts=texts,
        labels=labels
    )
```

### åµŒå…¥è´¨é‡è¯„ä¼°

```python
# æ–‡ä»¶åï¼š06_08_embedding_evaluation.py
"""
è¯„ä¼°åµŒå…¥è´¨é‡
"""

import numpy as np
from typing import List, Dict
from sklearn.metrics.pairwise import cosine_similarity

class EmbeddingEvaluator:
    """åµŒå…¥è¯„ä¼°å™¨"""

    def __init__(self, model):
        self.model = model

    def evaluate_retrieval(self, queries: list, documents: list, relevant_docs: list):
        """
        è¯„ä¼°æ£€ç´¢è´¨é‡

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            documents: æ–‡æ¡£åˆ—è¡¨
            relevant_docs: çœŸå®ç›¸å…³æ–‡æ¡£ï¼ˆåˆ—è¡¨çš„åˆ—è¡¨ï¼‰

        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        # ç”ŸæˆåµŒå…¥
        query_embeddings = self.model.encode(queries)
        doc_embeddings = self.model.encode(documents)

        # è®¡ç®—æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢è´¨é‡
        hit_count = 0
        mrr_sum = 0

        for i, query_emb in enumerate(query_embeddings):
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity([query_emb], doc_embeddings)[0]

            # æ’åº
            sorted_indices = np.argsort(similarities)[::-1]

            # Hit Rate (top-5)
            top5_indices = sorted_indices[:5]
            if any(idx in relevant_docs[i] for idx in top5_indices):
                hit_count += 1

            # MRR
            for rank, idx in enumerate(sorted_indices, 1):
                if idx in relevant_docs[i]:
                    mrr_sum += 1 / rank
                    break

        metrics = {
            "hit_rate_at_5": hit_count / len(queries),
            "mrr": mrr_sum / len(queries)
        }

        return metrics

    def print_report(self, metrics: Dict):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("åµŒå…¥è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        print("="*60)

        for metric, value in metrics.items():
            metric_name = metric.replace("_", " ").title()
            if "rate" in metric.lower():
                print(f"{metric_name:20s}: {value:.2%}")
            else:
                print(f"{metric_name:20s}: {value:.3f}")

        print("="*60)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer('all-MiniLM-L6-v2')
    evaluator = EmbeddingEvaluator(model)

    # æµ‹è¯•æ•°æ®
    queries = [
        "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä»€ä¹ˆè¯­è¨€ç”¨äºWebå¼€å‘ï¼Ÿ",
        "C++æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    ]

    documents = [
        "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "Javaä¹Ÿæ˜¯ç¼–ç¨‹è¯­è¨€",
        "JavaScriptä¸»è¦ç”¨äºWebå¼€å‘",
        "C++é€‚åˆç³»ç»Ÿç¼–ç¨‹",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    ]

    relevant_docs = [
        {0},  # ç¬¬ä¸€ä¸ªæŸ¥è¯¢ç›¸å…³ç¬¬0ä¸ªæ–‡æ¡£
        {2},  # ç¬¬äºŒä¸ªæŸ¥è¯¢ç›¸å…³ç¬¬2ä¸ªæ–‡æ¡£
        {3}   # ç¬¬ä¸‰ä¸ªæŸ¥è¯¢ç›¸å…³ç¬¬3ä¸ªæ–‡æ¡£
    ]

    # è¯„ä¼°
    metrics = evaluator.evaluate_retrieval(queries, documents, relevant_docs)
    evaluator.print_report(metrics)
```

---

## æ€»ç»“

### æœ¬ç« è¦ç‚¹å›é¡¾

1. **åµŒå…¥æ¨¡å‹åŸç†**
   - Transformeræ¶æ„å›é¡¾
   - åŒç¼–ç å™¨ç½‘ç»œ
   - å¯¹æ¯”å­¦ä¹ æŸå¤±

2. **æ¨¡å‹é€‰æ‹©**
   - ä¸»æµæ¨¡å‹å¯¹æ¯”
   - æ ¹æ®åœºæ™¯é€‰æ‹©
   - æƒè¡¡æ€§èƒ½ã€é€Ÿåº¦ã€æˆæœ¬

3. **æ¨¡å‹å¾®è°ƒ**
   - æ•°æ®å‡†å¤‡
   - å¾®è°ƒæµç¨‹
   - æ•ˆæœè¯„ä¼°

4. **å¯è§†åŒ–è¯„ä¼°**
   - t-SNEé™ç»´
   - ç©ºé—´å¯è§†åŒ–
   - è´¨é‡è¯„ä¼°æŒ‡æ ‡

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£TransformeråµŒå…¥åŸç†
- [ ] æŒæ¡ä¸»æµåµŒå…¥æ¨¡å‹å¯¹æ¯”
- [ ] èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æ¨¡å‹
- [ ] ç†è§£å¾®è°ƒè¿‡ç¨‹
- [ ] èƒ½å¤Ÿå¯è§†åŒ–åµŒå…¥ç©ºé—´
- [ ] æŒæ¡è´¨é‡è¯„ä¼°æ–¹æ³•

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- **ä¸‹ä¸€ç« **ï¼š[ç¬¬7ç« ï¼šé«˜çº§åˆ†å—ç­–ç•¥](./07-é«˜çº§åˆ†å—ç­–ç•¥.md)
- **ç›¸å…³**ï¼šä¼˜åŒ–åµŒå…¥æ¨¡å‹æ˜¯ç¬¬ä¸€æ­¥ï¼Œæ¥ä¸‹æ¥ä¼˜åŒ–æ•°æ®è´¨é‡

### æ‰©å±•èµ„æº

**æ¨èé˜…è¯»**ï¼š
1. [Sentence-BERTè®ºæ–‡](https://arxiv.org/abs/1908.10084)
2. [BGEæ¨¡å‹è®ºæ–‡](https://arxiv.org/abs/2309.07597)
3. [MTEB Benchmark](https://github.com/embeddings-benchmark/mteb)

**å®è·µé¡¹ç›®**ï¼š
- å¯¹æ¯”3ç§ä¸åŒçš„åµŒå…¥æ¨¡å‹
- åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹
- å¯è§†åŒ–å¹¶åˆ†æåµŒå…¥ç©ºé—´

---

**è¿”å›ç›®å½•** | **ä¸Šä¸€ç« ï¼šæ¨¡å—1æ€»ç»“** | **ä¸‹ä¸€ç« ï¼šé«˜çº§åˆ†å—ç­–ç•¥**

---

**æœ¬ç« ç»“æŸ**

> é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹æ˜¯RAGä¼˜åŒ–çš„ç¬¬ä¸€æ­¥ã€‚ä¸€ä¸ªå¥½çš„åµŒå…¥æ¨¡å‹å¯ä»¥è®©æ£€ç´¢è´¨é‡æå‡10-15%ï¼Œè¿™æ˜¯æœ€ç®€å•ä¹Ÿæ˜¯æœ€é‡è¦çš„ä¼˜åŒ–ä¹‹ä¸€ï¼
