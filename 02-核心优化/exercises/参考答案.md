# æ¨¡å—2ç»ƒä¹ é¢˜å‚è€ƒç­”æ¡ˆ

> æœ¬æ–‡æ¡£æä¾›äº†æ¨¡å—2ç»ƒä¹ é¢˜çš„å‚è€ƒç­”æ¡ˆå’Œå®ç°æ€è·¯ï¼Œä¾›å­¦ä¹ å‚è€ƒã€‚

---

## ğŸ’¡ ä½¿ç”¨è¯´æ˜

**å¦‚ä½•ä½¿ç”¨æœ¬ç­”æ¡ˆé›†**ï¼š

1. **å…ˆè‡ªå·±å°è¯•**
   - ç‹¬ç«‹æ€è€ƒé—®é¢˜
   - å°è¯•å®ç°è§£å†³æ–¹æ¡ˆ
   - é‡åˆ°å›°éš¾æ—¶å†å‚è€ƒ

2. **å¯¹æ¯”å­¦ä¹ **
   - å¯¹æ¯”ä½ çš„å®ç°å’Œå‚è€ƒç­”æ¡ˆ
   - ç†è§£ä¸åŒå®ç°æ–¹å¼çš„ä¼˜ç¼ºç‚¹
   - å­¦ä¹ æœ€ä½³å®è·µ

3. **ä¸¾ä¸€åä¸‰**
   - åŸºäºå‚è€ƒç­”æ¡ˆè¿›è¡Œæ”¹è¿›
   - å°è¯•ä¼˜åŒ–å’Œæ‰©å±•
   - åº”ç”¨åˆ°è‡ªå·±çš„é¡¹ç›®

---

## ç¬¬6ç« ï¼šåµŒå…¥æ¨¡å‹æ·±å…¥

### ç»ƒä¹ 6.1ï¼šæ¨¡å‹å¯¹æ¯”åˆ†æ

**å‚è€ƒå®ç°**ï¼š

```python
# æ–‡ä»¶ï¼šsolutions/ch06_01_model_comparison.py
"""
åµŒå…¥æ¨¡å‹å¯¹æ¯”åˆ†æ
"""

from sentence_transformers import SentenceTransformer
import time
import numpy as np


class EmbeddingModelComparator:
    """åµŒå…¥æ¨¡å‹å¯¹æ¯”å™¨"""

    def __init__(self, model_names: list):
        self.models = {}
        for name in model_names:
            print(f"åŠ è½½æ¨¡å‹: {name}")
            self.models[name] = SentenceTransformer(name)

    def compare_models(self, queries: list, docs: list, relevant_docs: dict):
        """
        å¯¹æ¯”æ¨¡å‹æ€§èƒ½

        Args:
            queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            docs: æ–‡æ¡£åˆ—è¡¨
            relevant_docs: ç›¸å…³æ–‡æ¡£æ˜ å°„ {query_idx: [doc_ids]}
        """
        results = {}

        for model_name, model in self.models.items():
            print(f"\nè¯„ä¼°æ¨¡å‹: {model_name}")

            # åµŒå…¥æ–‡æ¡£
            start = time.time()
            doc_embeddings = model.encode(docs, show_progress_bar=True)
            embed_time = time.time() - start

            # è®¡ç®—å‘é‡ç»´åº¦
            vector_dim = doc_embeddings.shape[1]

            # è®¡ç®—å†…å­˜å ç”¨
            memory_mb = doc_embeddings.nbytes / 1024 / 1024

            # è¯„ä¼°æ£€ç´¢è´¨é‡
            hit_rates = []
            for i, query in enumerate(queries):
                query_embedding = model.encode([query])

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = np.dot(query_embedding, doc_embeddings.T).flatten()

                # Top-10ç´¢å¼•
                top10_indices = np.argsort(similarities)[::-1][:10]

                # æ£€æŸ¥å‘½ä¸­ç‡
                relevant = relevant_docs.get(i, [])
                hit = any(idx in relevant for idx in top10_indices)
                hit_rates.append(1 if hit else 0)

            # æµ‹è¯•æŸ¥è¯¢é€Ÿåº¦
            start = time.time()
            for query in queries[:10]:  # åªæµ‹è¯•å‰10ä¸ª
                model.encode([query])
            avg_query_time = (time.time() - start) / 10

            results[model_name] = {
                'vector_dim': vector_dim,
                'embed_time': embed_time,
                'memory_mb': memory_mb,
                'hit_rate': np.mean(hit_rates),
                'avg_query_time': avg_query_time
            }

        return results

    def print_comparison(self, results: dict):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*80)
        print("æ¨¡å‹å¯¹æ¯”ç»“æœ")
        print("="*80)

        print(f"\n{'æ¨¡å‹':<30} {'ç»´åº¦':<10} {'å†…å­˜(MB)':<12} {'Hit Rate':<12} {'æŸ¥è¯¢æ—¶é—´(ms)'}")
        print("-"*80)

        for name, metrics in results.items():
            print(f"{name:<30} {metrics['vector_dim']:<10} "
                  f"{metrics['memory_mb']:<12.2f} {metrics['hit_rate']:<12.4f} "
                  f"{metrics['avg_query_time']*1000:<12.2f}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
    models_to_test = [
        "all-MiniLM-L6-v2",        # å¿«é€Ÿï¼Œè´¨é‡ä¸€èˆ¬
        "all-mpnet-base-v2",        # å¹³è¡¡
        "BAAI/bge-small-en-v1.5"    # é«˜è´¨é‡
    ]

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    queries = [
        "Pythonæ€§èƒ½ä¼˜åŒ–æŠ€å·§",
        "JavaScriptå¼‚æ­¥ç¼–ç¨‹",
        "æœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒ"
    ]

    docs = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼ä¼˜åŒ–æ€§èƒ½...",
        "JavaScriptçš„å¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨Promiseã€async/await...",
        "æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ...",
        # ... æ›´å¤šæ–‡æ¡£
    ] * 20  # 60ä¸ªæ–‡æ¡£

    relevant_docs = {
        0: [0, 3, 6],  # ç¬¬1ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£
        1: [1, 4, 7],
        2: [2, 5, 8]
    }

    # æ‰§è¡Œå¯¹æ¯”
    comparator = EmbeddingModelComparator(models_to_test)
    results = comparator.compare_models(queries, docs, relevant_docs)
    comparator.print_comparison(results)
```

**å…³é”®è¦ç‚¹**ï¼š
1. ä½¿ç”¨sentence-transformersåŠ è½½å¤šä¸ªæ¨¡å‹
2. è®°å½•æ¯ä¸ªæ¨¡å‹çš„å…³é”®æŒ‡æ ‡
3. ç”¨è¡¨æ ¼å½¢å¼æ¸…æ™°å±•ç¤ºå¯¹æ¯”ç»“æœ
4. ç»“è®ºï¼šBGEæ¨¡å‹åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šè¾¾åˆ°æœ€ä½³å¹³è¡¡

---

### ç»ƒä¹ 6.2ï¼šæ¨¡å‹å¾®è°ƒ

**å‚è€ƒå®ç°**ï¼š

```python
# æ–‡ä»¶ï¼šsolutions/ch06_02_model_finetuning.py
"""
åµŒå…¥æ¨¡å‹å¾®è°ƒ
"""

from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import random


def prepare_training_data(docs: list, queries: list):
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®

    æ ¼å¼ï¼š(query, positive_doc, negative_doc)
    """
    train_examples = []

    for query, relevant_docs in queries:
        # æ­£æ ·æœ¬
        for pos_doc in relevant_docs:
            # éšæœºé€‰æ‹©è´Ÿæ ·æœ¬
            neg_doc = random.choice([d for d in docs if d not in relevant_docs])

            example = InputExample(
                texts=[query, pos_doc, neg_doc]
            )
            train_examples.append(example)

    return train_examples


def finetune_model(base_model: str, train_data, output_path: str,
                   epochs: int = 3, batch_size: int = 16):
    """
    å¾®è°ƒåµŒå…¥æ¨¡å‹

    Args:
        base_model: åŸºç¡€æ¨¡å‹åç§°
        train_data: è®­ç»ƒæ•°æ®
        output_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹å¤§å°
    """
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = SentenceTransformer(base_model)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_data,
        shuffle=True,
        batch_size=batch_size
    )

    # å®šä¹‰æŸå¤±å‡½æ•°
    train_loss = losses.MultipleNegativesRankingLoss(model)

    # è®­ç»ƒ
    print(f"å¼€å§‹å¾®è°ƒï¼Œå…±{len(train_data)}ä¸ªæ ·æœ¬ï¼Œ{epochs}è½®")
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=100,
        output_path=output_path
    )

    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    return model


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡†å¤‡æ•°æ®
    docs = [f"æ–‡æ¡£{i}çš„å†…å®¹..." for i in range(100)]

    queries = [
        ("Pythonä¼˜åŒ–æŠ€å·§", ["docs/001.txt", "docs/005.txt"]),
        ("JavaScriptå…¥é—¨", ["docs/002.txt"]),
        # ... æ›´å¤šæŸ¥è¯¢
    ] * 10  # è‡³å°‘50ä¸ªæ ·æœ¬

    # å¾®è°ƒæ¨¡å‹
    model = finetune_model(
        base_model="BAAI/bge-small-en-v1.5",
        train_data=prepare_training_data(docs, queries),
        output_path="./finetuned_model",
        epochs=3
    )
```

**å…³é”®è¦ç‚¹**ï¼š
1. ä½¿ç”¨MultipleNegativesRankingLoss
2. æ•°æ®æ ¼å¼ï¼š(query, positive, negative)
3. è‡³å°‘50-100ä¸ªè®­ç»ƒæ ·æœ¬
4. å¾®è°ƒåé€šå¸¸èƒ½æå‡5-10%çš„Hit Rate

---

## ç¬¬7ç« ï¼šé«˜çº§åˆ†å—ç­–ç•¥

### ç»ƒä¹ 7.1ï¼šåˆ†å—ç­–ç•¥å¯¹æ¯”

**å‚è€ƒå®ç°**ï¼š

```python
# æ–‡ä»¶ï¼šsolutions/ch07_01_chunking_comparison.py
"""
åˆ†å—ç­–ç•¥å¯¹æ¯”
"""

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    SentenceTransformersTokenTextSplitter
)
from llama_index.node_parser import (
    SemanticSplitterNodeParser,
    SimpleFileNodeParser
)
from sentence_transformers import SentenceTransformer
import numpy as np


class ChunkingComparator:
    """åˆ†å—ç­–ç•¥å¯¹æ¯”å™¨"""

    def __init__(self, documents: list):
        self.documents = documents

    def test_fixed_chunking(self, chunk_size: int = 512, overlap: int = 50):
        """å›ºå®šé•¿åº¦åˆ†å—"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            separators=["\n\n", "\n", "ã€‚", " ", ""]
        )

        chunks = []
        for doc in self.documents:
            doc_chunks = splitter.split_text(doc)
            chunks.extend(doc_chunks)

        return {
            'strategy': 'fixed',
            'num_chunks': len(chunks),
            'avg_chunk_size': np.mean([len(c) for c in chunks])
        }

    def test_semantic_chunking(self):
        """è¯­ä¹‰åˆ†å—"""
        model = SentenceTransformer("all-MiniLM-L6-v2")
        splitter = SemanticSplitterNodeParser(
            buffer_size=1,
            breakpoint_percentile_threshold=95,
            embed_model=model
        )

        # ç®€åŒ–ç¤ºä¾‹
        chunks = []
        for doc in self.documents[:10]:  # åªå¤„ç†å‰10ä¸ªæ–‡æ¡£
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ›´å¤æ‚çš„å¤„ç†
            sentences = doc.split("ã€‚")
            chunks.extend([s + "ã€‚" for s in sentences if s.strip()])

        return {
            'strategy': 'semantic',
            'num_chunks': len(chunks),
            'avg_chunk_size': np.mean([len(c) for c in chunks])
        }

    def compare_and_recommend(self, queries, relevant_docs):
        """å¯¹æ¯”ç­–ç•¥å¹¶æ¨è"""
        results = []

        # æµ‹è¯•å›ºå®šåˆ†å—
        fixed_result = self.test_fixed_chunking(chunk_size=512)
        hit_rate = self._evaluate_strategy(fixed_result, queries, relevant_docs)
        fixed_result['hit_rate'] = hit_rate
        results.append(fixed_result)

        # æµ‹è¯•è¯­ä¹‰åˆ†å—
        semantic_result = self.test_semantic_chunking()
        hit_rate = self._evaluate_strategy(semantic_result, queries, relevant_docs)
        semantic_result['hit_rate'] = hit_rate
        results.append(semantic_result)

        # æ‰“å°å¯¹æ¯”
        print("\nåˆ†å—ç­–ç•¥å¯¹æ¯”:")
        print(f"{'ç­–ç•¥':<15} {'åˆ†å—æ•°':<10} {'å¹³å‡å¤§å°':<12} {'Hit Rate'}")
        print("-"*60)
        for r in results:
            print(f"{r['strategy']:<15} {r['num_chunks']:<10} "
                  f"{r['avg_chunk_size']:<12.0f} {r['hit_rate']:.4f}")

        # æ¨è
        best = max(results, key=lambda x: x['hit_rate'])
        print(f"\næ¨èç­–ç•¥: {best['strategy']} (Hit Rate: {best['hit_rate']:.4f})")

    def _evaluate_strategy(self, chunk_result, queries, relevant_docs):
        """è¯„ä¼°ç­–ç•¥æ•ˆæœï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…å®ç°éœ€è¦åµŒå…¥å’Œæ£€ç´¢
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿå€¼
        return 0.75 if chunk_result['strategy'] == 'semantic' else 0.68
```

**å…³é”®è¦ç‚¹**ï¼š
1. è¯­ä¹‰åˆ†å—é€šå¸¸ä¼˜äºå›ºå®šåˆ†å—ï¼ˆ+5-10% Hit Rateï¼‰
2. æœ€ä¼˜chunk_sizeé€šå¸¸åœ¨384-768ä¹‹é—´
3. åº”è¯¥æ ¹æ®å®é™…æ•°æ®æµ‹è¯•ç¡®å®š

---

## ç¬¬8ç« ï¼šæŸ¥è¯¢å¢å¼ºæŠ€æœ¯

### ç»ƒä¹ 8.1ï¼šHyDEå®ç°

**å‚è€ƒå®ç°**ï¼š

```python
# æ–‡ä»¶ï¼šsolutions/ch08_01_hyde.py
"""
HyDEå®ç°
"""

from openai import OpenAI
from sentence_transformers import SentenceTransformer
import numpy as np


class HyDERetriever:
    """
    HyDEæ£€ç´¢å™¨

    æµç¨‹ï¼š
    1. LLMç”Ÿæˆå‡è®¾ç­”æ¡ˆ
    2. åµŒå…¥å‡è®¾ç­”æ¡ˆ
    3. ç”¨å‡è®¾ç­”æ¡ˆæ£€ç´¢
    """

    def __init__(self, embedding_model, docs, openai_api_key):
        self.embedding_model = embedding_model
        self.docs = docs
        self.client = OpenAI(api_key=openai_api_key)

        # é¢„åµŒå…¥æ–‡æ¡£
        self.doc_embeddings = self.embedding_model.encode(docs)

    def generate_hypothetical(self, query: str) -> str:
        """ç”Ÿæˆå‡è®¾ç­”æ¡ˆ"""
        prompt = f"è¯·åŸºäºä»¥ä¸‹é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªè¯¦ç»†çš„å‡è®¾ç­”æ¡ˆã€‚é—®é¢˜ï¼š{query}"

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200
        )

        return response.choices[0].message.content

    def retrieve(self, query: str, use_hyde: bool = True, top_k: int = 5):
        """æ£€ç´¢æ–‡æ¡£"""
        if use_hyde:
            # HyDEæµç¨‹
            hypothetical = self.generate_hypothetical(query)
            query_embedding = self.embedding_model.encode([hypothetical])
        else:
            # ç›´æ¥æ£€ç´¢
            query_embedding = self.embedding_model.encode([query])

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(query_embedding, self.doc_embeddings.T).flatten()

        # Top-K
        top_k_indices = np.argsort(similarities)[::-1][:top_k]

        return [
            {
                'doc': self.docs[idx],
                'score': similarities[idx]
            }
            for idx in top_k_indices
        ]


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import os

    # å‡†å¤‡æ–‡æ¡£
    docs = [
        "Pythonçš„GILæ˜¯å…¨å±€è§£é‡Šå™¨é”ï¼Œå®ƒé™åˆ¶äº†å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ...",
        "JavaScriptçš„å¼‚æ­¥ç¼–ç¨‹åŸºäºäº‹ä»¶å¾ªç¯å’Œå›è°ƒé˜Ÿåˆ—...",
        # ... æ›´å¤šæ–‡æ¡£
    ] * 20

    # åˆ›å»ºHyDEæ£€ç´¢å™¨
    retriever = HyDERetriever(
        embedding_model=SentenceTransformer("BAAI/bge-small-en-v1.5"),
        docs=docs,
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    # å¯¹æ¯”æµ‹è¯•
    query = "å¦‚ä½•æå‡Pythonä»£ç çš„æ‰§è¡Œæ•ˆç‡ï¼Ÿ"

    print("ä¸ä½¿ç”¨HyDE:")
    results_without = retriever.retrieve(query, use_hyde=False)
    for r in results_without[:3]:
        print(f"  Score: {r['score']:.4f} - {r['doc'][:50]}...")

    print("\nä½¿ç”¨HyDE:")
    results_with = retriever.retrieve(query, use_hyde=True)
    for r in results_with[:3]:
        print(f"  Score: {r['score']:.4f} - {r['doc'][:50]}...")
```

**å…³é”®è¦ç‚¹**ï¼š
1. HyDEå¯¹æŠ½è±¡é—®é¢˜ç‰¹åˆ«æœ‰æ•ˆï¼ˆ+15-20% Hit Rateï¼‰
2. éœ€è¦LLM APIï¼Œä¼šå¢åŠ æˆæœ¬å’Œå»¶è¿Ÿ
3. äº‹å®æ€§é—®é¢˜å¯èƒ½ä¸éœ€è¦HyDE

---

## ç¬¬9ç« ï¼šæ··åˆæ£€ç´¢ä¸é‡æ’åº

### ç»ƒä¹ 9.1ï¼šRRFç®—æ³•å®ç°

**å‚è€ƒå®ç°**ï¼š

```python
# æ–‡ä»¶ï¼šsolutions/ch09_01_rrf.py
"""
RRFèåˆç®—æ³•å®ç°
"""

from typing import List, Tuple
import numpy as np


def rrf_fusion(rankings: List[List[Tuple[str, float]]],
               k: int = 60,
               weights: List[float] = None) -> List[Tuple[str, float]]:
    """
    RRFèåˆç®—æ³•

    Args:
        rankings: å¤šä¸ªæ£€ç´¢å™¨çš„æ’ååˆ—è¡¨
        k: å¹³æ»‘å‚æ•°
        weights: å„æ£€ç´¢å™¨çš„æƒé‡ï¼ˆå¯é€‰ï¼‰

    Returns:
        èåˆåçš„ç»“æœåˆ—è¡¨

    Example:
        >>> vector_results = [("doc1", 0.9), ("doc2", 0.8)]
        >>> bm25_results = [("doc2", 25.0), ("doc1", 20.0)]
        >>> fused = rrf_fusion([vector_results, bm25_results])
    """
    if weights is None:
        weights = [1.0] * len(rankings)

    # å½’ä¸€åŒ–æƒé‡
    weights = np.array(weights) / np.sum(weights)

    # å­˜å‚¨RRFåˆ†æ•°
    rrf_scores = {}

    # è®¡ç®—æ¯ä¸ªæ£€ç´¢å™¨çš„è´¡çŒ®
    for ranking, weight in zip(rankings, weights):
        for rank, (doc_id, _) in enumerate(ranking, start=1):
            # è®¡ç®—åŠ æƒå€’æ•°ç§©
            reciprocal_rank = weight / (k + rank)

            # ç´¯åŠ 
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = 0.0
            rrf_scores[doc_id] += reciprocal_rank

    # æ’åº
    fused = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

    return fused


# å•å…ƒæµ‹è¯•
def test_rrf_fusion():
    """æµ‹è¯•RRFç®—æ³•"""
    # æµ‹è¯•æ•°æ®
    vector_results = [
        ("doc1", 0.95),  # æ’å1
        ("doc2", 0.88),  # æ’å2
        ("doc3", 0.75),  # æ’å3
    ]

    bm25_results = [
        ("doc2", 28.5),  # æ’å1
        ("doc1", 22.1),  # æ’å2
        ("doc4", 18.5),  # æ’å3
    ]

    # æ‰§è¡Œèåˆ
    fused = rrf_fusion([vector_results, bm25_results], k=60)

    print("RRFèåˆç»“æœ:")
    for doc_id, score in fused:
        print(f"  {doc_id}: {score:.4f}")

    # éªŒè¯
    assert fused[0][0] in ["doc1", "doc2"], "Topç»“æœåº”è¯¥æ˜¯doc1æˆ–doc2"

    print("\næµ‹è¯•é€šè¿‡ï¼âœ…")


if __name__ == "__main__":
    test_rrf_fusion()
```

**å…³é”®è¦ç‚¹**ï¼š
1. RRFæ— éœ€å½’ä¸€åŒ–åˆ†æ•°ï¼Œåªä½¿ç”¨æ’å
2. å…¬å¼ï¼šRRF = Î£ weight / (k + rank)
3. kå‚æ•°é€šå¸¸è®¾ä¸º60
4. å¯ä»¥é€šè¿‡æƒé‡è°ƒæ•´ä¸åŒæ£€ç´¢å™¨çš„é‡è¦æ€§

---

## ç¬¬11ç« ï¼šæ€§èƒ½ä¼˜åŒ–

### ç»ƒä¹ 11.1ï¼šå¤šå±‚ç¼“å­˜å®ç°

**å‚è€ƒå®ç°**ï¼š

```python
# æ–‡ä»¶ï¼šsolutions/ch11_01_multi_layer_cache.py
"""
å¤šå±‚ç¼“å­˜å®ç°ï¼ˆL1 + L2ï¼‰
"""

import time
from collections import OrderedDict
from typing import Optional, Any
import hashlib
import json


class L1Cache:
    """L1ç¼“å­˜ï¼ˆå†…å­˜ï¼ŒLRUï¼‰"""

    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()
        self.timestamps = {}

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if key not in self.cache:
            return None

        # æ£€æŸ¥TTL
        if time.time() - self.timestamps[key] > self.ttl:
            del self.cache[key]
            del self.timestamps[key]
            return None

        # æ›´æ–°è®¿é—®é¡ºåºï¼ˆLRUï¼‰
        self.cache.move_to_end(key)
        return self.cache[key]

    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        # æ£€æŸ¥å®¹é‡
        if len(self.cache) >= self.max_size:
            # åˆ é™¤æœ€æ—§çš„
            self.cache.popitem(last=False)

        self.cache[key] = value
        self.timestamps[key] = time.time()

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.timestamps.clear()


class L2Cache:
    """L2ç¼“å­˜ï¼ˆRedisæˆ–æ–‡ä»¶ï¼‰"""

    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = cache_dir
        import os
        os.makedirs(cache_dir, exist_ok=True)

    def _get_path(self, key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        key_hash = hashlib.md5(key.encode()).hexdigest()
        return f"{self.cache_dir}/{key_hash}.json"

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        path = self._get_path(key)
        try:
            import os
            if not os.path.exists(path):
                return None

            # æ£€æŸ¥TTLï¼ˆå‡è®¾å­˜å‚¨åœ¨æ–‡ä»¶ä¸­ï¼‰
            with open(path, 'r') as f:
                data = json.load(f)

            if time.time() - data['timestamp'] > data['ttl']:
                os.remove(path)
                return None

            return data['value']
        except:
            return None

    def set(self, key: str, value: Any, ttl: int = 86400):
        """è®¾ç½®ç¼“å­˜"""
        path = self._get_path(key)
        data = {
            'value': value,
            'timestamp': time.time(),
            'ttl': ttl
        }
        with open(path, 'w') as f:
            json.dump(data, f)


class MultiLayerCache:
    """å¤šå±‚ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self):
        self.l1 = L1Cache(max_size=1000, ttl=3600)
        self.l2 = L2Cache(cache_dir="./cache")

        # ç»Ÿè®¡
        self.stats = {
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0,
            'total': 0
        }

    def get(self, key: str) -> Optional[Any]:
        """è·å–ï¼ˆå…ˆL1åL2ï¼‰"""
        self.stats['total'] += 1

        # L1æŸ¥æ‰¾
        value = self.l1.get(key)
        if value is not None:
            self.stats['l1_hits'] += 1
            return value

        # L2æŸ¥æ‰¾
        value = self.l2.get(key)
        if value is not None:
            self.stats['l2_hits'] += 1
            # å›å¡«L1
            self.l1.set(key, value)
            return value

        # æœªå‘½ä¸­
        self.stats['misses'] += 1
        return None

    def set(self, key: str, value: Any):
        """è®¾ç½®ï¼ˆåŒæ—¶å†™L1å’ŒL2ï¼‰"""
        self.l1.set(key, value)
        self.l2.set(key, value)

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats['total']
        return {
            'l1_hit_rate': self.stats['l1_hits'] / total if total > 0 else 0,
            'l2_hit_rate': self.stats['l2_hits'] / total if total > 0 else 0,
            'overall_hit_rate': (self.stats['l1_hits'] + self.stats['l2_hits']) / total if total > 0 else 0,
            'miss_rate': self.stats['misses'] / total if total > 0 else 0
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    cache = MultiLayerCache()

    # æµ‹è¯•
    print("æµ‹è¯•å¤šå±‚ç¼“å­˜:")

    # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆæœªå‘½ä¸­ï¼‰
    result1 = cache.get("test_key")
    print(f"ç¬¬ä¸€æ¬¡æŸ¥è¯¢: {result1}")

    # è®¾ç½®ç¼“å­˜
    cache.set("test_key", {"answer": "è¿™æ˜¯ç¼“å­˜çš„å†…å®¹"})

    # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆL1å‘½ä¸­ï¼‰
    result2 = cache.get("test_key")
    print(f"ç¬¬äºŒæ¬¡æŸ¥è¯¢: {result2}")

    # ç»Ÿè®¡
    stats = cache.get_stats()
    print(f"\nç¼“å­˜ç»Ÿè®¡:")
    print(f"  L1å‘½ä¸­ç‡: {stats['l1_hit_rate']*100:.1f}%")
    print(f"  L2å‘½ä¸­ç‡: {stats['l2_hit_rate']*100:.1f}%")
    print(f"  æ•´ä½“å‘½ä¸­ç‡: {stats['overall_hit_rate']*100:.1f}%")
```

**å…³é”®è¦ç‚¹**ï¼š
1. L1ï¼šå†…å­˜ç¼“å­˜ï¼Œé€Ÿåº¦å¿«ï¼Œå®¹é‡å°
2. L2ï¼šç£ç›˜/Redisï¼Œé€Ÿåº¦æ…¢ï¼Œå®¹é‡å¤§
3. L1æœªå‘½ä¸­æ—¶æŸ¥L2ï¼ŒL2æœªå‘½ä¸­æ—¶æ‰§è¡Œå®é™…æŸ¥è¯¢
4. L2å‘½ä¸­åå›å¡«L1
5. æ•´ä½“å‘½ä¸­ç‡å¯è¾¾80-90%

---

## ç»¼åˆé¡¹ç›®å‚è€ƒæ¶æ„

### InteliKB v2.0

**é¡¹ç›®ç»“æ„**ï¼š

```
intelikb_v2/
â”œâ”€â”€ config.py                  # é…ç½®
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ embedding.py          # åµŒå…¥æ¨¡å‹
â”‚   â”œâ”€â”€ reranker.py           # é‡æ’åº
â”‚   â””â”€â”€ llm.py                # LLMæ¥å£
â”œâ”€â”€ retrievers/
â”‚   â”œâ”€â”€ vector.py             # å‘é‡æ£€ç´¢
â”‚   â”œâ”€â”€ bm25.py               # BM25æ£€ç´¢
â”‚   â””â”€â”€ hybrid.py             # æ··åˆæ£€ç´¢
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ l1_cache.py           # å†…å­˜ç¼“å­˜
â”‚   â””â”€â”€ l2_cache.py           # Redisç¼“å­˜
â”œâ”€â”€ engine/
â”‚   â””â”€â”€ rag_engine.py         # RAGå¼•æ“
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py                # APIæœåŠ¡
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag.py           # æµ‹è¯•
â””â”€â”€ main.py                   # ä¸»ç¨‹åº
```

**æ ¸å¿ƒå¼•æ“å®ç°**ï¼ˆå·²åœ¨ç¬¬12ç« æä¾›ï¼‰

---

## ğŸ“ å­¦ä¹ å»ºè®®

### å¦‚ä½•ä½¿ç”¨å‚è€ƒç­”æ¡ˆ

1. **ç‹¬ç«‹æ€è€ƒä¼˜å…ˆ**
   - å…ˆè‡ªå·±å°è¯•è§£å†³é—®é¢˜
   - é‡åˆ°å›°éš¾æ—¶å†æŸ¥çœ‹ç­”æ¡ˆ
   - ç†è§£ç­”æ¡ˆçš„æ€è·¯

2. **å¯¹æ¯”ä¸åŒå®ç°**
   - ä½ çš„å®ç° vs å‚è€ƒç­”æ¡ˆ
   - åˆ†æå„è‡ªçš„ä¼˜ç¼ºç‚¹
   - é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ

3. **å®è·µæ”¹è¿›**
   - åŸºäºç­”æ¡ˆæ”¹è¿›ä»£ç 
   - æ·»åŠ é¢å¤–åŠŸèƒ½
   - ä¼˜åŒ–æ€§èƒ½

4. **ä¸¾ä¸€åä¸‰**
   - å°†æ–¹æ³•åº”ç”¨åˆ°å…¶ä»–é—®é¢˜
   - å°è¯•å˜åŒ–å’Œæ‰©å±•
   - ç§¯ç´¯ç»éªŒ

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å‚è€ƒç­”æ¡ˆä¸æ˜¯å”¯ä¸€è§£**
   - ç¼–ç¨‹æœ‰å¤šç§å®ç°æ–¹å¼
   - å…³æ³¨æ€è·¯è€Œéå…·ä½“ä»£ç 
   - é¼“åŠ±åˆ›æ–°å’Œä¼˜åŒ–

2. **ç†è§£åŸç†æ›´é‡è¦**
   - ä¸è¦åªæ˜¯å¤åˆ¶ç²˜è´´
   - ç†è§£æ¯ä¸€æ­¥çš„ä½œç”¨
   - çŸ¥é“ä¸ºä»€ä¹ˆè¿™æ ·åš

3. **æŒç»­å­¦ä¹ **
   - æŠ€æœ¯åœ¨ä¸æ–­å‘å±•
   - ä¿æŒå­¦ä¹ å¿ƒæ€
   - å…³æ³¨æœ€æ–°è¿›å±•

---

**ç¥ä½ å­¦ä¹ è¿›æ­¥ï¼** ğŸš€

> æœ‰ç–‘é—®æˆ–å»ºè®®ï¼Ÿæ¬¢è¿åœ¨GitHubæäº¤Issueæˆ–PRï¼
